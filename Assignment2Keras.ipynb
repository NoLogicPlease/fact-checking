{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "import zipfile\n",
        "import pickle\n",
        "import gensim\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from urllib import request\n",
        "\n",
        "import collections\n",
        "import gensim.downloader as gloader\n",
        "import sklearn.metrics as sk_metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "metadata": {
        "id": "GeSdeyC06gXH"
      },
      "id": "GeSdeyC06gXH",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to use key_to_index attribute from the embedding model\n",
        "! pip install gensim==4.1.2\n",
        "import gensim\n",
        "import gensim.downloader as gloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yk9TyQj7lXf",
        "outputId": "e64b4f69-46b4-404e-9486-df68e34d7887"
      },
      "id": "_yk9TyQj7lXf",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "eb78692f",
      "metadata": {
        "id": "eb78692f"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_SIZE = 100\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1780a936",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1780a936",
        "outputId": "e6b3567a-4ad7-4235-e4f2-22512952c9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use. \n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "        \n",
        "fix_random(42)\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4acf8c4",
      "metadata": {
        "id": "e4acf8c4"
      },
      "source": [
        "# PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e33cdb69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33cdb69",
        "outputId": "b37ad059-88b2-4c86-db0f-3ec64aad9583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/NLP/Assignment2  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/NLP/Assignment2\n",
            "/content/gdrive/My Drive/NLP/Assignment2\n",
            "Checking working directory:\n"
          ]
        }
      ],
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                           params={'id': toy_data_url_id},\n",
        "                                           stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "\n",
        "def pre_process(dataset, filename):  # clean the dataset\n",
        "    dataset.drop(dataset.columns[0], axis=1, inplace=True)  # remove first column of dataframe containing numbers\n",
        "    dataset.drop(['ID'], axis=1, inplace=True)\n",
        "    # remove numbers before each evidence\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'^\\d+\\t', '', x))\n",
        "    # remove everything after the period\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r' \\..*', ' .', x))\n",
        "    # remove round brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LRB-.*-RRB-', '', x))\n",
        "    # remove square brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LSB-.*-RSB-', '', x))\n",
        "\n",
        "    n_before = dataset.shape[0]\n",
        "    # removes instances longer than a threshold on evidence\n",
        "    # TODO: only on train\n",
        "    dataset = dataset[dataset['Evidence'].str.split().str.len() <= 100]\n",
        "    # remove all rows where there are single brackets in the evidence\n",
        "    dataset = dataset[~dataset['Evidence'].str.contains('|'.join(['-LRB-', '-LSB-', '-RRB-', '-RSB-']))]\n",
        "    n_after = dataset.shape[0]\n",
        "\n",
        "    # removes punctuation and excessive spaces\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r' +', ' ', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'^ +', '', x))\n",
        "    dataset = dataset.applymap(lambda x: x.lower())\n",
        "\n",
        "    labels = {'supports': 1, 'refutes': 0}\n",
        "    dataset = dataset.replace({'Label': labels})\n",
        "    # removes rows with empty elements\n",
        "    dataset = dataset[dataset['Evidence'] != '']\n",
        "    dataset = dataset[dataset['Claim'] != '']\n",
        "    dataset = dataset[dataset['Label'] != '']\n",
        "\n",
        "\n",
        "\n",
        "    rem_elements = n_before - n_after\n",
        "    print(f\"Removed {rem_elements}\\t ({100 * rem_elements / n_before:.2F}%)\"\n",
        "          f\" elements because of inconsistency on {filename}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB=True\n",
        "except:\n",
        "    IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"We're running Colab\")\n",
        "    # Mount the Google Drive at mount\n",
        "    mount='/content/gdrive'\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Switch to the directory on the Google Drive that you want to use\n",
        "    drive_root = mount + \"/My Drive/NLP/Assignment2\"\n",
        "    \n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "        print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "        os.makedirs(drive_root, exist_ok=True)\n",
        "    \n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "    print(\"Checking working directory:\")\n",
        "    %pwd\n",
        "\n",
        "# download_data('dataset')\n",
        "\n",
        "if not len(os.listdir(\"dataset_cleaned\")):\n",
        "    for file in os.listdir(\"dataset\"):\n",
        "        dataset_cleaned = pre_process(pd.read_csv(\"dataset/\" + file, sep=','), file)\n",
        "        dataset_cleaned.to_csv(os.path.join(\"dataset_cleaned\", file))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57177769",
      "metadata": {
        "id": "57177769"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ba4c24ef",
      "metadata": {
        "id": "ba4c24ef"
      },
      "outputs": [],
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, dataset_sentences, embedding_dim, glove_dict, glove_matrix):\n",
        "        self.embedding_matrix = None\n",
        "        self.value_to_key = {}\n",
        "        self.value_to_key_new = {}\n",
        "        self.key_to_value = {}\n",
        "        self.num_unique_words = 0\n",
        "        self.dataset_sentences = dataset_sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.glove_dict = glove_dict\n",
        "        self.glove_matrix = glove_matrix\n",
        "        self.unique_words = set()\n",
        "\n",
        "    def get_val_to_key(self):\n",
        "        return copy.deepcopy(self.value_to_key)\n",
        "\n",
        "    def tokenize(self):\n",
        "        self.value_to_key_new = {}\n",
        "        unique_words = set()\n",
        "        for sen in self.dataset_sentences:\n",
        "            for w in sen.split():\n",
        "                unique_words.add(w)  # get se of unique words\n",
        "        new_unique = unique_words - self.unique_words\n",
        "        for i, word in enumerate(new_unique):\n",
        "            if self.embedding_matrix is not None:\n",
        "                self.key_to_value[i + len(self.embedding_matrix)] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i + len(self.embedding_matrix)\n",
        "            else:\n",
        "                self.key_to_value[i] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i\n",
        "            self.value_to_key_new[word] = i\n",
        "\n",
        "        self.num_unique_words = len(new_unique)\n",
        "        self.unique_words = self.unique_words | new_unique  # union of unique words and new unique words\n",
        "\n",
        "    def __build_embedding_matrix_glove(self):\n",
        "        oov_words = []\n",
        "        tmp_embedding_matrix = np.zeros((self.num_unique_words, self.embedding_dim)) #dtype=np.float32\n",
        "        len_old_emb_matrix = len(self.embedding_matrix) if self.embedding_matrix is not None else 0\n",
        "        for word, idx in tqdm(self.value_to_key_new.items()):\n",
        "            try:\n",
        "                embedding_vector = self.glove_matrix[self.glove_dict[word]]\n",
        "                tmp_embedding_matrix[idx] = embedding_vector\n",
        "            except (KeyError, TypeError):\n",
        "                oov_words.append((word, idx + len_old_emb_matrix))\n",
        "        \n",
        "        if self.embedding_matrix is not None:\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, tmp_embedding_matrix))\n",
        "\n",
        "        else:\n",
        "            self.embedding_matrix = copy.deepcopy(tmp_embedding_matrix)\n",
        "        return oov_words\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        oov_words = self.__build_embedding_matrix_glove()\n",
        "        for word, idx in oov_words:\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=self.embedding_dim)\n",
        "            self.embedding_matrix[idx] = embedding_vector\n",
        "        return copy.deepcopy(self.embedding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD DATA"
      ],
      "metadata": {
        "id": "1s-Jy4k_8ZmO"
      },
      "id": "1s-Jy4k_8ZmO"
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD GLOVE\n",
        "try:\n",
        "    with open(f\"glove-{EMBEDDING_SIZE}.pkl\", 'rb') as f:\n",
        "        emb_model = pickle.load(f)\n",
        "except Exception:\n",
        "    emb_model = gloader.load(f\"glove-wiki-gigaword-{EMBEDDING_SIZE}\")\n",
        "    with open(f\"glove-{EMBEDDING_SIZE}.pkl\", 'wb') as f:\n",
        "        pickle.dump(emb_model, f)\n",
        "\n",
        "glove_dict = emb_model.key_to_index\n",
        "glove_matrix = emb_model.vectors\n",
        "\n",
        "train = pd.read_csv(\"dataset_cleaned/train_pairs.csv\")\n",
        "val = pd.read_csv(\"dataset_cleaned/val_pairs.csv\")\n",
        "test = pd.read_csv(\"dataset_cleaned/test_pairs.csv\")\n",
        "\n",
        "try:\n",
        "    with open(\"emb_mat.pkl\", 'rb') as f:\n",
        "        v4_matrix = pickle.load(f)\n",
        "    with open(\"val_to_key.pkl\", 'rb') as f:\n",
        "        v4_val_to_key = pickle.load(f)\n",
        "    with open(f\"tokenizer.pkl\", 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "except Exception:\n",
        "    tokenizer = Tokenizer(train[\"Claim\"] + ' ' + train[\"Evidence\"], EMBEDDING_SIZE, glove_dict, glove_matrix)\n",
        "    tokenizer.tokenize()\n",
        "    v2_matrix = tokenizer.build_embedding_matrix()\n",
        "    tokenizer.dataset_sentences = val[\"Claim\"] + ' ' + val[\"Evidence\"]\n",
        "    tokenizer.tokenize()\n",
        "    v3_matrix = tokenizer.build_embedding_matrix()\n",
        "    tokenizer.dataset_sentences = test[\"Claim\"] + ' ' + test[\"Evidence\"]\n",
        "    tokenizer.tokenize()\n",
        "    v4_matrix = tokenizer.build_embedding_matrix()\n",
        "    v4_val_to_key = tokenizer.get_val_to_key()\n",
        "    with open(f\"emb_mat.pkl\", 'wb') as f:\n",
        "        pickle.dump(v4_matrix, f)\n",
        "    with open(f\"val_to_key.pkl\", 'wb') as f:\n",
        "        pickle.dump(v4_val_to_key, f)\n",
        "    with open(f\"tokenizer.pkl\", 'wb') as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "v4_val_to_key.update((x, y+1) for x, y in v4_val_to_key.items())\n",
        "\n",
        "\n",
        "translate_tokens = {}\n",
        "key_val_list_items = list(tokenizer.key_to_value.items())\n",
        "for i, (token, value) in enumerate(key_val_list_items):\n",
        "    if i > 0:\n",
        "        translate_tokens[token] = key_val_list_items[i-1][1]\n",
        "    else:\n",
        "        translate_tokens[i] = '<PAD>'\n"
      ],
      "metadata": {
        "id": "B5STMeqA8YWe"
      },
      "id": "B5STMeqA8YWe",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e11402a",
      "metadata": {
        "id": "0e11402a"
      },
      "source": [
        "# GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e3a42578",
      "metadata": {
        "id": "e3a42578"
      },
      "outputs": [],
      "source": [
        "def generator(dataset, value_to_key):\n",
        "    dataset_size = dataset.shape[0]\n",
        "    dataset = dataset.to_numpy()[:, 1:]\n",
        "\n",
        "    refutes = dataset[dataset[:,2] == 0]\n",
        "    supports = dataset[dataset[:,2] == 1]\n",
        "    \n",
        "    while True:\n",
        "        X_claim = []\n",
        "        X_evid = []\n",
        "        y = []\n",
        "\n",
        "        rnd_choices_refutes = np.random.choice(np.arange(len(refutes)),replace=False, size=BATCH_SIZE//2)\n",
        "        rnd_choices_supports = np.random.choice(np.arange(len(supports)),replace=False, size=BATCH_SIZE//2)\n",
        "\n",
        "        batch = []\n",
        "        for i in range(BATCH_SIZE//2):\n",
        "            batch.append(list(refutes[rnd_choices_refutes[i]]))\n",
        "            batch.append(list(supports[rnd_choices_supports[i]]))\n",
        "        \n",
        "        random.shuffle(batch)\n",
        "\n",
        "        max_seq_claim = max([len(el[0].split()) for el in batch])\n",
        "        max_seq_evid = max([len(el[1].split()) for el in batch])\n",
        "\n",
        "        for sample in batch:\n",
        "            tokenized_claim = [value_to_key[word] for word in sample[0].split()]\n",
        "            tokenized_evid = [value_to_key[word] for word in sample[1].split()]\n",
        "\n",
        "            tmp_claim = [0] * (max_seq_claim - len(tokenized_claim)) + tokenized_claim\n",
        "            tmp_evid = [0] * (max_seq_evid - len(tokenized_evid)) + tokenized_evid\n",
        "\n",
        "            X_claim.append(tmp_claim)\n",
        "            X_evid.append(tmp_evid)\n",
        "            y.append(sample[2])\n",
        "\n",
        "        yield [np.array(X_claim), np.array(X_evid)], np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_finite(dataset, value_to_key):\n",
        "    \"\"\"\n",
        "        FOR A VALIDATION SET\n",
        "        batches will be doubled to go faster\n",
        "    \"\"\"\n",
        "    dataset_size = dataset.shape[0]\n",
        "    dataset = dataset.to_numpy()[:, 1:]\n",
        "    np.random.shuffle(dataset)\n",
        "\n",
        "    i = 0\n",
        "    \n",
        "    while True:\n",
        "        X_claim = []\n",
        "        X_evid = []\n",
        "        y = []\n",
        "\n",
        "        batch = dataset[i: i + BATCH_SIZE*2]\n",
        "\n",
        "        max_seq_claim = max([len(el[0].split()) for el in batch])\n",
        "        max_seq_evid = max([len(el[1].split()) for el in batch])\n",
        "\n",
        "        for sample in batch:\n",
        "            tokenized_claim = [value_to_key[word] for word in sample[0].split()]\n",
        "            tokenized_evid = [value_to_key[word] for word in sample[1].split()]\n",
        "\n",
        "            tmp_claim = [0] * (max_seq_claim - len(tokenized_claim)) + tokenized_claim\n",
        "            tmp_evid = [0] * (max_seq_evid - len(tokenized_evid)) + tokenized_evid\n",
        "\n",
        "            X_claim.append(tmp_claim)\n",
        "            X_evid.append(tmp_evid)\n",
        "            y.append(sample[2])\n",
        "\n",
        "        i += BATCH_SIZE*2\n",
        "        if i > len(dataset):\n",
        "            break\n",
        "\n",
        "        yield [np.array(X_claim), np.array(X_evid)], np.array(y)\n"
      ],
      "metadata": {
        "id": "4jGkTXRGmzmE"
      },
      "id": "4jGkTXRGmzmE",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[x_claims, x_evids], labels = next(generator(train, v4_val_to_key))\n",
        "print(x_claims[0])\n",
        "print(x_evids[0])\n",
        "print(labels[0])\n",
        "\n",
        "print(' '.join([translate_tokens[w] for w in x_claims[0]]))\n",
        "print(' '.join([translate_tokens[w] for w in x_evids[0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj25oe65ZFGr",
        "outputId": "8d15d08c-5172-48f3-e6ff-732ac6a69f6d"
      },
      "id": "Bj25oe65ZFGr",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0     0 19127\n",
            "  7837 22389  5765 28308  9696 13594]\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0  6493 13713 22389  5765 28308  9696\n",
            " 13594 25666   652 16778  5728  3111  9321 22718 31499 19906 27249 13932\n",
            "  5051 22143  9696 16778 10000   300 22430   548]\n",
            "1\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> slash left guns n roses in 1996\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> after leaving guns n roses in 1996 he cofounded the supergroup velvet revolver which reestablished him as a mainstream performer in the mid to late 2000s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ],
      "metadata": {
        "id": "toiQ0Fu8YYN0"
      },
      "id": "toiQ0Fu8YYN0"
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, compile_info, value_to_key, embedding_dim, \n",
        "                 embedding_matrix, l2_reg):\n",
        "\n",
        "        self.compile_info = compile_info\n",
        "        self.value_to_key = value_to_key\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "        input_claim = keras.layers.Input(shape=(None,))\n",
        "        input_evid = keras.layers.Input(shape=(None,))\n",
        "        emb_claim = layers.Embedding(input_dim=len(v4_val_to_key.keys()) + 1,\n",
        "                                            output_dim=self.embedding_dim,\n",
        "                                            mask_zero=True,\n",
        "                                            weights=[self.embedding_matrix],\n",
        "                                            trainable=False\n",
        "                                            )(input_claim)\n",
        "        emb_evid = layers.Embedding(input_dim=len(v4_val_to_key.keys()) + 1,\n",
        "                                            output_dim=self.embedding_dim,\n",
        "                                            mask_zero=True,\n",
        "                                            weights=[self.embedding_matrix],\n",
        "                                            trainable=False\n",
        "                                            )(input_evid)\n",
        "\n",
        "        # mettere max_seq_len come num recurrents?\n",
        "        lstm_claim, forward_h_claim, forward_c_claim = layers.LSTM(64, \n",
        "                                                                   return_sequences=True, \n",
        "                                                                   kernel_regularizer=l2(self.l2_reg),\n",
        "                                                                   return_state = True)(emb_claim) \n",
        "        lstm_evid, forward_h_evid, forward_c_evid = layers.LSTM(64, \n",
        "                                                                return_sequences=True, \n",
        "                                                                kernel_regularizer=l2(self.l2_reg), \n",
        "                                                                return_state = True)(emb_evid)\n",
        "        merge = Concatenate()([forward_h_claim, forward_h_evid])\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(merge)\n",
        "\n",
        "        model = keras.Model(inputs=[input_claim, input_evid], outputs=outputs)\n",
        "        model.compile(**self.compile_info)\n",
        "        model.summary()\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def show_history(self, history: keras.callbacks.History):\n",
        "\n",
        "        history_data = history.history\n",
        "        print(\"Displaying the following history keys: \", history_data.keys())\n",
        "\n",
        "        for key, value in history_data.items():\n",
        "            if not key.startswith('val'):\n",
        "                fig, ax = plt.subplots(1, 1)\n",
        "                ax.set_title(key)\n",
        "                ax.plot(value)\n",
        "                if 'val_{}'.format(key) in history_data:\n",
        "                    ax.plot(history_data['val_{}'.format(key)])\n",
        "                else:\n",
        "                    print(\"Couldn't find validation values for metric: \", key)\n",
        "\n",
        "                ax.set_ylabel(key)\n",
        "                ax.set_xlabel('epoch')\n",
        "                ax.legend(['train', 'val'], loc='best')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def train_model(self,\n",
        "                  train_generator,\n",
        "                  val_generator,\n",
        "                  steps_per_epoch,\n",
        "                  verbose,\n",
        "                  epochs,\n",
        "                  callbacks):\n",
        "        \n",
        "        print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "        history = self.model.fit(train_generator,\n",
        "                                 validation_data=val_generator,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 verbose=verbose,\n",
        "                                 epochs=epochs,\n",
        "                                 callbacks=callbacks)\n",
        "        print(\"Training completed! Showing history...\")\n",
        "\n",
        "        self.show_history(history)\n"
      ],
      "metadata": {
        "id": "OyTdmqhCKXbD"
      },
      "id": "OyTdmqhCKXbD",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compile_info = {\n",
        "    'optimizer': keras.optimizers.Nadam(learning_rate=1e-3),\n",
        "    'loss': 'binary_crossentropy',\n",
        "    'metrics': ['acc']\n",
        "}\n",
        "\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': 3, \n",
        "    'steps_per_epoch': train.shape[0] // BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_acc', \n",
        "                                                patience=4,\n",
        "                                                restore_best_weights=True)]\n",
        "}\n",
        "\n",
        "model_params = {\n",
        "    'compile_info': compile_info,\n",
        "    'value_to_key': v4_val_to_key,\n",
        "    'embedding_dim': EMBEDDING_SIZE,\n",
        "    'embedding_matrix': np.vstack((np.zeros((1, EMBEDDING_SIZE)), v4_matrix)),\n",
        "    'l2_reg' : 1e-5\n",
        "}\n",
        "\n",
        "prediction_info = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'verbose': 1\n",
        "}\n",
        "\n",
        "model = Model(**model_params)\n",
        "\n",
        "history = model.train_model(generator(train, v4_val_to_key),\n",
        "                            generator_finite(val, v4_val_to_key),\n",
        "                            **training_info)\n",
        "print(\"Training completed! Showing history...\")\n",
        "\n",
        "model.show_history(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulSkdVWr7tXc",
        "outputId": "cce8d7b2-38be-4015-b4ee-b3ecd64e1946"
      },
      "id": "ulSkdVWr7tXc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, None, 100)    3529500     ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, None, 100)    3529500     ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_6 (LSTM)                  [(None, None, 64),   42240       ['embedding_6[0][0]']            \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)                  [(None, None, 64),   42240       ['embedding_7[0][0]']            \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 128)          0           ['lstm_6[0][1]',                 \n",
            "                                                                  'lstm_7[0][1]']                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            129         ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,143,609\n",
            "Trainable params: 84,609\n",
            "Non-trainable params: 7,059,000\n",
            "__________________________________________________________________________________________________\n",
            "Start training! \n",
            "Parameters: {'verbose': 1, 'epochs': 3, 'steps_per_epoch': 946, 'callbacks': [<keras.callbacks.EarlyStopping object at 0x7fada3727e50>]}\n",
            "Epoch 1/3\n",
            "946/946 [==============================] - 205s 193ms/step - loss: 0.5286 - acc: 0.7250 - val_loss: 0.5173 - val_acc: 0.7283\n",
            "Epoch 2/3\n",
            "946/946 [==============================] - ETA: 0s - loss: 0.4570 - acc: 0.7768WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 27 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
            "946/946 [==============================] - 172s 182ms/step - loss: 0.4570 - acc: 0.7768\n",
            "Epoch 3/3\n",
            "946/946 [==============================] - ETA: 0s - loss: 0.4292 - acc: 0.7953WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
            "946/946 [==============================] - 173s 183ms/step - loss: 0.4292 - acc: 0.7953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FjCJkdLsZAE1"
      },
      "id": "FjCJkdLsZAE1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Assignment2Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}