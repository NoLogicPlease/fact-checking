{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "import zipfile\n",
        "import pickle\n",
        "import gensim\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from urllib import request\n",
        "\n",
        "import collections\n",
        "import gensim.downloader as gloader\n",
        "import sklearn.metrics as sk_metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "metadata": {
        "id": "GeSdeyC06gXH"
      },
      "id": "GeSdeyC06gXH",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to use key_to_index attribute from the embedding model\n",
        "! pip install gensim==4.1.2\n",
        "import gensim\n",
        "import gensim.downloader as gloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yk9TyQj7lXf",
        "outputId": "2ee729be-bc32-4853-ec00-c72c78c690d9"
      },
      "id": "_yk9TyQj7lXf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "eb78692f",
      "metadata": {
        "id": "eb78692f"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_SIZE = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1780a936",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1780a936",
        "outputId": "893d4985-28e9-4caa-decb-394dcb25ed2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 16 15:23:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use. \n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "        \n",
        "fix_random(42)\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4acf8c4",
      "metadata": {
        "id": "e4acf8c4"
      },
      "source": [
        "# PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e33cdb69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33cdb69",
        "outputId": "e28d0235-e5e5-4e33-ad10-d571590399ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/NLP/Assignment2  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/NLP/Assignment2\n",
            "/content/gdrive/My Drive/NLP/Assignment2\n",
            "Checking working directory:\n"
          ]
        }
      ],
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                           params={'id': toy_data_url_id},\n",
        "                                           stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "\n",
        "def pre_process(dataset, filename):  # clean the dataset\n",
        "    dataset.drop(dataset.columns[0], axis=1, inplace=True)  # remove first column of dataframe containing numbers\n",
        "    dataset.drop(['ID'], axis=1, inplace=True)\n",
        "    # remove numbers before each evidence\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'^\\d+\\t', '', x))\n",
        "    # remove everything after the period\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r' \\..*', ' .', x))\n",
        "    # remove round brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LRB-.*-RRB-', '', x))\n",
        "    # remove square brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LSB-.*-RSB-', '', x))\n",
        "\n",
        "    n_before = dataset.shape[0]\n",
        "    # removes instances longer than a threshold on evidence\n",
        "    # TODO: only on train\n",
        "    dataset = dataset[dataset['Evidence'].str.split().str.len() <= 100]\n",
        "    # remove all rows where there are single brackets in the evidence\n",
        "    dataset = dataset[~dataset['Evidence'].str.contains('|'.join(['-LRB-', '-LSB-', '-RRB-', '-RSB-']))]\n",
        "    n_after = dataset.shape[0]\n",
        "\n",
        "    # removes punctuation and excessive spaces\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r' +', ' ', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'^ +', '', x))\n",
        "    dataset = dataset.applymap(lambda x: x.lower())\n",
        "\n",
        "    labels = {'supports': 1, 'refutes': 0}\n",
        "    dataset = dataset.replace({'Label': labels})\n",
        "    # removes rows with empty elements\n",
        "    dataset = dataset[dataset['Evidence'] != '']\n",
        "    dataset = dataset[dataset['Claim'] != '']\n",
        "    dataset = dataset[dataset['Label'] != '']\n",
        "\n",
        "\n",
        "\n",
        "    rem_elements = n_before - n_after\n",
        "    print(f\"Removed {rem_elements}\\t ({100 * rem_elements / n_before:.2F}%)\"\n",
        "          f\" elements because of inconsistency on {filename}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB=True\n",
        "except:\n",
        "    IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"We're running Colab\")\n",
        "    # Mount the Google Drive at mount\n",
        "    mount='/content/gdrive'\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Switch to the directory on the Google Drive that you want to use\n",
        "    drive_root = mount + \"/My Drive/NLP/Assignment2\"\n",
        "    \n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "        print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "        os.makedirs(drive_root, exist_ok=True)\n",
        "    \n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "    print(\"Checking working directory:\")\n",
        "    %pwd\n",
        "\n",
        "# download_data('dataset')\n",
        "\n",
        "if not len(os.listdir(\"dataset_cleaned\")):\n",
        "    for file in os.listdir(\"dataset\"):\n",
        "        dataset_cleaned = pre_process(pd.read_csv(\"dataset/\" + file, sep=','), file)\n",
        "        dataset_cleaned.to_csv(os.path.join(\"dataset_cleaned\", file))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57177769",
      "metadata": {
        "id": "57177769"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ba4c24ef",
      "metadata": {
        "id": "ba4c24ef"
      },
      "outputs": [],
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, dataset_sentences, embedding_dim, glove_dict, glove_matrix):\n",
        "        self.embedding_matrix = None\n",
        "        self.value_to_key = {}\n",
        "        self.value_to_key_new = {}\n",
        "        self.key_to_value = {}\n",
        "        self.num_unique_words = 0\n",
        "        self.dataset_sentences = dataset_sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.glove_dict = glove_dict\n",
        "        self.glove_matrix = glove_matrix\n",
        "        self.unique_words = set()\n",
        "\n",
        "    def get_val_to_key(self):\n",
        "        return copy.deepcopy(self.value_to_key)\n",
        "\n",
        "    def tokenize(self):\n",
        "        self.value_to_key_new = {}\n",
        "        unique_words = set()\n",
        "        for sen in self.dataset_sentences:\n",
        "            for w in sen.split():\n",
        "                unique_words.add(w)  # get se of unique words\n",
        "        new_unique = unique_words - self.unique_words\n",
        "        for i, word in enumerate(new_unique):\n",
        "            if self.embedding_matrix is not None:\n",
        "                self.key_to_value[i + len(self.embedding_matrix)] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i + len(self.embedding_matrix)\n",
        "            else:\n",
        "                self.key_to_value[i] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i\n",
        "            self.value_to_key_new[word] = i\n",
        "\n",
        "        self.num_unique_words = len(new_unique)\n",
        "        self.unique_words = self.unique_words | new_unique  # union of unique words and new unique words\n",
        "\n",
        "    def __build_embedding_matrix_glove(self):\n",
        "        oov_words = []\n",
        "        tmp_embedding_matrix = np.zeros((self.num_unique_words, self.embedding_dim)) #dtype=np.float32\n",
        "        len_old_emb_matrix = len(self.embedding_matrix) if self.embedding_matrix is not None else 0\n",
        "        for word, idx in tqdm(self.value_to_key_new.items()):\n",
        "            try:\n",
        "                embedding_vector = self.glove_matrix[self.glove_dict[word]]\n",
        "                tmp_embedding_matrix[idx] = embedding_vector\n",
        "            except (KeyError, TypeError):\n",
        "                oov_words.append((word, idx + len_old_emb_matrix))\n",
        "        \n",
        "        if self.embedding_matrix is not None:\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, tmp_embedding_matrix))\n",
        "\n",
        "        else:\n",
        "            self.embedding_matrix = copy.deepcopy(tmp_embedding_matrix)\n",
        "        return oov_words\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        oov_words = self.__build_embedding_matrix_glove()\n",
        "        for word, idx in oov_words:\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=self.embedding_dim)\n",
        "            self.embedding_matrix[idx] = embedding_vector\n",
        "        return copy.deepcopy(self.embedding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD DATA"
      ],
      "metadata": {
        "id": "1s-Jy4k_8ZmO"
      },
      "id": "1s-Jy4k_8ZmO"
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD GLOVE\n",
        "try:\n",
        "    with open(f\"glove-{EMBEDDING_SIZE}.pkl\", 'rb') as f:\n",
        "        emb_model = pickle.load(f)\n",
        "except Exception:\n",
        "    emb_model = gloader.load(f\"glove-wiki-gigaword-{EMBEDDING_SIZE}\")\n",
        "    with open(f\"glove-{EMBEDDING_SIZE}.pkl\", 'wb') as f:\n",
        "        pickle.dump(emb_model, f)\n",
        "\n",
        "glove_dict = emb_model.key_to_index\n",
        "glove_matrix = emb_model.vectors\n",
        "\n",
        "train = pd.read_csv(\"dataset_cleaned/train_pairs.csv\")\n",
        "val = pd.read_csv(\"dataset_cleaned/val_pairs.csv\")\n",
        "test = pd.read_csv(\"dataset_cleaned/test_pairs.csv\")\n",
        "\n",
        "try:\n",
        "    with open(\"emb_mat.pkl\", 'rb') as f:\n",
        "        v4_matrix = pickle.load(f)\n",
        "    with open(\"val_to_key.pkl\", 'rb') as f:\n",
        "        v4_val_to_key = pickle.load(f)\n",
        "    with open(f\"tokenizer.pkl\", 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "except Exception:\n",
        "    tokenizer = Tokenizer(train[\"Claim\"] + ' ' + train[\"Evidence\"], EMBEDDING_SIZE, glove_dict, glove_matrix)\n",
        "    tokenizer.tokenize()\n",
        "    v2_matrix = tokenizer.build_embedding_matrix()\n",
        "    tokenizer.dataset_sentences = val[\"Claim\"] + ' ' + val[\"Evidence\"]\n",
        "    tokenizer.tokenize()\n",
        "    v3_matrix = tokenizer.build_embedding_matrix()\n",
        "    tokenizer.dataset_sentences = test[\"Claim\"] + ' ' + test[\"Evidence\"]\n",
        "    tokenizer.tokenize()\n",
        "    v4_matrix = tokenizer.build_embedding_matrix()\n",
        "    v4_val_to_key = tokenizer.get_val_to_key()\n",
        "    with open(f\"emb_mat.pkl\", 'wb') as f:\n",
        "        pickle.dump(v4_matrix, f)\n",
        "    with open(f\"val_to_key.pkl\", 'wb') as f:\n",
        "        pickle.dump(v4_val_to_key, f)\n",
        "    with open(f\"tokenizer.pkl\", 'wb') as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "v4_val_to_key.update((x, y+1) for x, y in v4_val_to_key.items())\n",
        "\n",
        "\n",
        "translate_tokens = {}\n",
        "key_val_list_items = list(tokenizer.key_to_value.items())\n",
        "for i, (token, value) in enumerate(key_val_list_items):\n",
        "    if i > 0:\n",
        "        translate_tokens[token] = key_val_list_items[i-1][1]\n",
        "    else:\n",
        "        translate_tokens[i] = '<PAD>'\n"
      ],
      "metadata": {
        "id": "B5STMeqA8YWe"
      },
      "id": "B5STMeqA8YWe",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e11402a",
      "metadata": {
        "id": "0e11402a"
      },
      "source": [
        "# GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "e3a42578",
      "metadata": {
        "id": "e3a42578"
      },
      "outputs": [],
      "source": [
        "def generator(dataset, value_to_key):\n",
        "    dataset_size = dataset.shape[0]\n",
        "    dataset = dataset.to_numpy()[:, 1:]\n",
        "\n",
        "    refutes = dataset[dataset[:,2] == 0]\n",
        "    supports = dataset[dataset[:,2] == 1]\n",
        "    \n",
        "    while True:\n",
        "        X_claim = []\n",
        "        X_evid = []\n",
        "        y = []\n",
        "\n",
        "        rnd_choices_refutes = np.random.choice(np.arange(len(refutes)),replace=False, size=BATCH_SIZE//2)\n",
        "        rnd_choices_supports = np.random.choice(np.arange(len(supports)),replace=False, size=BATCH_SIZE//2)\n",
        "\n",
        "        batch = []\n",
        "        for i in range(BATCH_SIZE//2):\n",
        "            batch.append(list(refutes[rnd_choices_refutes[i]]))\n",
        "            batch.append(list(supports[rnd_choices_supports[i]]))\n",
        "        \n",
        "        random.shuffle(batch)\n",
        "\n",
        "        max_seq_claim = max([len(el[0].split()) for el in batch])\n",
        "        max_seq_evid = max([len(el[1].split()) for el in batch])\n",
        "\n",
        "        for sample in batch:\n",
        "            tokenized_claim = [value_to_key[word] for word in sample[0].split()]\n",
        "            tokenized_evid = [value_to_key[word] for word in sample[1].split()]\n",
        "\n",
        "            tmp_claim = [0] * (max_seq_claim - len(tokenized_claim)) + tokenized_claim\n",
        "            tmp_evid = [0] * (max_seq_evid - len(tokenized_evid)) + tokenized_evid\n",
        "\n",
        "            X_claim.append(tmp_claim)\n",
        "            X_evid.append(tmp_evid)\n",
        "            y.append(sample[2])\n",
        "\n",
        "        yield [np.array(X_claim), np.array(X_evid)], np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_claims, x_evids, labels = next(generator(train, v4_val_to_key))\n",
        "print(x_claims[0])\n",
        "print(x_evids[0])\n",
        "print(labels[0])\n",
        "\n",
        "print(' '.join([translate_tokens[w] for w in x_claims[0]]))\n",
        "print(' '.join([translate_tokens[w] for w in x_evids[0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj25oe65ZFGr",
        "outputId": "cc22d93a-2ccd-4b3d-a855-0f52a8a59572"
      },
      "id": "Bj25oe65ZFGr",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0     0     0 15825  4831 26594 31326 29150 11887 12273  2494 13842\n",
            " 18357 20933]\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0  2671  7472 29150   258 22095 20508 18357 23840  4831 15893\n",
            " 18357 24246 26228 11770 13848 18357 29150 21243 19299 29150 20892 29150\n",
            " 20696  7472 20334]\n",
            "0\n",
            "<PAD> <PAD> <PAD> tom cruise turned down the role he was offered in cocktail\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> one of the biggest movie stars in hollywood cruise starred in several more successful films in the 1980s including the dramas the color of money\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ],
      "metadata": {
        "id": "toiQ0Fu8YYN0"
      },
      "id": "toiQ0Fu8YYN0"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Concatenate\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, compile_info, value_to_key, embedding_dim, \n",
        "                 embedding_matrix, l2_reg):\n",
        "\n",
        "        self.compile_info = compile_info\n",
        "        self.value_to_key = value_to_key\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "# SERVONO DUE EMBEDDING IN ENTRATE, DUE LSTM UNA PER CLAIM E UNA PER EVIDENCE, CONCATENATION E DENSI \n",
        "# INPUT_LENGTH NON È NECESSARIO IN EMBEDDING PERCHÈ CAMBIA SEMPRE\n",
        "# usare la funtional api con x, y = claim, evid per poter fare concatenazione, reshape ecc\n",
        "        model = keras.models.Sequential()\n",
        "        input_claim = keras.layers.Input(shape=(None,))\n",
        "        input_evid = keras.layers.Input(shape=(None,))\n",
        "        emb_claim = layers.Embedding(input_dim=len(v4_val_to_key.keys()), #qui ci sarebbe un +1 ma da errore\n",
        "                                            output_dim=EMBEDDING_SIZE,\n",
        "                                            mask_zero=True,\n",
        "                                            weights=[v4_matrix],\n",
        "                                            trainable=False\n",
        "                                            )(input_claim)\n",
        "        emb_evid = layers.Embedding(input_dim=len(v4_val_to_key.keys()),\n",
        "                                            output_dim=EMBEDDING_SIZE,\n",
        "                                            mask_zero=True,\n",
        "                                            weights=[v4_matrix],\n",
        "                                            trainable=False\n",
        "                                            )(input_evid)\n",
        "        print(input_claim.shape)\n",
        "        lstm_claim, forward_h_claim, forward_c_claim = layers.LSTM(EMBEDDING_SIZE, return_sequences=True, kernel_regularizer=l2(self.l2_reg), return_state = True)(emb_claim) #mettere max_seq_len come num recurrents?\n",
        "        #state_h_claim = Concatenate()([forward_h_claim, backward_h_claim]) \n",
        "        print(forward_h_claim.shape)\n",
        "        # print(state_h.shape)\n",
        "        print(lstm_claim.shape)\n",
        "        #print(backward_h_claim.shape)\n",
        "        #print(backward_c_claim.shape)\n",
        "        print(forward_c_claim.shape)\n",
        "        lstm_evid, forward_h_evid, forward_c_evid = layers.LSTM(EMBEDDING_SIZE, return_sequences=True, kernel_regularizer=l2(self.l2_reg), return_state = True)(emb_evid)\n",
        "        #state_h_evid = Concatenate()([forward_h_evid, backward_h_evid])\n",
        "        merge = Concatenate()([forward_h_claim, forward_h_evid])\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(merge)\n",
        "\n",
        "        model = keras.Model(inputs=[input_claim, input_evid], outputs=outputs)\n",
        "        model.compile(**self.compile_info)\n",
        "        model.summary()\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def show_history(self, history: keras.callbacks.History):\n",
        "\n",
        "        history_data = history.history\n",
        "        print(\"Displaying the following history keys: \", history_data.keys())\n",
        "\n",
        "        for key, value in history_data.items():\n",
        "            if not key.startswith('val'):\n",
        "                fig, ax = plt.subplots(1, 1)\n",
        "                ax.set_title(key)\n",
        "                ax.plot(value)\n",
        "                if 'val_{}'.format(key) in history_data:\n",
        "                    ax.plot(history_data['val_{}'.format(key)])\n",
        "                else:\n",
        "                    print(\"Couldn't find validation values for metric: \", key)\n",
        "\n",
        "                ax.set_ylabel(key)\n",
        "                ax.set_xlabel('epoch')\n",
        "                ax.legend(['train', 'val'], loc='best')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def train_model(self,\n",
        "                  train,\n",
        "                  x_val: np.ndarray,\n",
        "                  y_val: np.ndarray,\n",
        "                  training_info: dict):\n",
        "        print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "        history = self.model.fit(x=train[0], y=train[1],\n",
        "                                  validation_data=(x_val, y_val),\n",
        "                                  shuffle=True,\n",
        "                                  **training_info)\n",
        "        print(\"Training completed! Showing history...\")\n",
        "\n",
        "        self.show_history(history)\n",
        "\n",
        "model = Model(**model_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyTdmqhCKXbD",
        "outputId": "8cc07643-6fc8-4bf0-fd72-c2e25b5886b6"
      },
      "id": "OyTdmqhCKXbD",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None)\n",
            "(None, 100)\n",
            "(None, None, 100)\n",
            "(None, 100)\n",
            "Model: \"model_22\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_74 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_75 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_61 (Embedding)       (None, None, 100)    3529400     ['input_74[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_62 (Embedding)       (None, None, 100)    3529400     ['input_75[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_56 (LSTM)                 [(None, None, 100),  80400       ['embedding_61[0][0]']           \n",
            "                                 (None, 100),                                                     \n",
            "                                 (None, 100)]                                                     \n",
            "                                                                                                  \n",
            " lstm_57 (LSTM)                 [(None, None, 100),  80400       ['embedding_62[0][0]']           \n",
            "                                 (None, 100),                                                     \n",
            "                                 (None, 100)]                                                     \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 200)          0           ['lstm_56[0][1]',                \n",
            "                                                                  'lstm_57[0][1]']                \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 1)            201         ['concatenate_41[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,219,801\n",
            "Trainable params: 161,001\n",
            "Non-trainable params: 7,058,800\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = next(generator(train, v4_val_to_key))\n",
        "print(x_train[0][0])\n",
        "print(x_train[1][0])\n",
        "print(y_train)\n",
        "x_val, y_val = next(generator(val, v4_val_to_key))\n",
        "x_test, y_test = next(generator(test, v4_val_to_key))\n",
        "\n",
        "train_gen = generator(train, v4_val_to_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtleAr856v-S",
        "outputId": "69421f41-08ea-4429-85da-6ac61327abb9"
      },
      "id": "YtleAr856v-S",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0     0     0     0     0     0     0     0 13432  7710 31126 14572\n",
            " 29042 18357 16772  7472 28798 24741]\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            " 12273 16983  3303  6688  3448 14679 18357  2579 22700  8621 26481 18357\n",
            " 16772  7472 28798 24741   162 15127 30070 24246 14869 23020 12934 28031\n",
            " 22700 20202 22700 29494 10372 18357 27398 31006 22772  2976 28589 18357\n",
            " 31727]\n",
            "[0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compile_info = {\n",
        "    'optimizer': keras.optimizers.Nadam(learning_rate=1e-2),\n",
        "    'loss': 'binary_crossentropy',\n",
        "    'metrics': ['acc']\n",
        "}\n",
        "\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': 10,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_acc', \n",
        "                                                patience=4,\n",
        "                                                restore_best_weights=True)]\n",
        "}\n",
        "\n",
        "model_params = {\n",
        "    'compile_info': compile_info,\n",
        "    'value_to_key': v4_val_to_key,\n",
        "    'embedding_dim': EMBEDDING_SIZE,\n",
        "    'embedding_matrix': np.vstack((np.zeros((1, EMBEDDING_SIZE)),\n",
        "                       v4_matrix)),\n",
        "    'l2_reg' : 1e-5\n",
        "}\n",
        "\n",
        "prediction_info = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'verbose': 1\n",
        "}\n",
        "\n",
        "TRAINING=True\n",
        "step_len = train.shape[0] // BATCH_SIZE\n",
        "if TRAINING:\n",
        "        model = Model(**model_params)\n",
        "\n",
        "\n",
        "        history = model.model.fit(train_gen, steps_per_epoch=step_len,\n",
        "                          validation_data=(x_val, y_val),\n",
        "                          **training_info)\n",
        "        print(\"Training completed! Showing history...\")\n",
        "\n",
        "        self.show_history(history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #history = model.train_model(generator(train, v4_val_to_key),\n",
        "                                    #generator(val, v4_val_to_key), training_info=training_info)\n",
        "       # model.model.save(m_name)\n",
        "      #  models.append(model.model)\n",
        "      #  histories.append(history)\n",
        "else:\n",
        "  #  models = [keras.models.load_model(m) for m in model_names]\n",
        "   pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ulSkdVWr7tXc",
        "outputId": "6fd372d2-d2fa-43a7-9372-7d491d9f6f73"
      },
      "id": "ulSkdVWr7tXc",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None)\n",
            "(None, 100)\n",
            "(None, None, 100)\n",
            "(None, 100)\n",
            "Model: \"model_27\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_84 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_85 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_71 (Embedding)       (None, None, 100)    3529400     ['input_84[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_72 (Embedding)       (None, None, 100)    3529400     ['input_85[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_66 (LSTM)                 [(None, None, 100),  80400       ['embedding_71[0][0]']           \n",
            "                                 (None, 100),                                                     \n",
            "                                 (None, 100)]                                                     \n",
            "                                                                                                  \n",
            " lstm_67 (LSTM)                 [(None, None, 100),  80400       ['embedding_72[0][0]']           \n",
            "                                 (None, 100),                                                     \n",
            "                                 (None, 100)]                                                     \n",
            "                                                                                                  \n",
            " concatenate_46 (Concatenate)   (None, 200)          0           ['lstm_66[0][1]',                \n",
            "                                                                  'lstm_67[0][1]']                \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (None, 1)            201         ['concatenate_46[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,219,801\n",
            "Trainable params: 161,001\n",
            "Non-trainable params: 7,058,800\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "  68/3784 [..............................] - ETA: 14:29 - loss: 0.6708 - acc: 0.5979"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-e7cecaeb003b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         history = model.model.fit(train_gen, steps_per_epoch=step_len,\n\u001b[1;32m     37\u001b[0m                           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                           **training_info)\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed! Showing history...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Assignment2Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}