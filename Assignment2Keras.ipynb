{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "import zipfile\n",
        "import pickle\n",
        "import gensim\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from urllib import request\n",
        "\n",
        "import collections\n",
        "import gensim.downloader as gloader\n",
        "import sklearn.metrics as sk_metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "metadata": {
        "id": "GeSdeyC06gXH"
      },
      "id": "GeSdeyC06gXH",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to use key_to_index attribute from the embedding model\n",
        "! pip install gensim==4.1.2\n",
        "import gensim\n",
        "import gensim.downloader as gloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yk9TyQj7lXf",
        "outputId": "e906d7bb-4577-4b0f-bdcb-f70004fc9420"
      },
      "id": "_yk9TyQj7lXf",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "eb78692f",
      "metadata": {
        "id": "eb78692f"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_SIZE = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "1780a936",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1780a936",
        "outputId": "6f30e147-9eaa-4ab7-cb06-66049d39c959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 16 13:47:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use. \n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "        \n",
        "fix_random(42)\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4acf8c4",
      "metadata": {
        "id": "e4acf8c4"
      },
      "source": [
        "# PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "e33cdb69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33cdb69",
        "outputId": "df76eaf7-1777-4f85-dfca-df72df3dc71f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/NLP/Assignment2  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/NLP/Assignment2\n",
            "/content/gdrive/My Drive/NLP/Assignment2\n",
            "Checking working directory:\n"
          ]
        }
      ],
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                           params={'id': toy_data_url_id},\n",
        "                                           stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "\n",
        "def pre_process(dataset, filename):  # clean the dataset\n",
        "    dataset.drop(dataset.columns[0], axis=1, inplace=True)  # remove first column of dataframe containing numbers\n",
        "    dataset.drop(['ID'], axis=1, inplace=True)\n",
        "    # remove numbers before each evidence\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'^\\d+\\t', '', x))\n",
        "    # remove everything after the period\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r' \\..*', ' .', x))\n",
        "    # remove round brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LRB-.*-RRB-', '', x))\n",
        "    # remove square brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LSB-.*-RSB-', '', x))\n",
        "\n",
        "    n_before = dataset.shape[0]\n",
        "    # removes instances longer than a threshold on evidence\n",
        "    # TODO: only on train\n",
        "    dataset = dataset[dataset['Evidence'].str.split().str.len() <= 100]\n",
        "    # remove all rows where there are single brackets in the evidence\n",
        "    dataset = dataset[~dataset['Evidence'].str.contains('|'.join(['-LRB-', '-LSB-', '-RRB-', '-RSB-']))]\n",
        "    n_after = dataset.shape[0]\n",
        "\n",
        "    # removes punctuation and excessive spaces\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r' +', ' ', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'^ +', '', x))\n",
        "    dataset = dataset.applymap(lambda x: x.lower())\n",
        "\n",
        "    labels = {'supports': 1, 'refutes': 0}\n",
        "    dataset = dataset.replace({'Label': labels})\n",
        "    # removes rows with empty elements\n",
        "    dataset = dataset[dataset['Evidence'] != '']\n",
        "    dataset = dataset[dataset['Claim'] != '']\n",
        "    dataset = dataset[dataset['Label'] != '']\n",
        "\n",
        "\n",
        "\n",
        "    rem_elements = n_before - n_after\n",
        "    print(f\"Removed {rem_elements}\\t ({100 * rem_elements / n_before:.2F}%)\"\n",
        "          f\" elements because of inconsistency on {filename}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB=True\n",
        "except:\n",
        "    IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"We're running Colab\")\n",
        "    # Mount the Google Drive at mount\n",
        "    mount='/content/gdrive'\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Switch to the directory on the Google Drive that you want to use\n",
        "    drive_root = mount + \"/My Drive/NLP/Assignment2\"\n",
        "    \n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "        print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "        os.makedirs(drive_root, exist_ok=True)\n",
        "    \n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "    print(\"Checking working directory:\")\n",
        "    %pwd\n",
        "\n",
        "# download_data('dataset')\n",
        "\n",
        "if not len(os.listdir(\"dataset_cleaned\")):\n",
        "    for file in os.listdir(\"dataset\"):\n",
        "        dataset_cleaned = pre_process(pd.read_csv(\"dataset/\" + file, sep=','), file)\n",
        "        dataset_cleaned.to_csv(os.path.join(\"dataset_cleaned\", file))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57177769",
      "metadata": {
        "id": "57177769"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "ba4c24ef",
      "metadata": {
        "id": "ba4c24ef"
      },
      "outputs": [],
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, dataset_sentences, embedding_dim, glove_dict, glove_matrix):\n",
        "        self.embedding_matrix = None\n",
        "        self.value_to_key = {}\n",
        "        self.value_to_key_new = {}\n",
        "        self.key_to_value = {}\n",
        "        self.num_unique_words = 0\n",
        "        self.dataset_sentences = dataset_sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.glove_dict = glove_dict\n",
        "        self.glove_matrix = glove_matrix\n",
        "        self.unique_words = set()\n",
        "\n",
        "    def get_val_to_key(self):\n",
        "        return copy.deepcopy(self.value_to_key)\n",
        "\n",
        "    def tokenize(self):\n",
        "        self.value_to_key_new = {}\n",
        "        unique_words = set()\n",
        "        for sen in self.dataset_sentences:\n",
        "            for w in sen.split():\n",
        "                unique_words.add(w)  # get se of unique words\n",
        "        new_unique = unique_words - self.unique_words\n",
        "        for i, word in enumerate(new_unique):\n",
        "            if self.embedding_matrix is not None:\n",
        "                self.key_to_value[i + len(self.embedding_matrix)] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i + len(self.embedding_matrix)\n",
        "            else:\n",
        "                self.key_to_value[i] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i\n",
        "            self.value_to_key_new[word] = i\n",
        "\n",
        "        self.num_unique_words = len(new_unique)\n",
        "        self.unique_words = self.unique_words | new_unique  # union of unique words and new unique words\n",
        "\n",
        "    def __build_embedding_matrix_glove(self):\n",
        "        oov_words = []\n",
        "        tmp_embedding_matrix = np.zeros((self.num_unique_words, self.embedding_dim)) #dtype=np.float32\n",
        "        len_old_emb_matrix = len(self.embedding_matrix) if self.embedding_matrix is not None else 0\n",
        "        for word, idx in tqdm(self.value_to_key_new.items()):\n",
        "            try:\n",
        "                embedding_vector = self.glove_matrix[self.glove_dict[word]]\n",
        "                tmp_embedding_matrix[idx] = embedding_vector\n",
        "            except (KeyError, TypeError):\n",
        "                oov_words.append((word, idx + len_old_emb_matrix))\n",
        "        \n",
        "        if self.embedding_matrix is not None:\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, tmp_embedding_matrix))\n",
        "\n",
        "        else:\n",
        "            self.embedding_matrix = copy.deepcopy(tmp_embedding_matrix)\n",
        "        return oov_words\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        oov_words = self.__build_embedding_matrix_glove()\n",
        "        for word, idx in oov_words:\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=self.embedding_dim)\n",
        "            self.embedding_matrix[idx] = embedding_vector\n",
        "        return copy.deepcopy(self.embedding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD DATA"
      ],
      "metadata": {
        "id": "1s-Jy4k_8ZmO"
      },
      "id": "1s-Jy4k_8ZmO"
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD GLOVE\n",
        "try:\n",
        "    with open(f\"glove-{EMBEDDING_SIZE}.pkl\", 'rb') as f:\n",
        "        emb_model = pickle.load(f)\n",
        "except Exception:\n",
        "    emb_model = gloader.load(f\"glove-wiki-gigaword-{EMBEDDING_SIZE}\")\n",
        "    with open(f\"glove-{EMBEDDING_SIZE}.pkl\", 'wb') as f:\n",
        "        pickle.dump(emb_model, f)\n",
        "\n",
        "glove_dict = emb_model.key_to_index\n",
        "glove_matrix = emb_model.vectors\n",
        "\n",
        "train = pd.read_csv(\"dataset_cleaned/train_pairs.csv\")\n",
        "val = pd.read_csv(\"dataset_cleaned/val_pairs.csv\")\n",
        "test = pd.read_csv(\"dataset_cleaned/test_pairs.csv\")\n",
        "\n",
        "try:\n",
        "    with open(\"emb_mat.pkl\", 'rb') as f:\n",
        "        v4_matrix = pickle.load(f)\n",
        "    with open(\"val_to_key.pkl\", 'rb') as f:\n",
        "        v4_val_to_key = pickle.load(f)\n",
        "    with open(f\"tokenizer.pkl\", 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "except Exception:\n",
        "    tokenizer = Tokenizer(train[\"Claim\"] + ' ' + train[\"Evidence\"], EMBEDDING_SIZE, glove_dict, glove_matrix)\n",
        "    tokenizer.tokenize()\n",
        "    v2_matrix = tokenizer.build_embedding_matrix()\n",
        "    tokenizer.dataset_sentences = val[\"Claim\"] + ' ' + val[\"Evidence\"]\n",
        "    tokenizer.tokenize()\n",
        "    v3_matrix = tokenizer.build_embedding_matrix()\n",
        "    tokenizer.dataset_sentences = test[\"Claim\"] + ' ' + test[\"Evidence\"]\n",
        "    tokenizer.tokenize()\n",
        "    v4_matrix = tokenizer.build_embedding_matrix()\n",
        "    v4_val_to_key = tokenizer.get_val_to_key()\n",
        "    with open(f\"emb_mat.pkl\", 'wb') as f:\n",
        "        pickle.dump(v4_matrix, f)\n",
        "    with open(f\"val_to_key.pkl\", 'wb') as f:\n",
        "        pickle.dump(v4_val_to_key, f)\n",
        "    with open(f\"tokenizer.pkl\", 'wb') as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "v4_val_to_key.update((x, y+1) for x, y in v4_val_to_key.items())\n",
        "\n",
        "\n",
        "translate_tokens = {}\n",
        "key_val_list_items = list(tokenizer.key_to_value.items())\n",
        "for i, (token, value) in enumerate(key_val_list_items):\n",
        "    if i > 0:\n",
        "        translate_tokens[token] = key_val_list_items[i-1][1]\n",
        "    else:\n",
        "        translate_tokens[i] = '<PAD>'\n"
      ],
      "metadata": {
        "id": "B5STMeqA8YWe"
      },
      "id": "B5STMeqA8YWe",
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e11402a",
      "metadata": {
        "id": "0e11402a"
      },
      "source": [
        "# GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "e3a42578",
      "metadata": {
        "id": "e3a42578"
      },
      "outputs": [],
      "source": [
        "def generator(dataset, value_to_key):\n",
        "    dataset_size = dataset.shape[0]\n",
        "    dataset = dataset.to_numpy()[:, 1:]\n",
        "\n",
        "    refutes = dataset[dataset[:,2] == 0]\n",
        "    supports = dataset[dataset[:,2] == 1]\n",
        "    \n",
        "    while True:\n",
        "        X_claim = []\n",
        "        X_evid = []\n",
        "        y = []\n",
        "\n",
        "        rnd_choices_refutes = np.random.choice(np.arange(len(refutes)),replace=False, size=BATCH_SIZE//2)\n",
        "        rnd_choices_supports = np.random.choice(np.arange(len(supports)),replace=False, size=BATCH_SIZE//2)\n",
        "\n",
        "        batch = []\n",
        "        for i in range(BATCH_SIZE//2):\n",
        "            batch.append(list(refutes[rnd_choices_refutes[i]]))\n",
        "            batch.append(list(supports[rnd_choices_supports[i]]))\n",
        "        \n",
        "        random.shuffle(batch)\n",
        "\n",
        "        max_seq_claim = max([len(el[0].split()) for el in batch])\n",
        "        max_seq_evid = max([len(el[1].split()) for el in batch])\n",
        "\n",
        "        for sample in batch:\n",
        "            tokenized_claim = [value_to_key[word] for word in sample[0].split()]\n",
        "            tokenized_evid = [value_to_key[word] for word in sample[1].split()]\n",
        "\n",
        "            tmp_claim = [0] * (max_seq_claim - len(tokenized_claim)) + tokenized_claim\n",
        "            tmp_evid = [0] * (max_seq_evid - len(tokenized_evid)) + tokenized_evid\n",
        "\n",
        "            X_claim.append(tmp_claim)\n",
        "            X_evid.append(tmp_evid)\n",
        "            y.append(sample[2])\n",
        "\n",
        "        yield np.array(X_claim), np.array(X_evid), np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_claims, x_evids, labels = next(generator(train, v4_val_to_key))\n",
        "print(x_claims[0])\n",
        "print(x_evids[0])\n",
        "print(labels[0])\n",
        "\n",
        "print(' '.join([translate_tokens[w] for w in x_claims[0]]))\n",
        "print(' '.join([translate_tokens[w] for w in x_evids[0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj25oe65ZFGr",
        "outputId": "67f6c3e7-6feb-425e-b17e-752e0d7b592a"
      },
      "id": "Bj25oe65ZFGr",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0   846 14893\n",
            " 16775 16273  5973]\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0 31253  9644 19473 26566 17326 23157  2522 27098 31421 13888  6423\n",
            " 24270 21346 12272  6861 13898 31035 17027  1759 24397  9644 17338 12191\n",
            "  2522 16273 13627  6170]\n",
            "1\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> there are people in ontario\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> it is canada s most populous province by a large margin accounting for nearly 40 percent of all canadians and is the secondlargest province in total area\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ],
      "metadata": {
        "id": "toiQ0Fu8YYN0"
      },
      "id": "toiQ0Fu8YYN0"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SERVONO DUE EMBEDDING IN ENTRATE, DUE LSTM UNA PER CLAIM E UNA PER EVIDENCE, CONCATENATION E DENSI \n",
        "# INPUT_LENGTH NON È NECESSARIO IN EMBEDDING PERCHÈ CAMBIA SEMPRE\n",
        "# usare la funtional api con x, y = claim, evid per poter fare concatenazione, reshape ecc\n",
        "\n",
        "inputs = keras.layers.Input(shape=(None,))\n",
        "claims = layers.Embedding(input_dim=len(v4_val_to_key.keys()) + 1,\n",
        "                                    output_dim=EMBEDDING_SIZE,\n",
        "                                    mask_zero=True,\n",
        "                                    weights=[v4_matrix],\n",
        "                                    trainable=False\n",
        "                                    )(inputs[0])\n",
        "evids = layers.Embedding(input_dim=len(v4_val_to_key.keys()) + 1,\n",
        "                                    output_dim=EMBEDDING_SIZE,\n",
        "                                    mask_zero=True,\n",
        "                                    weights=[v4_matrix],\n",
        "                                    trainable=False\n",
        "                                    )(inputs[1])\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(self.num_recurrent_units, return_sequences=True, kernel_regularizer=l2(self.l2_reg)))(x)\n",
        "outputs = layers.TimeDistributed(layers.Dense(self.num_labels, activation=\"softmax\"))(x)\n",
        "\n",
        "\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(**self.compile_info)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "OyTdmqhCKXbD",
        "outputId": "65837d75-3657-46bd-e1b9-c2e4f4ba99b7"
      },
      "id": "OyTdmqhCKXbD",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ec359c02a593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m x = layers.Embedding(input_dim=len(value_to_key.keys()) + 1,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                     \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mmask_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'value_to_key' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PdbP1O1eYRkV"
      },
      "id": "PdbP1O1eYRkV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Assignment2Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}