{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2FactChecking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGoJUhHIxywn"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6CoTN8vxzKx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import requests\n",
        "import zipfile\n",
        "import pickle\n",
        "import gensim\n",
        "import random\n",
        "import gensim.downloader as gloader\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to use key_to_index attribute from the embedding model\n",
        "! pip install gensim==4.1.2\n",
        "import gensim\n",
        "import gensim.downloader as gloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8U2Aq6FCgj-",
        "outputId": "94703b3c-3358-41e1-ca67-83dd0d5e8479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fhHGHAk10EH"
      },
      "source": [
        "EMBEDDING_DIMENSION = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS = 3\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYwOT8L87ORu",
        "outputId": "53c5f9b1-26ae-474c-f166-21fac1a5fb97"
      },
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use. \n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True        \n",
        "    \n",
        "fix_random(42)\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 15 09:13:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk5ohYbXyO69"
      },
      "source": [
        "# PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC-BPPk4yOuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a5f7bb-2da8-458b-fefd-fdec50a0f717"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                           params={'id': toy_data_url_id},\n",
        "                                           stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "\n",
        "def pre_process(dataset, filename):  # clean the dataset\n",
        "    dataset.drop(dataset.columns[0], axis=1, inplace=True)  # remove first column of dataframe containing numbers\n",
        "    dataset.drop(['ID'], axis=1, inplace=True)\n",
        "    # remove numbers before each evidence\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'^\\d+\\t', '', x))\n",
        "    # remove everything after the period\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r' \\..*', ' .', x))\n",
        "    # remove round brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LRB-.*-RRB-', '', x))\n",
        "    # remove square brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LSB-.*-RSB-', '', x))\n",
        "\n",
        "    n_before = dataset.shape[0]\n",
        "    # removes instances longer than a threshold on evidence\n",
        "    # TODO: only on train\n",
        "    dataset = dataset[dataset['Evidence'].str.split().str.len() <= 100]\n",
        "    # remove all rows where there are single brackets in the evidence\n",
        "    dataset = dataset[~dataset['Evidence'].str.contains('|'.join(['-LRB-', '-LSB-', '-RRB-', '-RSB-']))]\n",
        "    n_after = dataset.shape[0]\n",
        "\n",
        "    # removes punctuation and excessive spaces\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r' +', ' ', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'^ +', '', x))\n",
        "    dataset = dataset.applymap(lambda x: x.lower())\n",
        "\n",
        "    labels = {'supports': 1, 'refutes': 0}\n",
        "    dataset = dataset.replace({'Label': labels})\n",
        "    # removes rows with empty elements\n",
        "    dataset = dataset[dataset['Evidence'] != '']\n",
        "    dataset = dataset[dataset['Claim'] != '']\n",
        "    dataset = dataset[dataset['Label'] != '']\n",
        "\n",
        "\n",
        "\n",
        "    rem_elements = n_before - n_after\n",
        "    print(f\"Removed {rem_elements}\\t ({100 * rem_elements / n_before:.2F}%)\"\n",
        "          f\" elements because of inconsistency on {filename}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB=True\n",
        "except:\n",
        "    IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"We're running Colab\")\n",
        "    # Mount the Google Drive at mount\n",
        "    mount='/content/gdrive'\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Switch to the directory on the Google Drive that you want to use\n",
        "    drive_root = mount + \"/My Drive/NLP/Assignment2\"\n",
        "    \n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "        print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "        os.makedirs(drive_root, exist_ok=True)\n",
        "    \n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "    print(\"Checking working directory:\")\n",
        "    %pwd\n",
        "\n",
        "# download_data('dataset')\n",
        "\n",
        "if not len(os.listdir(\"dataset_cleaned\")):\n",
        "    for file in os.listdir(\"dataset\"):\n",
        "        dataset_cleaned = pre_process(pd.read_csv(\"dataset/\" + file, sep=','), file)\n",
        "        dataset_cleaned.to_csv(os.path.join(\"dataset_cleaned\", file))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/NLP/Assignment2  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/NLP/Assignment2\n",
            "/content/gdrive/My Drive/NLP/Assignment2\n",
            "Checking working directory:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGgYfXJqyD4n"
      },
      "source": [
        "# CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkUKdAkSyHvT"
      },
      "source": [
        "## FactDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPOc3HdsvyDI"
      },
      "source": [
        "class FactDataset(Dataset):\n",
        "    def __init__(self, emb_dim, glove_dict, glove_matrix, split='train', tokenizer=None):\n",
        "        pairs_claim_evid = pd.read_csv(f\"dataset_cleaned/{split}_pairs.csv\")\n",
        "        # tokenization & embeddings\n",
        "        text = pairs_claim_evid['Claim'] + ' ' + pairs_claim_evid['Evidence']\n",
        "        text = text.astype(str).to_list()\n",
        "\n",
        "        # IMPORTANT: if a tokenizer is already computed for example on the train set, for val set we have to extend\n",
        "        if tokenizer:\n",
        "            self.tokenizer = tokenizer\n",
        "            self.tokenizer.dataset_sentences = text\n",
        "        else:\n",
        "            self.tokenizer = Tokenizer(text, emb_dim, glove_dict, glove_matrix)\n",
        "\n",
        "        self.tokenizer.tokenize()\n",
        "        self.val_to_key = self.tokenizer.get_val_to_key()\n",
        "        self.emb_matrix = self.tokenizer.build_embedding_matrix()\n",
        "        print(len(self.emb_matrix))\n",
        "\n",
        "        lengths = pairs_claim_evid['Evidence'].str.split().str.len().to_numpy()\n",
        "\n",
        "        # print(len([i for i, el in enumerate(lengths) if el > 100]))\n",
        "\n",
        "        self.max_claim_len = int(pairs_claim_evid['Claim'].str.split().str.len().max())\n",
        "        self.max_evid_len = int(pairs_claim_evid['Evidence'].str.split().str.len().max())\n",
        "        self.max_seq_len = int(max(self.max_evid_len, self.max_claim_len))\n",
        "\n",
        "        # tester function\n",
        "        if not self._check_tokenizer():\n",
        "            raise ValueError\n",
        "\n",
        "        # create x as list of sentences\n",
        "        # sentences are splitted in claim evidence\n",
        "        self.x = []\n",
        "        print(f\"Creating FactDataset: \\n\")\n",
        "        for i, (claim_sen, evid_sen) in tqdm(enumerate(zip(pairs_claim_evid['Claim'], pairs_claim_evid['Evidence']))):\n",
        "            sentence = [[], []]\n",
        "            for word in claim_sen.split():\n",
        "                sentence[0].append(self.val_to_key[word])\n",
        "            for word in evid_sen.split():\n",
        "                sentence[1].append(self.val_to_key[word])\n",
        "\n",
        "            # Padding deprecated here\n",
        "            # sentence[0] = sentence[0] + [0] * (self.max_seq_len - len(claim_sen.split()))\n",
        "            # sentence[1] = sentence[1] + [0] * (self.max_seq_len - len(evid_sen.split()))\n",
        "            self.x.append(sentence)\n",
        "\n",
        "         \n",
        "        # self.x = torch.tensor(self.x)\n",
        "        # self.y = torch.tensor(pairs_claim_evid['Label'])\n",
        "\n",
        "        # one-hot: [0,1] => supports, [1,0] => refutes\n",
        "        self.y = [[0, 1] if l==1 else [1, 0] for l in pairs_claim_evid['Label'].to_list()]\n",
        "        self.n_samples = pairs_claim_evid.shape[0]\n",
        "        print(f\"Max sequence length: {self.max_seq_len}\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # xi = [[claim], [evid]], yi = [label]\n",
        "        return [self.x[index], self.y[index]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def _check_tokenizer(self):\n",
        "        word = 'the'\n",
        "        for i, el in enumerate(self.tokenizer.glove_matrix[self.tokenizer.glove_dict[word]]):\n",
        "            if el != self.emb_matrix[self.val_to_key[word]][i]:\n",
        "                print(\"Check Tokenizer, possible bugs\")\n",
        "                return False\n",
        "        return True\n",
        "    def get_tokenizer(self):\n",
        "        return self.tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJHDwAfYyK_X"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mHt8SIP1WY_"
      },
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, dataset_sentences, embedding_dim, glove_dict, glove_matrix):\n",
        "        self.embedding_matrix = None\n",
        "        self.value_to_key = {}\n",
        "        self.value_to_key_new = {}\n",
        "        self.key_to_value = {}\n",
        "        self.num_unique_words = 0\n",
        "        self.dataset_sentences = dataset_sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.glove_dict = glove_dict\n",
        "        self.glove_matrix = glove_matrix\n",
        "        self.unique_words = set()\n",
        "\n",
        "    def get_val_to_key(self):\n",
        "        return copy.deepcopy(self.value_to_key)\n",
        "\n",
        "    def tokenize(self):\n",
        "        self.value_to_key_new = {}\n",
        "        unique_words = set()\n",
        "        for sen in self.dataset_sentences:\n",
        "            for w in sen.split():\n",
        "                unique_words.add(w)  # get set of unique words\n",
        "        new_unique = unique_words - self.unique_words\n",
        "        for i, word in enumerate(new_unique):\n",
        "            if self.embedding_matrix is not None:\n",
        "                self.key_to_value[i + len(self.embedding_matrix)] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i + len(self.embedding_matrix)\n",
        "            else:\n",
        "                self.key_to_value[i] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i\n",
        "            self.value_to_key_new[word] = i\n",
        "\n",
        "        self.num_unique_words = len(new_unique)\n",
        "        self.unique_words = self.unique_words | new_unique  # union of unique words and new unique words\n",
        "\n",
        "\n",
        "    def __build_embedding_matrix_glove(self):\n",
        "        oov_words = []\n",
        "        tmp_embedding_matrix = np.zeros((self.num_unique_words, self.embedding_dim), dtype=np.float32)\n",
        "        len_old_emb_matrix = len(self.embedding_matrix) if self.embedding_matrix is not None else 0\n",
        "        print(f\"Finding OOVs: \")\n",
        "        for word, idx in tqdm(self.value_to_key_new.items()):\n",
        "            try:\n",
        "                embedding_vector = self.glove_matrix[self.glove_dict[word]]\n",
        "                tmp_embedding_matrix[idx] = embedding_vector\n",
        "            except (KeyError, TypeError):\n",
        "                oov_words.append((word, idx + len_old_emb_matrix))\n",
        "        if self.embedding_matrix is not None:\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, tmp_embedding_matrix))\n",
        "        else:\n",
        "            self.embedding_matrix = tmp_embedding_matrix\n",
        "        return oov_words\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        oov_words = self.__build_embedding_matrix_glove()\n",
        "        print(f\"Solving OOVs: \")\n",
        "        for word, idx in tqdm(oov_words):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=self.embedding_dim)\n",
        "            self.embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "        # PADDING (feat. David Guetta)\n",
        "        first = self.embedding_matrix[0]\n",
        "        if np.count_nonzero(first) != 0:\n",
        "            first = self.embedding_matrix[0]\n",
        "            self.embedding_matrix[0] = np.zeros(self.embedding_dim)\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, first))\n",
        "\n",
        "            word_to_change = min(self.value_to_key.items(), key=lambda x: x[1])[0]\n",
        "            self.key_to_value[0] = '<PAD>'\n",
        "            self.value_to_key['<PAD>'] = 0\n",
        "            self.value_to_key[word_to_change] = len(self.embedding_matrix) - 1\n",
        "            self.key_to_value[len(self.embedding_matrix) - 1] = word_to_change\n",
        "\n",
        "        return copy.deepcopy(self.embedding_matrix)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERVX9LTl1Y5O"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuZv5Omm1gA_"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, embedding_dim, output_dim, pre_trained_emb,\n",
        "                 merging_type='concatenation', sentence_type='last_state'):\n",
        "        super().__init__()\n",
        "        self.merging_type = merging_type\n",
        "        self.sentence_type = sentence_type\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings=pre_trained_emb, freeze=True, padding_idx=0)\n",
        "        self.rnn_claim = nn.RNN(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
        "        self.rnn_evid = nn.RNN(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(embedding_dim * (2 if merging_type == 'concatenation' else 1), 2)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.i = 0\n",
        "        # Find a good way to initialize\n",
        "        # torch.nn.init.uniform_(self.fc.weight, a=0.49, b=0.51)\n",
        "\n",
        "    def forward(self, claim, evid):\n",
        "        # claim = [max_tok, emb_dim]\n",
        "        # evid = [max_tok, emb_dim]\n",
        "        # self.i+=1\n",
        "\n",
        "        # [print(' '.join([str(el.cpu().item()) for el in claim[i]])) for i in range(32)]\n",
        "\n",
        "        seq_lengths_claim = torch.tensor(list(map(torch.count_nonzero, claim)))\n",
        "        seq_lengths_evid = torch.tensor(list(map(torch.count_nonzero, evid)))\n",
        "        \n",
        "        seq_lengths_claim, perm_idx_claim = seq_lengths_claim.sort(0, descending=True)\n",
        "        seq_lengths_evid, perm_idx_evid = seq_lengths_evid.sort(0, descending=True)\n",
        "\n",
        "        # print(seq_lengths_claim)\n",
        "\n",
        "        seq_claim = claim[perm_idx_claim]\n",
        "        seq_evid = evid[perm_idx_evid]\n",
        "\n",
        "        emb_claim = self.embedding(seq_claim.long())# [batch_size, max_len_in_batch, 50]\n",
        "        emb_evid = self.embedding(seq_evid.long())# [batch_size, max_len_in_batch, 50]\n",
        "\n",
        "        # print(emb_claim[5, -1])\n",
        "\n",
        "        packed_claim = pack_padded_sequence(emb_claim, seq_lengths_claim, batch_first=True)\n",
        "        packed_evid = pack_padded_sequence(emb_evid, seq_lengths_evid, batch_first=True)\n",
        "\n",
        "        output_claim, hidden_claim = self.rnn_claim(packed_claim) \n",
        "        output_evid, hidden_evid = self.rnn_evid(packed_evid) \n",
        "\n",
        "        output_claim_unpack, claim_lens_unpacked = pad_packed_sequence(output_claim, batch_first=True)\n",
        "        output_evid_unpack, evid_lens_unpacked = pad_packed_sequence(output_evid, batch_first=True)\n",
        "\n",
        "\n",
        "        # print()\n",
        "        # print(claim_lens_unpacked)\n",
        "        # [print(output_claim_unpack[0, i]) for i in range(17)]\n",
        "        # print(output_claim_unpack.size())\n",
        "        # print(output_claim_idxs_first_zero)\n",
        "\n",
        "\n",
        "        # RIGUARDARE DA QUI IL MERGE 1 E 2\n",
        "        if self.sentence_type == 'last_state':\n",
        "            merge_input1 = output_claim_unpack[:, claim_lens_unpacked-1] # [batch_size, EMB_DIM]\n",
        "            merge_input2 = output_claim_unpack[:, claim_lens_unpacked-1] # [batch_size, EMB_DIM]\n",
        "            \n",
        "            print(\"\\n CLAIM RNN\", merge_input1)\n",
        "            print(merge_input1.size())\n",
        "            print(\"\\n EVID RNN\", merge_input2)\n",
        "\n",
        "        elif self.sentence_type == 'average_outputs':\n",
        "            # recheck shapes on this, not yet tested\n",
        "            merge_input1 = torch.mean(output_claim)\n",
        "            merge_input2 = torch.mean(output_evid)\n",
        "        elif self.sentence_type == 'simple_mlp':\n",
        "            raise NotImplementedError\n",
        "        else: # mean of tokens\n",
        "            raise NotImplementedError\n",
        "\n",
        "        if self.merging_type == 'concatenation':\n",
        "            merge = torch.cat([merge_input1, merge_input2], 1) # [batch_size, EMB_DIM*2]\n",
        "            # print(\"\\n MERGE\", merge)\n",
        "            # if self.i == 30:\n",
        "            #     raise\n",
        "        elif self.merging_type == 'sum':\n",
        "            merge = torch.sum([merge_input1, merge_input2], dim=0) # [batch_size, EMB_DIM]\n",
        "        else:\n",
        "            merge = torch.mean([merge_input1, merge_input2], dim=0)  # [batch_size, EMB_DIM]\n",
        "\n",
        "        dense = self.fc(merge)\n",
        "        # print(\"SOFTMAX:\", self.softmax(dense))\n",
        "        # print(\"############################### NEXT BATCH ###########################\")\n",
        "        return self.softmax(dense)\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"Model with '{self.sentence_type}' \"\\\n",
        "               f\"sentence embeddings and '{self.merging_type}' merging type.\""
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh5uBlVUzsO8"
      },
      "source": [
        "### Model functions for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7eOK7wizqsl"
      },
      "source": [
        "##### General functions for Models #####\n",
        "\n",
        "def progress_bar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
        "    \"\"\"\n",
        "    credits to: https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total: \n",
        "        print()\n",
        "\n",
        "def train_epoch(model:nn.Module,\n",
        "                iterator_train: torch.utils.data.DataLoader,\n",
        "                device: torch.device,\n",
        "                optimizer: torch.optim,\n",
        "                loss,\n",
        "                epoch,\n",
        "                epochs):\n",
        "\n",
        "    accuracy_epoch = 0\n",
        "    loss_epoch = 0\n",
        "    num_batches = len(iterator_train)\n",
        "\n",
        "    progress_bar(0, num_batches, prefix='', suffix='Initializing', length=20, printEnd='')\n",
        "\n",
        "    \n",
        "    model.train()\n",
        "    for i, batch in enumerate(iterator_train):\n",
        "\n",
        "        # print(len(batch[0]))\n",
        "        # raise\n",
        "\n",
        "        labels = batch[1].to(torch.float32) \n",
        "        claims = batch[0][0].to(torch.float32)\n",
        "        evids = batch[0][1].to(torch.float32)\n",
        "\n",
        "        # if i==0:\n",
        "        #     print()\n",
        "        #     print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claims[0]])) # claim\n",
        "        #     print(' '.join([test.tokenizer.key_to_value[el.item()] for el in evids[0]])) # evid\n",
        "        #     print('supports' if labels[0].item() == 1 else 'refutes') # label\n",
        "        #     raise\n",
        "\n",
        "        predictions = model(claims, evids) # claim, evid as inputs\n",
        "        \n",
        "        # REMEMBER: round to nearest integer for dense and softmax\n",
        "        accuracy_step = binary_accuracy(torch.round(predictions), labels)\n",
        "        accuracy_epoch += accuracy_step\n",
        "\n",
        "        # if i <= 30:\n",
        "        #     print()\n",
        "        #     print(\"PREDS: \", predictions)\n",
        "        #     print(\"TRUES: \", labels)\n",
        "\n",
        "        # if i == 30:\n",
        "        #     raise\n",
        "\n",
        "        loss_step = loss(predictions, labels)\n",
        "        loss_epoch += loss_step.item()\n",
        "\n",
        "        loss_step.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % (num_batches // 20) == 0:\n",
        "            train_info_str = f\"-- Epoch: {epoch}/{epochs} \"\\\n",
        "                                f\"-- Step: {i + 1}/{num_batches} \"\\\n",
        "                                f\"-- Acc: {accuracy_step:.2F} \"\\\n",
        "                                f\"-- Loss: {loss_step.item():.2F}\"\n",
        "\n",
        "            progress_bar(i + 1, num_batches, prefix='', suffix=train_info_str, length=20, printEnd='')\n",
        "            \n",
        "    loss_epoch /= num_batches\n",
        "    accuracy_epoch /= num_batches\n",
        "    return loss_step.item(), accuracy_step\n",
        "\n",
        "def validate_epoch(model:nn.Module,\n",
        "                    iterator_val: torch.utils.data.DataLoader,\n",
        "                    device: torch.device,\n",
        "                    optimizer: torch.optim,\n",
        "                    loss,\n",
        "                    epoch):\n",
        "\n",
        "    accuracy_epoch = 0\n",
        "    loss_epoch = 0\n",
        "    num_batches = len(iterator_val)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator_val):\n",
        "            labels = batch[1].to(torch.float32) \n",
        "            claims = batch[0][0].to(torch.float32)\n",
        "            evids = batch[0][1].to(torch.float32)\n",
        "\n",
        "            predictions = model(claims, evids) # claim, evid as inputs        \n",
        "            \n",
        "            # REMEMBER: round to nearest integer for dense and argmax for softmax\n",
        "            accuracy_step = binary_accuracy(torch.round(predictions), labels)\n",
        "            accuracy_epoch += accuracy_step\n",
        "\n",
        "            loss_step = loss(predictions, labels)\n",
        "            loss_epoch += loss_step.item()\n",
        "\n",
        "    loss_epoch /= num_batches\n",
        "    accuracy_epoch /= num_batches\n",
        "\n",
        "    return loss_step.item(), accuracy_step\n",
        "    \n",
        "\n",
        "def train_model(model, \n",
        "                epochs, \n",
        "                batch_size, \n",
        "                iterator_train, \n",
        "                iterator_validation, \n",
        "                optimizer, \n",
        "                loss, \n",
        "                num_train_samples, \n",
        "                device):\n",
        "    model.to(device)\n",
        "    num_iters = num_train_samples // batch_size\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train_epoch(model, iterator_train, device, optimizer, loss, epoch, epochs)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc = validate_epoch(model, iterator_validation, device, optimizer, loss, epoch)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        epoch_info_str = f\"-- Epoch: {epoch}/{epochs} \"\\\n",
        "                         f\"-- Step: {num_iters}/{num_iters} \"\\\n",
        "                         f\"-- Acc: {train_acc:.2F} \"\\\n",
        "                         f\"-- Loss: {train_loss:.2F} \"\\\n",
        "                         f\"-- Acc_Val: {val_acc:.2F} \"\\\n",
        "                         f\"-- Loss_Val: {val_loss:.2F}\"\n",
        "        progress_bar(num_iters, num_iters, prefix='', suffix=epoch_info_str, length=20, printEnd='')\n",
        "        \n",
        "    return train_losses, train_accs, val_losses, val_accs\n",
        "\n",
        "\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    correct = [torch.equal(pred, target) for pred, target in zip(preds, y)]\n",
        "    acc = sum(correct) / len(y)\n",
        "    return acc\n"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWCd1T3Y2MVE"
      },
      "source": [
        "# LOADING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQMjEAKg1tEj"
      },
      "source": [
        "def load_dataset():\n",
        "    # LOAD GLOVE\n",
        "    try:\n",
        "        with open(f\"glove-{EMBEDDING_DIMENSION}.pkl\", 'rb') as f:\n",
        "            emb_model = pickle.load(f)\n",
        "    except Exception:\n",
        "        emb_model = gloader.load(f\"glove-wiki-gigaword-{EMBEDDING_DIMENSION}\")\n",
        "        with open(f\"glove-{EMBEDDING_DIMENSION}.pkl\", 'wb') as f:\n",
        "            pickle.dump(emb_model, f)\n",
        "\n",
        "    glove_dict = emb_model.key_to_index\n",
        "    glove_matrix = emb_model.vectors\n",
        "\n",
        "    # LOAD CLEANED DATA IN TORCH DATASET OBJECTS\n",
        "    splits = {}\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        try:\n",
        "            with open(f\"{os.path.join('dataset_torched', split)}.pkl\", 'rb') as f:\n",
        "                splits[split] = pickle.load(f)\n",
        "        except Exception:\n",
        "            if split == 'train':\n",
        "                tokenizer = None\n",
        "            elif split == 'val':\n",
        "                tokenizer = splits['train'].tokenizer\n",
        "            else:\n",
        "                tokenizer = splits['val'].tokenizer \n",
        "\n",
        "            splits[split] = FactDataset(EMBEDDING_DIMENSION, glove_dict, glove_matrix, split, tokenizer=tokenizer)\n",
        "            with open(f\"{os.path.join('dataset_torched', split)}.pkl\", 'wb') as f:\n",
        "                pickle.dump(splits[split], f)\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "splits = load_dataset()\n",
        "train, val, test = splits['train'], splits['val'], splits['test']"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZT5H9gtL3as",
        "outputId": "bd221d74-577e-43ee-efd7-27f25d07029c"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[12083, 136, 4457, 8074, 29785, 18095, 12083, 12049, 20449, 26075, 9109], [2773, 21537, 28304, 32163, 10086, 7129, 8074, 6845, 6809, 29972, 18095, 12083, 9245, 24980, 20449, 30765, 9109]], [[28084, 19819, 21582], [20209, 24556, 13183, 10237, 25852, 29234, 27881, 19938, 22097, 26254, 32833, 7129, 21582, 21115, 12083, 1538]]], [[0, 1], [0, 1]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.scottcondron.com/jupyter/visualisation/audio/2020/12/02/dataloaders-samplers-collate.html#Input-to-your-collate-function\n",
        "\n",
        "def chunk(indices, chunk_size):\n",
        "    return torch.split(torch.tensor(indices), chunk_size)\n",
        "\n",
        "\n",
        "class SequentialSampler(Sampler[int]):\n",
        "    def __init__(self, dataset, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset = dataset \n",
        "        self.indices = random.sample(list(range(len(dataset))), len(dataset))\n",
        "        self.batch_size = batch_size        \n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = chunk(self.indices, self.batch_size)\n",
        "        return iter(batches)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)//self.batch_size\n",
        "    \n",
        "def collate_fn(batch):\n",
        "    label_list, claim_evid, = [], [[], []]\n",
        "    batch = [list(sample) for sample in batch]\n",
        "\n",
        "    first_el = batch[0]\n",
        "    # print(' '.join([test.tokenizer.key_to_value[el] for el in first_el[0][0]])) # claim\n",
        "    # print(' '.join([test.tokenizer.key_to_value[el] for el in first_el[0][1]])) # evid\n",
        "    # print('supports' if first_el[1] == 1 else 'refutes') # label\n",
        "\n",
        "    for _text, _label in batch:  # [  [[claim, evid], label] ... ]\n",
        "        label_list.append(_label)\n",
        "        claim_evid[0].append(torch.tensor(_text[0]))\n",
        "        claim_evid[1].append(torch.tensor(_text[1]))    \n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    claim_evid[0] = pad_sequence(claim_evid[0], batch_first=True, padding_value=0)\n",
        "    claim_evid[1] = pad_sequence(claim_evid[1], batch_first=True, padding_value=0)\n",
        "\n",
        "    # print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claim_evid[0][0]]))\n",
        "    # print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claim_evid[1][0]]))\n",
        "\n",
        "    \n",
        "    return (claim_evid[0].to(device), claim_evid[1].to(device)), label_list.to(device)\n",
        "\n",
        "\n",
        "\n",
        "sequential_sampler_train = SequentialSampler(train, BATCH_SIZE)\n",
        "sequential_sampler_val = SequentialSampler(val, BATCH_SIZE)\n",
        "sequential_sampler_test = SequentialSampler(test, BATCH_SIZE)\n",
        "\n",
        "\n",
        "dataloader_train = DataLoader(dataset=train,\n",
        "                              pin_memory=False, shuffle=False, collate_fn=collate_fn,\n",
        "                               batch_sampler=sequential_sampler_train)\n",
        "\n",
        "dataloader_val = DataLoader(dataset=val,\n",
        "                            pin_memory=False, shuffle=False, collate_fn=collate_fn,\n",
        "                             batch_sampler=sequential_sampler_val)\n",
        "\n",
        "dataloader_test = DataLoader(dataset=test, \n",
        "                             pin_memory=False, shuffle=False, collate_fn=collate_fn,\n",
        "                              batch_sampler=sequential_sampler_test)\n",
        "\n",
        "print(len(dataloader_train))\n",
        "\n"
      ],
      "metadata": {
        "id": "4maEcn9ZBK6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49aa529-0ebf-4187-e099-98bb286c9d23"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ED8m5BXBXWo",
        "outputId": "751a0055-1d30-4ee3-dea7-de7017a696cb"
      },
      "source": [
        "# TESTING DATALOADERS\n",
        "for i, batch in enumerate(dataloader_test):\n",
        "    # dataloader return batch by batch\n",
        "\n",
        "    # print(batch[0][1])\n",
        "\n",
        "    labels = batch[1]\n",
        "    claims = batch[0][0]\n",
        "    evids = batch[0][1]\n",
        "\n",
        "    print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claims[0]]))\n",
        "    print(' '.join([test.tokenizer.key_to_value[el.item()] for el in evids[0]]))\n",
        "    print('supports' if labels[0] == [0, 1] else 'refutes')\n",
        "\n",
        "\n",
        "    if i == 0:\n",
        "        break\n",
        "\n",
        "# CHECK PADDING TOKEN \n",
        "# print(f\"PADDING STRING: {val.tokenizer.key_to_value[0]}\")\n",
        "# print(val.tokenizer.key_to_value)\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the xfiles starred an actress <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "gillian leigh anderson obe is an americanbritish film television and theatre actress activist and writer <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "refutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.tokenizer.value_to_key['<PAD>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDacFeWxcFBh",
        "outputId": "cd9f8006-c658-40a7-bbf3-9e3511f6b40a"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0IcF8uSk-q2"
      },
      "source": [
        "# TRAIN VARIOUS MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JleCKqxecVF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "666b165a-7f27-4faf-fb73-1b7f73679fc2"
      },
      "source": [
        "sentence_emb_types = [\n",
        "                    'last_state',\n",
        "                    'average_outputs',\n",
        "                    'simple_mlp',\n",
        "                    'mean_of_tokens'\n",
        "]\n",
        "\n",
        "merging_types = [\n",
        "                'concatenation',\n",
        "                'sum',\n",
        "                'mean'\n",
        "]\n",
        "\n",
        "\n",
        "for sentence_type in sentence_emb_types:\n",
        "    for merging_type in merging_types:\n",
        "        model_params = {\n",
        "            'embedding_dim': EMBEDDING_DIMENSION,\n",
        "            'output_dim': NUM_CLASSES,\n",
        "            'pre_trained_emb': torch.tensor(test.emb_matrix).to(device),\n",
        "            'merging_type': merging_type,\n",
        "            'sentence_type': sentence_type\n",
        "        }\n",
        "\n",
        "        model = Model(**model_params)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        model = model.to(device)\n",
        "        loss = loss.to(device)\n",
        "\n",
        "        # summary(model, (2, 90)\n",
        "        # quit()\n",
        "\n",
        "        training_info = {\n",
        "            'model': model,\n",
        "            'epochs': EPOCHS,\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'iterator_train': dataloader_train,\n",
        "            'iterator_validation': dataloader_val,\n",
        "            'optimizer': optimizer,\n",
        "            'loss': loss,\n",
        "            'num_train_samples': train.n_samples,\n",
        "            'device': device\n",
        "        }\n",
        "\n",
        "        train_losses, train_accs, val_losses, val_accs = train_model(**training_info)\n",
        "\n",
        "        # REMEMBER: delete the breaks after debugging\n",
        "        break\n",
        "    break"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r |--------------------| 0.0% Initializing\n",
            " CLAIM RNN tensor([[[-0.3623, -0.1691,  0.2425,  ..., -0.1009, -0.0418,  0.3423],\n",
            "         [-0.2106, -0.3166, -0.1216,  ...,  0.6586,  0.1639, -0.0477],\n",
            "         [-0.2192,  0.4255,  0.6297,  ..., -0.1548,  0.4241, -0.0190],\n",
            "         ...,\n",
            "         [-0.0980,  0.0112, -0.4542,  ...,  0.4539,  0.1122, -0.0693],\n",
            "         [-0.3255,  0.3389,  0.3974,  ..., -0.0682,  0.1691,  0.4063],\n",
            "         [-0.3255,  0.3389,  0.3974,  ..., -0.0682,  0.1691,  0.4063]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0622,  0.0374,  0.4137,  ...,  0.2901,  0.3355,  0.2916],\n",
            "         [-0.0659, -0.2863,  0.3257,  ...,  0.5151,  0.3480,  0.3082],\n",
            "         ...,\n",
            "         [ 0.7467, -0.4993, -0.1299,  ...,  0.8217, -0.1131,  0.2087],\n",
            "         [-0.4527,  0.1815,  0.1858,  ...,  0.1695, -0.1534,  0.0461],\n",
            "         [-0.4527,  0.1815,  0.1858,  ...,  0.1695, -0.1534,  0.0461]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0650,  0.0996,  0.0467,  ...,  0.2884,  0.2729,  0.5029],\n",
            "         ...,\n",
            "         [ 0.0116, -0.1046, -0.0531,  ...,  0.1195, -0.0507,  0.2881],\n",
            "         [ 0.3470, -0.5182, -0.3869,  ...,  0.6492, -0.1503,  0.2938],\n",
            "         [ 0.3470, -0.5182, -0.3869,  ...,  0.6492, -0.1503,  0.2938]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [-0.4096,  0.2141,  0.3158,  ..., -0.1581,  0.3960,  0.4488],\n",
            "         [ 0.0710, -0.1505,  0.2933,  ...,  0.1418, -0.1888,  0.4522],\n",
            "         [ 0.0710, -0.1505,  0.2933,  ...,  0.1418, -0.1888,  0.4522]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.1334, -0.3688,  0.2260,  ..., -0.0037, -0.1776, -0.1162],\n",
            "         [ 0.1334, -0.3688,  0.2260,  ..., -0.0037, -0.1776, -0.1162]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.1416, -0.4635, -0.0188,  ...,  0.4586,  0.2536, -0.0853],\n",
            "         [ 0.1416, -0.4635, -0.0188,  ...,  0.4586,  0.2536, -0.0853]]],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "torch.Size([32, 32, 100])\n",
            "\n",
            " EVID RNN tensor([[[-0.3623, -0.1691,  0.2425,  ..., -0.1009, -0.0418,  0.3423],\n",
            "         [-0.2106, -0.3166, -0.1216,  ...,  0.6586,  0.1639, -0.0477],\n",
            "         [-0.2192,  0.4255,  0.6297,  ..., -0.1548,  0.4241, -0.0190],\n",
            "         ...,\n",
            "         [-0.0980,  0.0112, -0.4542,  ...,  0.4539,  0.1122, -0.0693],\n",
            "         [-0.3255,  0.3389,  0.3974,  ..., -0.0682,  0.1691,  0.4063],\n",
            "         [-0.3255,  0.3389,  0.3974,  ..., -0.0682,  0.1691,  0.4063]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0622,  0.0374,  0.4137,  ...,  0.2901,  0.3355,  0.2916],\n",
            "         [-0.0659, -0.2863,  0.3257,  ...,  0.5151,  0.3480,  0.3082],\n",
            "         ...,\n",
            "         [ 0.7467, -0.4993, -0.1299,  ...,  0.8217, -0.1131,  0.2087],\n",
            "         [-0.4527,  0.1815,  0.1858,  ...,  0.1695, -0.1534,  0.0461],\n",
            "         [-0.4527,  0.1815,  0.1858,  ...,  0.1695, -0.1534,  0.0461]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0650,  0.0996,  0.0467,  ...,  0.2884,  0.2729,  0.5029],\n",
            "         ...,\n",
            "         [ 0.0116, -0.1046, -0.0531,  ...,  0.1195, -0.0507,  0.2881],\n",
            "         [ 0.3470, -0.5182, -0.3869,  ...,  0.6492, -0.1503,  0.2938],\n",
            "         [ 0.3470, -0.5182, -0.3869,  ...,  0.6492, -0.1503,  0.2938]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [-0.4096,  0.2141,  0.3158,  ..., -0.1581,  0.3960,  0.4488],\n",
            "         [ 0.0710, -0.1505,  0.2933,  ...,  0.1418, -0.1888,  0.4522],\n",
            "         [ 0.0710, -0.1505,  0.2933,  ...,  0.1418, -0.1888,  0.4522]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.1334, -0.3688,  0.2260,  ..., -0.0037, -0.1776, -0.1162],\n",
            "         [ 0.1334, -0.3688,  0.2260,  ..., -0.0037, -0.1776, -0.1162]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.1416, -0.4635, -0.0188,  ...,  0.4586,  0.2536, -0.0853],\n",
            "         [ 0.1416, -0.4635, -0.0188,  ...,  0.4586,  0.2536, -0.0853]]],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-3a81d030fa8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         }\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtraining_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# REMEMBER: delete the breaks after debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-ee0915d1a497>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, batch_size, iterator_train, iterator_validation, optimizer, loss, num_train_samples, device)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-ee0915d1a497>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, iterator_train, device, optimizer, loss, epoch, epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#     raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# claim, evid as inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# REMEMBER: round to nearest integer for dense and softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-138-56403f23644e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, claim, evid)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mmerge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerge_input1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_input2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch_size, EMB_DIM]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;31m# print(\"SOFTMAX:\", self.softmax(dense))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# print(\"############################### NEXT BATCH ###########################\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2048x100 and 200x2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# problemi con le label e target, sempre piu allineati"
      ],
      "metadata": {
        "id": "lYDUmjg749st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# saving model\n",
        "torch.save(model, 'model/model.pkl')"
      ],
      "metadata": {
        "id": "iB9jcI2YXUbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model_params = {\n",
        "            'embedding_dim': EMBEDDING_DIMENSION,\n",
        "            'output_dim': NUM_CLASSES,\n",
        "            'pre_trained_emb': torch.tensor(test.emb_matrix).to(device),\n",
        "            'merging_type': 'last_state',\n",
        "            'sentence_type': 'concatenation'\n",
        "        }\n",
        "\n",
        "\n",
        "model = Model(**model_params)\n",
        "model = torch.load('model/model.pkl')\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "-8CzEllyXf8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model:nn.Module,\n",
        "            claims,\n",
        "            evids):\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        claims = claims.to(torch.int64)\n",
        "        evids = evids.to(torch.int64)\n",
        "\n",
        "        prediction = torch.round(model(claims, evids)).cpu().tolist()\n",
        "        print(f\"PREDICTED LABEL: {'supports' if prediction == [.0, 1.0] else 'refutes'}\")\n",
        "\n",
        "for i, batch in enumerate(dataloader_test):\n",
        "    for j in range(BATCH_SIZE):\n",
        "\n",
        "        labels = batch[1].cpu().tolist()\n",
        "        claims = batch[0][0]\n",
        "        evids = batch[0][1]\n",
        "\n",
        "        # print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claims[j]]))\n",
        "        # print(' '.join([test.tokenizer.key_to_value[el.item()] for el in evids[j]]))\n",
        "        print(f\"TRUE LABEL: {'supports' if labels[j] == [0, 1] else 'refutes'}\")\n",
        "\n",
        "        predict(model, torch.unsqueeze(claims[j], 0), torch.unsqueeze(evids[j], 0))\n",
        "\n",
        "    if i == 20:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOnU0LVpUAJL",
        "outputId": "d5138c94-4243-47f1-9e74-279b8078d413"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSWBjOyWys0g"
      },
      "source": [
        "#1 SISTEMARE LOSS\n",
        "#2 NON STAMPARE SEMPRE LO STESSO\n",
        "#3 "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}