{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2FactChecking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGoJUhHIxywn"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6CoTN8vxzKx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import requests\n",
        "import zipfile\n",
        "import pickle\n",
        "import gensim\n",
        "import random\n",
        "import gensim.downloader as gloader\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to use key_to_index attribute from the embedding model\n",
        "! pip install gensim==4.1.2\n",
        "import gensim\n",
        "import gensim.downloader as gloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8U2Aq6FCgj-",
        "outputId": "94703b3c-3358-41e1-ca67-83dd0d5e8479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fhHGHAk10EH"
      },
      "source": [
        "EMBEDDING_DIMENSION = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS = 3\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYwOT8L87ORu",
        "outputId": "53c5f9b1-26ae-474c-f166-21fac1a5fb97"
      },
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use. \n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True        \n",
        "    \n",
        "fix_random(42)\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 15 09:13:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk5ohYbXyO69"
      },
      "source": [
        "# PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC-BPPk4yOuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a5f7bb-2da8-458b-fefd-fdec50a0f717"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                           params={'id': toy_data_url_id},\n",
        "                                           stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "\n",
        "def pre_process(dataset, filename):  # clean the dataset\n",
        "    dataset.drop(dataset.columns[0], axis=1, inplace=True)  # remove first column of dataframe containing numbers\n",
        "    dataset.drop(['ID'], axis=1, inplace=True)\n",
        "    # remove numbers before each evidence\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'^\\d+\\t', '', x))\n",
        "    # remove everything after the period\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r' \\..*', ' .', x))\n",
        "    # remove round brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LRB-.*-RRB-', '', x))\n",
        "    # remove square brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LSB-.*-RSB-', '', x))\n",
        "\n",
        "    n_before = dataset.shape[0]\n",
        "    # removes instances longer than a threshold on evidence\n",
        "    # TODO: only on train\n",
        "    dataset = dataset[dataset['Evidence'].str.split().str.len() <= 100]\n",
        "    # remove all rows where there are single brackets in the evidence\n",
        "    dataset = dataset[~dataset['Evidence'].str.contains('|'.join(['-LRB-', '-LSB-', '-RRB-', '-RSB-']))]\n",
        "    n_after = dataset.shape[0]\n",
        "\n",
        "    # removes punctuation and excessive spaces\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r' +', ' ', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'^ +', '', x))\n",
        "    dataset = dataset.applymap(lambda x: x.lower())\n",
        "\n",
        "    labels = {'supports': 1, 'refutes': 0}\n",
        "    dataset = dataset.replace({'Label': labels})\n",
        "    # removes rows with empty elements\n",
        "    dataset = dataset[dataset['Evidence'] != '']\n",
        "    dataset = dataset[dataset['Claim'] != '']\n",
        "    dataset = dataset[dataset['Label'] != '']\n",
        "\n",
        "\n",
        "\n",
        "    rem_elements = n_before - n_after\n",
        "    print(f\"Removed {rem_elements}\\t ({100 * rem_elements / n_before:.2F}%)\"\n",
        "          f\" elements because of inconsistency on {filename}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB=True\n",
        "except:\n",
        "    IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"We're running Colab\")\n",
        "    # Mount the Google Drive at mount\n",
        "    mount='/content/gdrive'\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Switch to the directory on the Google Drive that you want to use\n",
        "    drive_root = mount + \"/My Drive/NLP/Assignment2\"\n",
        "    \n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "        print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "        os.makedirs(drive_root, exist_ok=True)\n",
        "    \n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "    print(\"Checking working directory:\")\n",
        "    %pwd\n",
        "\n",
        "# download_data('dataset')\n",
        "\n",
        "if not len(os.listdir(\"dataset_cleaned\")):\n",
        "    for file in os.listdir(\"dataset\"):\n",
        "        dataset_cleaned = pre_process(pd.read_csv(\"dataset/\" + file, sep=','), file)\n",
        "        dataset_cleaned.to_csv(os.path.join(\"dataset_cleaned\", file))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/NLP/Assignment2  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/NLP/Assignment2\n",
            "/content/gdrive/My Drive/NLP/Assignment2\n",
            "Checking working directory:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGgYfXJqyD4n"
      },
      "source": [
        "# CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkUKdAkSyHvT"
      },
      "source": [
        "## FactDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPOc3HdsvyDI"
      },
      "source": [
        "class FactDataset(Dataset):\n",
        "    def __init__(self, emb_dim, glove_dict, glove_matrix, split='train', tokenizer=None):\n",
        "        pairs_claim_evid = pd.read_csv(f\"dataset_cleaned/{split}_pairs.csv\")\n",
        "        # tokenization & embeddings\n",
        "        text = pairs_claim_evid['Claim'] + ' ' + pairs_claim_evid['Evidence']\n",
        "        text = text.astype(str).to_list()\n",
        "\n",
        "        # IMPORTANT: if a tokenizer is already computed for example on the train set, for val set we have to extend\n",
        "        if tokenizer:\n",
        "            self.tokenizer = tokenizer\n",
        "            self.tokenizer.dataset_sentences = text\n",
        "        else:\n",
        "            self.tokenizer = Tokenizer(text, emb_dim, glove_dict, glove_matrix)\n",
        "\n",
        "        self.tokenizer.tokenize()\n",
        "        self.val_to_key = self.tokenizer.get_val_to_key()\n",
        "        self.emb_matrix = self.tokenizer.build_embedding_matrix()\n",
        "        print(len(self.emb_matrix))\n",
        "\n",
        "        lengths = pairs_claim_evid['Evidence'].str.split().str.len().to_numpy()\n",
        "\n",
        "        # print(len([i for i, el in enumerate(lengths) if el > 100]))\n",
        "\n",
        "        self.max_claim_len = int(pairs_claim_evid['Claim'].str.split().str.len().max())\n",
        "        self.max_evid_len = int(pairs_claim_evid['Evidence'].str.split().str.len().max())\n",
        "        self.max_seq_len = int(max(self.max_evid_len, self.max_claim_len))\n",
        "\n",
        "        # tester function\n",
        "        if not self._check_tokenizer():\n",
        "            raise ValueError\n",
        "\n",
        "        # create x as list of sentences\n",
        "        # sentences are splitted in claim evidence\n",
        "        self.x = []\n",
        "        print(f\"Creating FactDataset: \\n\")\n",
        "        for i, (claim_sen, evid_sen) in tqdm(enumerate(zip(pairs_claim_evid['Claim'], pairs_claim_evid['Evidence']))):\n",
        "            sentence = [[], []]\n",
        "            for word in claim_sen.split():\n",
        "                sentence[0].append(self.val_to_key[word])\n",
        "            for word in evid_sen.split():\n",
        "                sentence[1].append(self.val_to_key[word])\n",
        "\n",
        "            # Padding deprecated here\n",
        "            # sentence[0] = sentence[0] + [0] * (self.max_seq_len - len(claim_sen.split()))\n",
        "            # sentence[1] = sentence[1] + [0] * (self.max_seq_len - len(evid_sen.split()))\n",
        "            self.x.append(sentence)\n",
        "\n",
        "         \n",
        "        # self.x = torch.tensor(self.x)\n",
        "        # self.y = torch.tensor(pairs_claim_evid['Label'])\n",
        "\n",
        "        # one-hot: [0,1] => supports, [1,0] => refutes\n",
        "        self.y = [[0, 1] if l==1 else [1, 0] for l in pairs_claim_evid['Label'].to_list()]\n",
        "        self.n_samples = pairs_claim_evid.shape[0]\n",
        "        print(f\"Max sequence length: {self.max_seq_len}\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # xi = [[claim], [evid]], yi = [label]\n",
        "        return [self.x[index], self.y[index]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def _check_tokenizer(self):\n",
        "        word = 'the'\n",
        "        for i, el in enumerate(self.tokenizer.glove_matrix[self.tokenizer.glove_dict[word]]):\n",
        "            if el != self.emb_matrix[self.val_to_key[word]][i]:\n",
        "                print(\"Check Tokenizer, possible bugs\")\n",
        "                return False\n",
        "        return True\n",
        "    def get_tokenizer(self):\n",
        "        return self.tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJHDwAfYyK_X"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mHt8SIP1WY_"
      },
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, dataset_sentences, embedding_dim, glove_dict, glove_matrix):\n",
        "        self.embedding_matrix = None\n",
        "        self.value_to_key = {}\n",
        "        self.value_to_key_new = {}\n",
        "        self.key_to_value = {}\n",
        "        self.num_unique_words = 0\n",
        "        self.dataset_sentences = dataset_sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.glove_dict = glove_dict\n",
        "        self.glove_matrix = glove_matrix\n",
        "        self.unique_words = set()\n",
        "\n",
        "    def get_val_to_key(self):\n",
        "        return copy.deepcopy(self.value_to_key)\n",
        "\n",
        "    def tokenize(self):\n",
        "        self.value_to_key_new = {}\n",
        "        unique_words = set()\n",
        "        for sen in self.dataset_sentences:\n",
        "            for w in sen.split():\n",
        "                unique_words.add(w)  # get set of unique words\n",
        "        new_unique = unique_words - self.unique_words\n",
        "        for i, word in enumerate(new_unique):\n",
        "            if self.embedding_matrix is not None:\n",
        "                self.key_to_value[i + len(self.embedding_matrix)] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i + len(self.embedding_matrix)\n",
        "            else:\n",
        "                self.key_to_value[i] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i\n",
        "            self.value_to_key_new[word] = i\n",
        "\n",
        "        self.num_unique_words = len(new_unique)\n",
        "        self.unique_words = self.unique_words | new_unique  # union of unique words and new unique words\n",
        "\n",
        "\n",
        "    def __build_embedding_matrix_glove(self):\n",
        "        oov_words = []\n",
        "        tmp_embedding_matrix = np.zeros((self.num_unique_words, self.embedding_dim), dtype=np.float32)\n",
        "        len_old_emb_matrix = len(self.embedding_matrix) if self.embedding_matrix is not None else 0\n",
        "        print(f\"Finding OOVs: \")\n",
        "        for word, idx in tqdm(self.value_to_key_new.items()):\n",
        "            try:\n",
        "                embedding_vector = self.glove_matrix[self.glove_dict[word]]\n",
        "                tmp_embedding_matrix[idx] = embedding_vector\n",
        "            except (KeyError, TypeError):\n",
        "                oov_words.append((word, idx + len_old_emb_matrix))\n",
        "        if self.embedding_matrix is not None:\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, tmp_embedding_matrix))\n",
        "        else:\n",
        "            self.embedding_matrix = tmp_embedding_matrix\n",
        "        return oov_words\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        oov_words = self.__build_embedding_matrix_glove()\n",
        "        print(f\"Solving OOVs: \")\n",
        "        for word, idx in tqdm(oov_words):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=self.embedding_dim)\n",
        "            self.embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "        # PADDING (feat. David Guetta)\n",
        "        first = self.embedding_matrix[0]\n",
        "        if np.count_nonzero(first) != 0:\n",
        "            first = self.embedding_matrix[0]\n",
        "            self.embedding_matrix[0] = np.zeros(self.embedding_dim)\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, first))\n",
        "\n",
        "            word_to_change = min(self.value_to_key.items(), key=lambda x: x[1])[0]\n",
        "            self.key_to_value[0] = '<PAD>'\n",
        "            self.value_to_key['<PAD>'] = 0\n",
        "            self.value_to_key[word_to_change] = len(self.embedding_matrix) - 1\n",
        "            self.key_to_value[len(self.embedding_matrix) - 1] = word_to_change\n",
        "\n",
        "        return copy.deepcopy(self.embedding_matrix)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERVX9LTl1Y5O"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuZv5Omm1gA_"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, embedding_dim, output_dim, pre_trained_emb,\n",
        "                 merging_type='concatenation', sentence_type='last_state'):\n",
        "        super().__init__()\n",
        "        self.merging_type = merging_type\n",
        "        self.sentence_type = sentence_type\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings=pre_trained_emb, freeze=True, padding_idx=0)\n",
        "        self.rnn_claim = nn.RNN(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
        "        self.rnn_evid = nn.RNN(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(embedding_dim * (2 if merging_type == 'concatenation' else 1), 2)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.i = 0\n",
        "        # Find a good way to initialize\n",
        "        # torch.nn.init.uniform_(self.fc.weight, a=0.49, b=0.51)\n",
        "\n",
        "    def forward(self, claim, evid):\n",
        "        # claim = [max_tok, emb_dim]\n",
        "        # evid = [max_tok, emb_dim]\n",
        "        # self.i+=1\n",
        "\n",
        "        seq_lengths_claim = torch.tensor(list(map(len, claim)))\n",
        "        seq_lengths_evid = torch.tensor(list(map(len, evid)))\n",
        "\n",
        "        seq_lengths_claim, perm_idx_claim = seq_lengths_claim.sort(0, descending=True)\n",
        "        seq_lengths_evid, perm_idx_evid = seq_lengths_evid.sort(0, descending=True)\n",
        "\n",
        "        seq_claim = claim[perm_idx_claim]\n",
        "        seq_evid = evid[perm_idx_evid]\n",
        "\n",
        "        emb_claim = self.embedding(seq_claim.long())# [batch_size, max_len_in_batch, 50]\n",
        "        emb_evid = self.embedding(seq_evid.long())# [batch_size, max_len_in_batch, 50]\n",
        "\n",
        "        packed_claim = pack_padded_sequence(emb_claim, seq_lengths_claim, batch_first=True)\n",
        "        packed_evid = pack_padded_sequence(emb_evid, seq_lengths_evid, batch_first=True)\n",
        "\n",
        "        output_claim, hidden_claim = self.rnn_claim(packed_claim) \n",
        "        output_evid, hidden_evid = self.rnn_evid(packed_evid) \n",
        "\n",
        "        output_claim_unpack, claim_lens_unpacked = pad_packed_sequence(output_claim, batch_first=True)\n",
        "        output_evid_unpack, evid_lens_unpacked = pad_packed_sequence(output_evid, batch_first=True)\n",
        "\n",
        "        output_claim_idxs_first_zero = torch.argmin(torch.abs(torch.sum(output_claim_unpack, dim=2)), dim=1, keepdim=True)\n",
        "\n",
        "        print()\n",
        "        [print(output_claim_unpack[i, -1]) for i in range(32)]\n",
        "        print(output_evid_unpack.size())\n",
        "        print(torch.sum(output_claim_unpack, dim=2).size())\n",
        "\n",
        "        print(torch.sum(output_claim_unpack, dim=2))\n",
        "        print(output_claim_idxs_first_zero)\n",
        "\n",
        "        raise\n",
        "\n",
        "        if self.sentence_type == 'last_state':\n",
        "            merge_input1 = output_claim_unpack[:, -1, :] # [batch_size, EMB_DIM]\n",
        "            merge_input2 = output_evid_unpack[:, -1, :] # [batch_size, EMB_DIM]\n",
        "            \n",
        "            # print(\"\\n CLAIM RNN\", merge_input1)\n",
        "            # print(merge_input1.size())\n",
        "            # print(\"\\n EVID RNN\", merge_input2)\n",
        "\n",
        "        elif self.sentence_type == 'average_outputs':\n",
        "            # recheck shapes on this, not yet tested\n",
        "            merge_input1 = torch.mean(output_claim)\n",
        "            merge_input2 = torch.mean(output_evid)\n",
        "        elif self.sentence_type == 'simple_mlp':\n",
        "            raise NotImplementedError\n",
        "        else: # mean of tokens\n",
        "            raise NotImplementedError\n",
        "\n",
        "        if self.merging_type == 'concatenation':\n",
        "            merge = torch.cat([merge_input1, merge_input2], 1) # [batch_size, EMB_DIM*2]\n",
        "            # print(\"\\n MERGE\", merge)\n",
        "            # if self.i == 30:\n",
        "            #     raise\n",
        "        elif self.merging_type == 'sum':\n",
        "            merge = torch.sum([merge_input1, merge_input2], dim=0) # [batch_size, EMB_DIM]\n",
        "        else:\n",
        "            merge = torch.mean([merge_input1, merge_input2], dim=0)  # [batch_size, EMB_DIM]\n",
        "\n",
        "        dense = self.fc(merge)\n",
        "        # print(\"SOFTMAX:\", self.softmax(dense))\n",
        "        # print(\"############################### NEXT BATCH ###########################\")\n",
        "        return self.softmax(dense)\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"Model with '{self.sentence_type}' \"\\\n",
        "               f\"sentence embeddings and '{self.merging_type}' merging type.\""
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh5uBlVUzsO8"
      },
      "source": [
        "### Model functions for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7eOK7wizqsl"
      },
      "source": [
        "##### General functions for Models #####\n",
        "\n",
        "def progress_bar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
        "    \"\"\"\n",
        "    credits to: https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total: \n",
        "        print()\n",
        "\n",
        "def train_epoch(model:nn.Module,\n",
        "                iterator_train: torch.utils.data.DataLoader,\n",
        "                device: torch.device,\n",
        "                optimizer: torch.optim,\n",
        "                loss,\n",
        "                epoch,\n",
        "                epochs):\n",
        "\n",
        "    accuracy_epoch = 0\n",
        "    loss_epoch = 0\n",
        "    num_batches = len(iterator_train)\n",
        "\n",
        "    progress_bar(0, num_batches, prefix='', suffix='Initializing', length=20, printEnd='')\n",
        "\n",
        "    \n",
        "    model.train()\n",
        "    for i, batch in enumerate(iterator_train):\n",
        "\n",
        "        # print(len(batch[0]))\n",
        "        # raise\n",
        "\n",
        "        labels = batch[1].to(torch.float32) \n",
        "        claims = batch[0][0].to(torch.float32)\n",
        "        evids = batch[0][1].to(torch.float32)\n",
        "\n",
        "        # if i==0:\n",
        "        #     print()\n",
        "        #     print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claims[0]])) # claim\n",
        "        #     print(' '.join([test.tokenizer.key_to_value[el.item()] for el in evids[0]])) # evid\n",
        "        #     print('supports' if labels[0].item() == 1 else 'refutes') # label\n",
        "        #     raise\n",
        "\n",
        "        predictions = model(claims, evids) # claim, evid as inputs\n",
        "        \n",
        "        # REMEMBER: round to nearest integer for dense and softmax\n",
        "        accuracy_step = binary_accuracy(torch.round(predictions), labels)\n",
        "        accuracy_epoch += accuracy_step\n",
        "\n",
        "        # if i <= 30:\n",
        "        #     print()\n",
        "        #     print(\"PREDS: \", predictions)\n",
        "        #     print(\"TRUES: \", labels)\n",
        "\n",
        "        # if i == 30:\n",
        "        #     raise\n",
        "\n",
        "        loss_step = loss(predictions, labels)\n",
        "        loss_epoch += loss_step.item()\n",
        "\n",
        "        loss_step.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % (num_batches // 20) == 0:\n",
        "            train_info_str = f\"-- Epoch: {epoch}/{epochs} \"\\\n",
        "                                f\"-- Step: {i + 1}/{num_batches} \"\\\n",
        "                                f\"-- Acc: {accuracy_step:.2F} \"\\\n",
        "                                f\"-- Loss: {loss_step.item():.2F}\"\n",
        "\n",
        "            progress_bar(i + 1, num_batches, prefix='', suffix=train_info_str, length=20, printEnd='')\n",
        "            \n",
        "    loss_epoch /= num_batches\n",
        "    accuracy_epoch /= num_batches\n",
        "    return loss_step.item(), accuracy_step\n",
        "\n",
        "def validate_epoch(model:nn.Module,\n",
        "                    iterator_val: torch.utils.data.DataLoader,\n",
        "                    device: torch.device,\n",
        "                    optimizer: torch.optim,\n",
        "                    loss,\n",
        "                    epoch):\n",
        "\n",
        "    accuracy_epoch = 0\n",
        "    loss_epoch = 0\n",
        "    num_batches = len(iterator_val)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator_val):\n",
        "            labels = batch[1].to(torch.float32) \n",
        "            claims = batch[0][0].to(torch.float32)\n",
        "            evids = batch[0][1].to(torch.float32)\n",
        "\n",
        "            predictions = model(claims, evids) # claim, evid as inputs        \n",
        "            \n",
        "            # REMEMBER: round to nearest integer for dense and argmax for softmax\n",
        "            accuracy_step = binary_accuracy(torch.round(predictions), labels)\n",
        "            accuracy_epoch += accuracy_step\n",
        "\n",
        "            loss_step = loss(predictions, labels)\n",
        "            loss_epoch += loss_step.item()\n",
        "\n",
        "    loss_epoch /= num_batches\n",
        "    accuracy_epoch /= num_batches\n",
        "\n",
        "    return loss_step.item(), accuracy_step\n",
        "    \n",
        "\n",
        "def train_model(model, \n",
        "                epochs, \n",
        "                batch_size, \n",
        "                iterator_train, \n",
        "                iterator_validation, \n",
        "                optimizer, \n",
        "                loss, \n",
        "                num_train_samples, \n",
        "                device):\n",
        "    model.to(device)\n",
        "    num_iters = num_train_samples // batch_size\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train_epoch(model, iterator_train, device, optimizer, loss, epoch, epochs)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc = validate_epoch(model, iterator_validation, device, optimizer, loss, epoch)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        epoch_info_str = f\"-- Epoch: {epoch}/{epochs} \"\\\n",
        "                         f\"-- Step: {num_iters}/{num_iters} \"\\\n",
        "                         f\"-- Acc: {train_acc:.2F} \"\\\n",
        "                         f\"-- Loss: {train_loss:.2F} \"\\\n",
        "                         f\"-- Acc_Val: {val_acc:.2F} \"\\\n",
        "                         f\"-- Loss_Val: {val_loss:.2F}\"\n",
        "        progress_bar(num_iters, num_iters, prefix='', suffix=epoch_info_str, length=20, printEnd='')\n",
        "        \n",
        "    return train_losses, train_accs, val_losses, val_accs\n",
        "\n",
        "\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    correct = [torch.equal(pred, target) for pred, target in zip(preds, y)]\n",
        "    acc = sum(correct) / len(y)\n",
        "    return acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWCd1T3Y2MVE"
      },
      "source": [
        "# LOADING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQMjEAKg1tEj"
      },
      "source": [
        "def load_dataset():\n",
        "    # LOAD GLOVE\n",
        "    try:\n",
        "        with open(f\"glove-{EMBEDDING_DIMENSION}.pkl\", 'rb') as f:\n",
        "            emb_model = pickle.load(f)\n",
        "    except Exception:\n",
        "        emb_model = gloader.load(f\"glove-wiki-gigaword-{EMBEDDING_DIMENSION}\")\n",
        "        with open(f\"glove-{EMBEDDING_DIMENSION}.pkl\", 'wb') as f:\n",
        "            pickle.dump(emb_model, f)\n",
        "\n",
        "    glove_dict = emb_model.key_to_index\n",
        "    glove_matrix = emb_model.vectors\n",
        "\n",
        "    # LOAD CLEANED DATA IN TORCH DATASET OBJECTS\n",
        "    splits = {}\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        try:\n",
        "            with open(f\"{os.path.join('dataset_torched', split)}.pkl\", 'rb') as f:\n",
        "                splits[split] = pickle.load(f)\n",
        "        except Exception:\n",
        "            if split == 'train':\n",
        "                tokenizer = None\n",
        "            elif split == 'val':\n",
        "                tokenizer = splits['train'].tokenizer\n",
        "            else:\n",
        "                tokenizer = splits['val'].tokenizer \n",
        "\n",
        "            splits[split] = FactDataset(EMBEDDING_DIMENSION, glove_dict, glove_matrix, split, tokenizer=tokenizer)\n",
        "            with open(f\"{os.path.join('dataset_torched', split)}.pkl\", 'wb') as f:\n",
        "                pickle.dump(splits[split], f)\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "splits = load_dataset()\n",
        "train, val, test = splits['train'], splits['val'], splits['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZT5H9gtL3as",
        "outputId": "e2351c98-9667-4545-fb6b-02097093df14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[12083, 136, 4457, 8074, 29785, 18095, 12083, 12049, 20449, 26075, 9109], [2773, 21537, 28304, 32163, 10086, 7129, 8074, 6845, 6809, 29972, 18095, 12083, 9245, 24980, 20449, 30765, 9109]], [[28084, 19819, 21582], [20209, 24556, 13183, 10237, 25852, 29234, 27881, 19938, 22097, 26254, 32833, 7129, 21582, 21115, 12083, 1538]]], [[0, 1], [0, 1]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.scottcondron.com/jupyter/visualisation/audio/2020/12/02/dataloaders-samplers-collate.html#Input-to-your-collate-function\n",
        "\n",
        "def chunk(indices, chunk_size):\n",
        "    return torch.split(torch.tensor(indices), chunk_size)\n",
        "\n",
        "\n",
        "class SequentialSampler(Sampler[int]):\n",
        "    def __init__(self, dataset, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset = dataset \n",
        "        self.indices = random.sample(list(range(len(dataset))), len(dataset))\n",
        "        self.batch_size = batch_size        \n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = chunk(self.indices, self.batch_size)\n",
        "        return iter(batches)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)//self.batch_size\n",
        "    \n",
        "def collate_fn(batch):\n",
        "    label_list, claim_evid, = [], [[], []]\n",
        "    batch = [list(sample) for sample in batch]\n",
        "\n",
        "    first_el = batch[0]\n",
        "    # print(' '.join([test.tokenizer.key_to_value[el] for el in first_el[0][0]])) # claim\n",
        "    # print(' '.join([test.tokenizer.key_to_value[el] for el in first_el[0][1]])) # evid\n",
        "    # print('supports' if first_el[1] == 1 else 'refutes') # label\n",
        "\n",
        "    for _text, _label in batch:  # [  [[claim, evid], label] ... ]\n",
        "        label_list.append(_label)\n",
        "        claim_evid[0].append(torch.tensor(_text[0]))\n",
        "        claim_evid[1].append(torch.tensor(_text[1]))    \n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    claim_evid[0] = pad_sequence(claim_evid[0], batch_first=True, padding_value=0)\n",
        "    claim_evid[1] = pad_sequence(claim_evid[1], batch_first=True, padding_value=0)\n",
        "\n",
        "    # print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claim_evid[0][0]]))\n",
        "    # print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claim_evid[1][0]]))\n",
        "\n",
        "    \n",
        "    return (claim_evid[0].to(device), claim_evid[1].to(device)), label_list.to(device)\n",
        "\n",
        "\n",
        "\n",
        "sequential_sampler_train = SequentialSampler(train, BATCH_SIZE)\n",
        "sequential_sampler_val = SequentialSampler(val, BATCH_SIZE)\n",
        "sequential_sampler_test = SequentialSampler(test, BATCH_SIZE)\n",
        "\n",
        "\n",
        "dataloader_train = DataLoader(dataset=train,\n",
        "                              pin_memory=False, shuffle=False, collate_fn=collate_fn,\n",
        "                               batch_sampler=sequential_sampler_train)\n",
        "\n",
        "dataloader_val = DataLoader(dataset=val,\n",
        "                            pin_memory=False, shuffle=False, collate_fn=collate_fn,\n",
        "                             batch_sampler=sequential_sampler_val)\n",
        "\n",
        "dataloader_test = DataLoader(dataset=test, \n",
        "                             pin_memory=False, shuffle=False, collate_fn=collate_fn,\n",
        "                              batch_sampler=sequential_sampler_test)\n",
        "\n",
        "print(len(dataloader_train))\n",
        "\n"
      ],
      "metadata": {
        "id": "4maEcn9ZBK6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4b6674-d5be-484c-a102-e45c8c349485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ED8m5BXBXWo",
        "outputId": "fff6bfa5-a975-47a1-9137-f3c1f7c54b99"
      },
      "source": [
        "# TESTING DATALOADERS\n",
        "for i, batch in enumerate(dataloader_test):\n",
        "    # dataloader return batch by batch\n",
        "\n",
        "    # print(batch[0][1])\n",
        "\n",
        "    labels = batch[1]\n",
        "    claims = batch[0][0]\n",
        "    evids = batch[0][1]\n",
        "\n",
        "    print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claims[0]]))\n",
        "    print(' '.join([test.tokenizer.key_to_value[el.item()] for el in evids[0]]))\n",
        "    print('supports' if labels[0] == [0, 1] else 'refutes')\n",
        "\n",
        "\n",
        "    if i == 0:\n",
        "        break\n",
        "\n",
        "# CHECK PADDING TOKEN \n",
        "# print(f\"PADDING STRING: {val.tokenizer.key_to_value[0]}\")\n",
        "# print(val.tokenizer.key_to_value)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mutiny on the bounty was adapted exactly once into a movie <PAD> <PAD> <PAD>\n",
            "it is the second american film to be made from the novel the first being mutiny on the bounty <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "refutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.tokenizer.value_to_key['<PAD>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDacFeWxcFBh",
        "outputId": "2aab00f5-9140-4933-d75c-fc29a79c5f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0IcF8uSk-q2"
      },
      "source": [
        "# TRAIN VARIOUS MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JleCKqxecVF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a95a2980-a875-474d-991e-2e1c4d9aba1e"
      },
      "source": [
        "sentence_emb_types = [\n",
        "                    'last_state',\n",
        "                    'average_outputs',\n",
        "                    'simple_mlp',\n",
        "                    'mean_of_tokens'\n",
        "]\n",
        "\n",
        "merging_types = [\n",
        "                'concatenation',\n",
        "                'sum',\n",
        "                'mean'\n",
        "]\n",
        "\n",
        "\n",
        "for sentence_type in sentence_emb_types:\n",
        "    for merging_type in merging_types:\n",
        "        model_params = {\n",
        "            'embedding_dim': EMBEDDING_DIMENSION,\n",
        "            'output_dim': NUM_CLASSES,\n",
        "            'pre_trained_emb': torch.tensor(test.emb_matrix).to(device),\n",
        "            'merging_type': merging_type,\n",
        "            'sentence_type': sentence_type\n",
        "        }\n",
        "\n",
        "        model = Model(**model_params)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        model = model.to(device)\n",
        "        loss = loss.to(device)\n",
        "\n",
        "        # summary(model, (2, 90)\n",
        "        # quit()\n",
        "\n",
        "        training_info = {\n",
        "            'model': model,\n",
        "            'epochs': EPOCHS,\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'iterator_train': dataloader_train,\n",
        "            'iterator_validation': dataloader_val,\n",
        "            'optimizer': optimizer,\n",
        "            'loss': loss,\n",
        "            'num_train_samples': train.n_samples,\n",
        "            'device': device\n",
        "        }\n",
        "\n",
        "        train_losses, train_accs, val_losses, val_accs = train_model(**training_info)\n",
        "\n",
        "        # REMEMBER: delete the breaks after debugging\n",
        "        break\n",
        "    break"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r |--------------------| 0.0% Initializing\n",
            "tensor([-0.0470,  0.0032,  0.1078, -0.0395, -0.0184,  0.0178,  0.0557, -0.1810,\n",
            "         0.0641, -0.0687,  0.1372, -0.0466,  0.1223,  0.0143, -0.0725,  0.0722,\n",
            "         0.0261,  0.1779,  0.1304, -0.0624, -0.1249, -0.1180, -0.0241, -0.1356,\n",
            "        -0.1568, -0.1545,  0.1989,  0.0261,  0.1779,  0.0542, -0.1414,  0.0446,\n",
            "        -0.0186,  0.0028,  0.1013,  0.1417, -0.0049, -0.0346, -0.0957, -0.0776,\n",
            "         0.0722, -0.0544,  0.0199, -0.0408,  0.0316,  0.2567,  0.0554,  0.1549,\n",
            "         0.1056,  0.0853,  0.0329, -0.0699, -0.0289,  0.1002,  0.1541, -0.1160,\n",
            "         0.0493, -0.0251, -0.1584, -0.1016, -0.0798,  0.0485,  0.0751,  0.0006,\n",
            "         0.1644,  0.1493, -0.1324, -0.0561, -0.0903,  0.1500, -0.1353,  0.1137,\n",
            "         0.0938,  0.0575,  0.0669,  0.0179, -0.1150, -0.1089, -0.1543, -0.1785,\n",
            "         0.0772,  0.1376, -0.0864, -0.1078,  0.1567,  0.0087, -0.0778, -0.0518,\n",
            "        -0.0369,  0.0205, -0.1191,  0.0645, -0.0156, -0.0793,  0.1172,  0.1084,\n",
            "         0.1624,  0.0969,  0.0576,  0.0510], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0474,  0.0046,  0.1077, -0.0379, -0.0193,  0.0173,  0.0556, -0.1802,\n",
            "         0.0649, -0.0692,  0.1349, -0.0468,  0.1207,  0.0128, -0.0721,  0.0718,\n",
            "         0.0290,  0.1795,  0.1299, -0.0604, -0.1260, -0.1162, -0.0244, -0.1343,\n",
            "        -0.1576, -0.1544,  0.2008,  0.0252,  0.1802,  0.0558, -0.1406,  0.0462,\n",
            "        -0.0177,  0.0008,  0.1021,  0.1420, -0.0047, -0.0341, -0.0947, -0.0770,\n",
            "         0.0690, -0.0541,  0.0193, -0.0425,  0.0318,  0.2576,  0.0569,  0.1552,\n",
            "         0.1046,  0.0842,  0.0326, -0.0722, -0.0297,  0.0996,  0.1541, -0.1147,\n",
            "         0.0494, -0.0249, -0.1586, -0.1016, -0.0788,  0.0490,  0.0734,  0.0008,\n",
            "         0.1642,  0.1502, -0.1334, -0.0558, -0.0907,  0.1495, -0.1356,  0.1124,\n",
            "         0.0938,  0.0577,  0.0682,  0.0167, -0.1149, -0.1101, -0.1543, -0.1771,\n",
            "         0.0782,  0.1358, -0.0885, -0.1063,  0.1571,  0.0096, -0.0771, -0.0539,\n",
            "        -0.0367,  0.0213, -0.1209,  0.0653, -0.0139, -0.0816,  0.1158,  0.1095,\n",
            "         0.1616,  0.0963,  0.0585,  0.0515], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0470,  0.0035,  0.1085, -0.0398, -0.0191,  0.0179,  0.0569, -0.1817,\n",
            "         0.0652, -0.0691,  0.1369, -0.0464,  0.1217,  0.0134, -0.0726,  0.0724,\n",
            "         0.0264,  0.1783,  0.1301, -0.0618, -0.1236, -0.1175, -0.0233, -0.1360,\n",
            "        -0.1569, -0.1542,  0.1991,  0.0268,  0.1783,  0.0539, -0.1404,  0.0450,\n",
            "        -0.0193,  0.0030,  0.1019,  0.1411, -0.0059, -0.0359, -0.0958, -0.0774,\n",
            "         0.0719, -0.0553,  0.0206, -0.0415,  0.0324,  0.2563,  0.0561,  0.1558,\n",
            "         0.1060,  0.0849,  0.0333, -0.0693, -0.0284,  0.0995,  0.1544, -0.1158,\n",
            "         0.0502, -0.0247, -0.1585, -0.1012, -0.0792,  0.0476,  0.0754,  0.0004,\n",
            "         0.1650,  0.1492, -0.1325, -0.0562, -0.0917,  0.1491, -0.1349,  0.1130,\n",
            "         0.0930,  0.0570,  0.0662,  0.0175, -0.1148, -0.1089, -0.1535, -0.1786,\n",
            "         0.0772,  0.1387, -0.0873, -0.1090,  0.1566,  0.0083, -0.0774, -0.0517,\n",
            "        -0.0360,  0.0218, -0.1196,  0.0643, -0.0155, -0.0803,  0.1181,  0.1080,\n",
            "         0.1618,  0.0960,  0.0573,  0.0509], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-4.6880e-02,  4.0576e-03,  1.0905e-01, -3.9876e-02, -1.8913e-02,\n",
            "         1.7597e-02,  5.6753e-02, -1.8200e-01,  6.4671e-02, -6.9033e-02,\n",
            "         1.3688e-01, -4.6739e-02,  1.2152e-01,  1.2275e-02, -7.2823e-02,\n",
            "         7.1871e-02,  2.6663e-02,  1.7881e-01,  1.3022e-01, -6.1761e-02,\n",
            "        -1.2341e-01, -1.1725e-01, -2.4118e-02, -1.3564e-01, -1.5653e-01,\n",
            "        -1.5432e-01,  2.0023e-01,  2.5922e-02,  1.7807e-01,  5.4603e-02,\n",
            "        -1.4024e-01,  4.4632e-02, -1.8746e-02,  3.1999e-03,  1.0189e-01,\n",
            "         1.4106e-01, -5.8959e-03, -3.6259e-02, -9.5637e-02, -7.8098e-02,\n",
            "         7.1419e-02, -5.5366e-02,  2.0274e-02, -4.1212e-02,  3.2037e-02,\n",
            "         2.5604e-01,  5.6068e-02,  1.5573e-01,  1.0547e-01,  8.4941e-02,\n",
            "         3.3791e-02, -6.9962e-02, -2.8849e-02,  9.9925e-02,  1.5459e-01,\n",
            "        -1.1621e-01,  5.0331e-02, -2.5521e-02, -1.5820e-01, -1.0199e-01,\n",
            "        -7.9036e-02,  4.7506e-02,  7.5213e-02,  1.3247e-04,  1.6519e-01,\n",
            "         1.4983e-01, -1.3242e-01, -5.6496e-02, -9.1183e-02,  1.4908e-01,\n",
            "        -1.3420e-01,  1.1339e-01,  9.2930e-02,  5.6633e-02,  6.6673e-02,\n",
            "         1.7430e-02, -1.1458e-01, -1.0918e-01, -1.5351e-01, -1.7846e-01,\n",
            "         7.7677e-02,  1.3898e-01, -8.6976e-02, -1.0900e-01,  1.5681e-01,\n",
            "         8.3504e-03, -7.7257e-02, -5.2107e-02, -3.5851e-02,  2.1321e-02,\n",
            "        -1.1909e-01,  6.3462e-02, -1.5375e-02, -8.0562e-02,  1.1884e-01,\n",
            "         1.0833e-01,  1.6184e-01,  9.6430e-02,  5.7475e-02,  5.0899e-02],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0490,  0.0033,  0.1024, -0.0330, -0.0166,  0.0137,  0.0442, -0.1770,\n",
            "         0.0636, -0.0703,  0.1353, -0.0504,  0.1254,  0.0158, -0.0678,  0.0724,\n",
            "         0.0258,  0.1775,  0.1295, -0.0642, -0.1294, -0.1225, -0.0228, -0.1320,\n",
            "        -0.1581, -0.1587,  0.2019,  0.0248,  0.1802,  0.0571, -0.1457,  0.0485,\n",
            "        -0.0155, -0.0006,  0.0999,  0.1455, -0.0008, -0.0300, -0.0955, -0.0740,\n",
            "         0.0738, -0.0524,  0.0176, -0.0364,  0.0299,  0.2575,  0.0499,  0.1518,\n",
            "         0.1028,  0.0872,  0.0264, -0.0738, -0.0300,  0.1023,  0.1509, -0.1159,\n",
            "         0.0458, -0.0221, -0.1604, -0.0991, -0.0764,  0.0526,  0.0716,  0.0017,\n",
            "         0.1582,  0.1485, -0.1339, -0.0540, -0.0883,  0.1520, -0.1390,  0.1140,\n",
            "         0.0948,  0.0627,  0.0673,  0.0154, -0.1167, -0.1090, -0.1629, -0.1756,\n",
            "         0.0789,  0.1306, -0.0846, -0.1013,  0.1568,  0.0142, -0.0781, -0.0568,\n",
            "        -0.0406,  0.0142, -0.1183,  0.0684, -0.0150, -0.0774,  0.1093,  0.1138,\n",
            "         0.1645,  0.1007,  0.0591,  0.0518], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0475,  0.0033,  0.1076, -0.0393, -0.0188,  0.0181,  0.0561, -0.1811,\n",
            "         0.0651, -0.0689,  0.1378, -0.0471,  0.1222,  0.0132, -0.0730,  0.0723,\n",
            "         0.0260,  0.1786,  0.1299, -0.0618, -0.1237, -0.1170, -0.0236, -0.1350,\n",
            "        -0.1572, -0.1542,  0.2004,  0.0257,  0.1788,  0.0545, -0.1406,  0.0443,\n",
            "        -0.0195,  0.0030,  0.1023,  0.1414, -0.0054, -0.0360, -0.0954, -0.0785,\n",
            "         0.0715, -0.0552,  0.0200, -0.0415,  0.0325,  0.2554,  0.0567,  0.1544,\n",
            "         0.1057,  0.0852,  0.0333, -0.0702, -0.0287,  0.1006,  0.1541, -0.1163,\n",
            "         0.0499, -0.0250, -0.1577, -0.1015, -0.0793,  0.0473,  0.0745,  0.0009,\n",
            "         0.1651,  0.1494, -0.1332, -0.0564, -0.0910,  0.1483, -0.1346,  0.1125,\n",
            "         0.0926,  0.0571,  0.0657,  0.0181, -0.1142, -0.1091, -0.1530, -0.1780,\n",
            "         0.0772,  0.1382, -0.0874, -0.1087,  0.1567,  0.0085, -0.0778, -0.0522,\n",
            "        -0.0358,  0.0206, -0.1194,  0.0637, -0.0157, -0.0800,  0.1189,  0.1096,\n",
            "         0.1623,  0.0963,  0.0571,  0.0513], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0478,  0.0005,  0.1055, -0.0365, -0.0189,  0.0161,  0.0544, -0.1792,\n",
            "         0.0679, -0.0711,  0.1402, -0.0443,  0.1284,  0.0163, -0.0706,  0.0703,\n",
            "         0.0274,  0.1748,  0.1311, -0.0621, -0.1217, -0.1223, -0.0272, -0.1349,\n",
            "        -0.1557, -0.1544,  0.2019,  0.0281,  0.1786,  0.0556, -0.1424,  0.0474,\n",
            "        -0.0186,  0.0015,  0.0988,  0.1449, -0.0014, -0.0275, -0.1017, -0.0742,\n",
            "         0.0750, -0.0503,  0.0183, -0.0430,  0.0307,  0.2546,  0.0548,  0.1501,\n",
            "         0.1059,  0.0909,  0.0283, -0.0697, -0.0289,  0.1004,  0.1492, -0.1172,\n",
            "         0.0520, -0.0217, -0.1562, -0.0995, -0.0780,  0.0533,  0.0727,  0.0010,\n",
            "         0.1601,  0.1494, -0.1312, -0.0552, -0.0933,  0.1510, -0.1363,  0.1093,\n",
            "         0.0988,  0.0595,  0.0664,  0.0175, -0.1162, -0.1052, -0.1559, -0.1801,\n",
            "         0.0735,  0.1349, -0.0820, -0.1084,  0.1585,  0.0083, -0.0781, -0.0556,\n",
            "        -0.0388,  0.0133, -0.1209,  0.0647, -0.0166, -0.0752,  0.1099,  0.1099,\n",
            "         0.1636,  0.0949,  0.0582,  0.0481], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0470,  0.0027,  0.1088, -0.0397, -0.0188,  0.0175,  0.0566, -0.1815,\n",
            "         0.0653, -0.0689,  0.1365, -0.0469,  0.1221,  0.0134, -0.0731,  0.0721,\n",
            "         0.0269,  0.1783,  0.1304, -0.0619, -0.1239, -0.1175, -0.0236, -0.1358,\n",
            "        -0.1569, -0.1539,  0.1997,  0.0255,  0.1787,  0.0546, -0.1405,  0.0447,\n",
            "        -0.0190,  0.0028,  0.1017,  0.1413, -0.0056, -0.0360, -0.0960, -0.0779,\n",
            "         0.0719, -0.0545,  0.0201, -0.0414,  0.0328,  0.2557,  0.0564,  0.1549,\n",
            "         0.1060,  0.0846,  0.0332, -0.0692, -0.0282,  0.1003,  0.1548, -0.1162,\n",
            "         0.0495, -0.0255, -0.1580, -0.1014, -0.0787,  0.0472,  0.0755,  0.0009,\n",
            "         0.1647,  0.1497, -0.1324, -0.0560, -0.0913,  0.1494, -0.1344,  0.1134,\n",
            "         0.0930,  0.0567,  0.0657,  0.0174, -0.1150, -0.1095, -0.1531, -0.1787,\n",
            "         0.0773,  0.1381, -0.0874, -0.1089,  0.1574,  0.0081, -0.0774, -0.0519,\n",
            "        -0.0359,  0.0219, -0.1194,  0.0640, -0.0153, -0.0798,  0.1186,  0.1085,\n",
            "         0.1620,  0.0959,  0.0574,  0.0513], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0475,  0.0082,  0.1032, -0.0365, -0.0220,  0.0149,  0.0528, -0.1816,\n",
            "         0.0614, -0.0683,  0.1346, -0.0452,  0.1225,  0.0184, -0.0666,  0.0748,\n",
            "         0.0269,  0.1764,  0.1269, -0.0656, -0.1319, -0.1201, -0.0225, -0.1361,\n",
            "        -0.1584, -0.1517,  0.1987,  0.0282,  0.1820,  0.0513, -0.1446,  0.0474,\n",
            "        -0.0179,  0.0005,  0.0997,  0.1443, -0.0062, -0.0291, -0.0953, -0.0719,\n",
            "         0.0716, -0.0540,  0.0174, -0.0400,  0.0308,  0.2605,  0.0527,  0.1556,\n",
            "         0.1032,  0.0862,  0.0292, -0.0725, -0.0313,  0.0972,  0.1539, -0.1116,\n",
            "         0.0481, -0.0198, -0.1622, -0.0949, -0.0821,  0.0502,  0.0749,  0.0010,\n",
            "         0.1636,  0.1522, -0.1325, -0.0567, -0.0897,  0.1499, -0.1376,  0.1131,\n",
            "         0.0979,  0.0598,  0.0685,  0.0136, -0.1157, -0.1082, -0.1588, -0.1784,\n",
            "         0.0756,  0.1341, -0.0848, -0.1032,  0.1537,  0.0121, -0.0755, -0.0539,\n",
            "        -0.0396,  0.0191, -0.1196,  0.0664, -0.0167, -0.0820,  0.1111,  0.1072,\n",
            "         0.1635,  0.1000,  0.0580,  0.0510], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0470,  0.0037,  0.1082, -0.0397, -0.0194,  0.0182,  0.0572, -0.1819,\n",
            "         0.0652, -0.0692,  0.1372, -0.0460,  0.1219,  0.0137, -0.0724,  0.0721,\n",
            "         0.0265,  0.1783,  0.1300, -0.0619, -0.1235, -0.1178, -0.0234, -0.1361,\n",
            "        -0.1570, -0.1540,  0.1988,  0.0273,  0.1786,  0.0534, -0.1401,  0.0453,\n",
            "        -0.0196,  0.0032,  0.1021,  0.1411, -0.0060, -0.0358, -0.0958, -0.0771,\n",
            "         0.0717, -0.0554,  0.0206, -0.0418,  0.0324,  0.2563,  0.0562,  0.1557,\n",
            "         0.1061,  0.0850,  0.0331, -0.0690, -0.0283,  0.0992,  0.1544, -0.1156,\n",
            "         0.0504, -0.0245, -0.1585, -0.1010, -0.0795,  0.0476,  0.0756,  0.0006,\n",
            "         0.1654,  0.1490, -0.1321, -0.0563, -0.0918,  0.1491, -0.1353,  0.1127,\n",
            "         0.0933,  0.0570,  0.0664,  0.0176, -0.1148, -0.1087, -0.1531, -0.1788,\n",
            "         0.0769,  0.1391, -0.0871, -0.1090,  0.1563,  0.0083, -0.0773, -0.0515,\n",
            "        -0.0360,  0.0216, -0.1196,  0.0641, -0.0156, -0.0803,  0.1178,  0.1077,\n",
            "         0.1620,  0.0959,  0.0572,  0.0508], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0489,  0.0022,  0.1069, -0.0387, -0.0197,  0.0173,  0.0553, -0.1809,\n",
            "         0.0659, -0.0706,  0.1374, -0.0466,  0.1223,  0.0139, -0.0739,  0.0722,\n",
            "         0.0278,  0.1782,  0.1293, -0.0609, -0.1256, -0.1168, -0.0258, -0.1336,\n",
            "        -0.1563, -0.1539,  0.2011,  0.0251,  0.1793,  0.0549, -0.1422,  0.0447,\n",
            "        -0.0179,  0.0018,  0.1010,  0.1398, -0.0036, -0.0350, -0.0965, -0.0767,\n",
            "         0.0696, -0.0530,  0.0194, -0.0418,  0.0326,  0.2555,  0.0591,  0.1538,\n",
            "         0.1049,  0.0856,  0.0319, -0.0738, -0.0281,  0.1012,  0.1533, -0.1145,\n",
            "         0.0488, -0.0250, -0.1569, -0.1010, -0.0787,  0.0494,  0.0735,  0.0024,\n",
            "         0.1635,  0.1491, -0.1326, -0.0551, -0.0916,  0.1500, -0.1348,  0.1108,\n",
            "         0.0939,  0.0586,  0.0667,  0.0163, -0.1147, -0.1108, -0.1533, -0.1775,\n",
            "         0.0772,  0.1363, -0.0885, -0.1076,  0.1567,  0.0077, -0.0787, -0.0525,\n",
            "        -0.0363,  0.0205, -0.1193,  0.0658, -0.0146, -0.0818,  0.1174,  0.1106,\n",
            "         0.1637,  0.0984,  0.0569,  0.0515], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0487,  0.0017,  0.1055, -0.0389, -0.0186,  0.0150,  0.0530, -0.1805,\n",
            "         0.0639, -0.0700,  0.1378, -0.0477,  0.1213,  0.0122, -0.0715,  0.0716,\n",
            "         0.0275,  0.1783,  0.1285, -0.0615, -0.1242, -0.1191, -0.0259, -0.1324,\n",
            "        -0.1558, -0.1553,  0.1995,  0.0239,  0.1782,  0.0566, -0.1408,  0.0456,\n",
            "        -0.0168,  0.0006,  0.1022,  0.1436, -0.0019, -0.0321, -0.0954, -0.0785,\n",
            "         0.0721, -0.0526,  0.0201, -0.0409,  0.0297,  0.2551,  0.0534,  0.1547,\n",
            "         0.1048,  0.0874,  0.0315, -0.0728, -0.0292,  0.1017,  0.1530, -0.1153,\n",
            "         0.0485, -0.0223, -0.1594, -0.1012, -0.0778,  0.0499,  0.0715, -0.0012,\n",
            "         0.1615,  0.1503, -0.1335, -0.0556, -0.0904,  0.1494, -0.1353,  0.1122,\n",
            "         0.0943,  0.0583,  0.0671,  0.0170, -0.1166, -0.1083, -0.1564, -0.1763,\n",
            "         0.0786,  0.1354, -0.0873, -0.1070,  0.1576,  0.0111, -0.0772, -0.0528,\n",
            "        -0.0379,  0.0191, -0.1183,  0.0638, -0.0131, -0.0802,  0.1177,  0.1132,\n",
            "         0.1631,  0.0972,  0.0584,  0.0530], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0274, -0.0266,  0.1156, -0.0478, -0.0033,  0.0043,  0.0631, -0.1859,\n",
            "         0.0766, -0.0778,  0.1404, -0.0352,  0.1316,  0.0353, -0.0792,  0.0746,\n",
            "         0.0137,  0.1788,  0.1407, -0.0690, -0.1314, -0.1366, -0.0335, -0.1455,\n",
            "        -0.1357, -0.1621,  0.1842,  0.0299,  0.1672,  0.0574, -0.1510,  0.0450,\n",
            "        -0.0064,  0.0013,  0.0914,  0.1319,  0.0083, -0.0409, -0.1153, -0.0807,\n",
            "         0.0809, -0.0476,  0.0230, -0.0496,  0.0252,  0.2585,  0.0515,  0.1546,\n",
            "         0.1103,  0.0942,  0.0286, -0.0624, -0.0183,  0.0901,  0.1530, -0.1032,\n",
            "         0.0520, -0.0288, -0.1678, -0.1022, -0.0775,  0.0462,  0.0842, -0.0067,\n",
            "         0.1538,  0.1334, -0.1059, -0.0527, -0.0940,  0.1632, -0.1378,  0.1103,\n",
            "         0.1094,  0.0522,  0.0733,  0.0219, -0.1139, -0.1087, -0.1524, -0.1811,\n",
            "         0.0923,  0.1340, -0.0702, -0.1095,  0.1746,  0.0065, -0.0858, -0.0453,\n",
            "        -0.0310,  0.0215, -0.1162,  0.0637, -0.0236, -0.0744,  0.1160,  0.1040,\n",
            "         0.1597,  0.0996,  0.0583,  0.0578], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0487,  0.0022,  0.1083, -0.0398, -0.0178,  0.0188,  0.0553, -0.1808,\n",
            "         0.0654, -0.0692,  0.1386, -0.0479,  0.1219,  0.0135, -0.0734,  0.0715,\n",
            "         0.0238,  0.1784,  0.1306, -0.0625, -0.1232, -0.1187, -0.0219, -0.1355,\n",
            "        -0.1593, -0.1557,  0.1997,  0.0255,  0.1786,  0.0526, -0.1390,  0.0437,\n",
            "        -0.0193,  0.0036,  0.1031,  0.1416, -0.0059, -0.0373, -0.0944, -0.0799,\n",
            "         0.0741, -0.0555,  0.0212, -0.0387,  0.0329,  0.2538,  0.0564,  0.1544,\n",
            "         0.1071,  0.0851,  0.0327, -0.0694, -0.0280,  0.1011,  0.1540, -0.1177,\n",
            "         0.0481, -0.0261, -0.1576, -0.1018, -0.0787,  0.0467,  0.0760,  0.0014,\n",
            "         0.1654,  0.1474, -0.1331, -0.0566, -0.0911,  0.1482, -0.1355,  0.1126,\n",
            "         0.0909,  0.0575,  0.0649,  0.0187, -0.1147, -0.1091, -0.1543, -0.1778,\n",
            "         0.0757,  0.1403, -0.0872, -0.1091,  0.1565,  0.0094, -0.0775, -0.0515,\n",
            "        -0.0368,  0.0207, -0.1172,  0.0636, -0.0155, -0.0800,  0.1210,  0.1093,\n",
            "         0.1614,  0.0957,  0.0568,  0.0533], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0478,  0.0035,  0.1081, -0.0390, -0.0194,  0.0183,  0.0554, -0.1808,\n",
            "         0.0655, -0.0684,  0.1364, -0.0472,  0.1219,  0.0129, -0.0723,  0.0724,\n",
            "         0.0264,  0.1782,  0.1288, -0.0616, -0.1236, -0.1173, -0.0237, -0.1352,\n",
            "        -0.1571, -0.1544,  0.1998,  0.0255,  0.1791,  0.0548, -0.1404,  0.0454,\n",
            "        -0.0200,  0.0032,  0.1018,  0.1425, -0.0063, -0.0353, -0.0953, -0.0781,\n",
            "         0.0716, -0.0555,  0.0205, -0.0414,  0.0323,  0.2562,  0.0563,  0.1551,\n",
            "         0.1059,  0.0845,  0.0332, -0.0694, -0.0283,  0.1008,  0.1545, -0.1166,\n",
            "         0.0503, -0.0248, -0.1575, -0.1004, -0.0792,  0.0479,  0.0750,  0.0014,\n",
            "         0.1648,  0.1502, -0.1338, -0.0557, -0.0910,  0.1490, -0.1349,  0.1132,\n",
            "         0.0926,  0.0577,  0.0654,  0.0167, -0.1155, -0.1090, -0.1536, -0.1786,\n",
            "         0.0776,  0.1379, -0.0881, -0.1090,  0.1559,  0.0086, -0.0772, -0.0520,\n",
            "        -0.0362,  0.0225, -0.1195,  0.0647, -0.0148, -0.0798,  0.1179,  0.1092,\n",
            "         0.1620,  0.0959,  0.0570,  0.0505], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0323,  0.0056,  0.1117, -0.0409, -0.0179,  0.0222,  0.0468, -0.1755,\n",
            "         0.0536, -0.0779,  0.1326, -0.0357,  0.1154,  0.0143, -0.0510,  0.0798,\n",
            "         0.0019,  0.1821,  0.1163, -0.0635, -0.1345, -0.1171, -0.0408, -0.1555,\n",
            "        -0.1407, -0.1736,  0.1968,  0.0376,  0.1663,  0.0590, -0.1369,  0.0475,\n",
            "        -0.0204,  0.0057,  0.1017,  0.1497, -0.0097, -0.0259, -0.1083, -0.0667,\n",
            "         0.0732, -0.0654,  0.0278, -0.0394,  0.0177,  0.2619,  0.0387,  0.1658,\n",
            "         0.1052,  0.0816,  0.0186, -0.0808, -0.0431,  0.1009,  0.1638, -0.1019,\n",
            "         0.0446, -0.0146, -0.1614, -0.1085, -0.0954,  0.0654,  0.0895, -0.0087,\n",
            "         0.1578,  0.1356, -0.1146, -0.0462, -0.0738,  0.1565, -0.1497,  0.1008,\n",
            "         0.1032,  0.0403,  0.0857,  0.0148, -0.1261, -0.0849, -0.1789, -0.1831,\n",
            "         0.0931,  0.1422, -0.0651, -0.1066,  0.1476,  0.0239, -0.0720, -0.0363,\n",
            "        -0.0524,  0.0313, -0.1042,  0.0773, -0.0181, -0.0931,  0.1064,  0.0997,\n",
            "         0.1549,  0.1154,  0.0692,  0.0587], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0465,  0.0031,  0.1087, -0.0395, -0.0190,  0.0175,  0.0563, -0.1813,\n",
            "         0.0650, -0.0695,  0.1366, -0.0465,  0.1213,  0.0136, -0.0719,  0.0723,\n",
            "         0.0261,  0.1783,  0.1302, -0.0617, -0.1237, -0.1178, -0.0228, -0.1366,\n",
            "        -0.1566, -0.1556,  0.1987,  0.0272,  0.1779,  0.0547, -0.1403,  0.0458,\n",
            "        -0.0187,  0.0028,  0.1022,  0.1413, -0.0051, -0.0353, -0.0959, -0.0772,\n",
            "         0.0719, -0.0550,  0.0210, -0.0412,  0.0318,  0.2564,  0.0546,  0.1559,\n",
            "         0.1063,  0.0846,  0.0327, -0.0695, -0.0286,  0.0995,  0.1541, -0.1159,\n",
            "         0.0499, -0.0247, -0.1587, -0.1019, -0.0788,  0.0480,  0.0754, -0.0004,\n",
            "         0.1642,  0.1490, -0.1318, -0.0557, -0.0907,  0.1500, -0.1349,  0.1130,\n",
            "         0.0934,  0.0568,  0.0667,  0.0178, -0.1156, -0.1084, -0.1544, -0.1787,\n",
            "         0.0771,  0.1387, -0.0865, -0.1086,  0.1570,  0.0093, -0.0771, -0.0512,\n",
            "        -0.0372,  0.0221, -0.1189,  0.0643, -0.0153, -0.0805,  0.1175,  0.1083,\n",
            "         0.1615,  0.0962,  0.0576,  0.0514], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0739, -0.0010,  0.1870, -0.0572,  0.0335, -0.0053,  0.0640, -0.1741,\n",
            "         0.1022, -0.0325,  0.0976, -0.0834,  0.1241, -0.0229, -0.0674,  0.0871,\n",
            "        -0.0502,  0.1925,  0.1705, -0.0287, -0.0844, -0.1583,  0.0698, -0.1865,\n",
            "        -0.1519, -0.2262,  0.1015,  0.0654,  0.1793,  0.0143, -0.1017,  0.1022,\n",
            "        -0.0477,  0.0702,  0.1551,  0.1090, -0.0430, -0.0545, -0.0757,  0.0004,\n",
            "         0.0621, -0.1140,  0.0772, -0.0333,  0.0954,  0.2657,  0.0284,  0.2363,\n",
            "         0.1717,  0.0494,  0.0524,  0.0429,  0.0107,  0.0559,  0.2393, -0.1590,\n",
            "         0.0805, -0.0809, -0.2212, -0.0904, -0.0607, -0.0164,  0.1055, -0.0463,\n",
            "         0.1508,  0.0903, -0.1076, -0.0056, -0.1554,  0.1571, -0.1123,  0.1939,\n",
            "         0.0563,  0.0193,  0.0324, -0.0380, -0.1296, -0.1182, -0.1587, -0.1551,\n",
            "         0.0981,  0.1706, -0.0553, -0.0800,  0.1683, -0.0092, -0.0169, -0.0250,\n",
            "        -0.0204,  0.0777, -0.0745,  0.0712, -0.0208, -0.0465,  0.1269,  0.1000,\n",
            "         0.1582,  0.0617,  0.0138,  0.0117], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0478,  0.0007,  0.1064, -0.0405, -0.0176,  0.0173,  0.0545, -0.1821,\n",
            "         0.0626, -0.0706,  0.1376, -0.0491,  0.1216,  0.0149, -0.0715,  0.0736,\n",
            "         0.0246,  0.1783,  0.1322, -0.0623, -0.1254, -0.1176, -0.0236, -0.1357,\n",
            "        -0.1550, -0.1557,  0.1992,  0.0278,  0.1774,  0.0558, -0.1435,  0.0434,\n",
            "        -0.0165,  0.0030,  0.1012,  0.1407, -0.0032, -0.0348, -0.0957, -0.0751,\n",
            "         0.0735, -0.0528,  0.0205, -0.0399,  0.0319,  0.2572,  0.0569,  0.1560,\n",
            "         0.1065,  0.0865,  0.0303, -0.0734, -0.0279,  0.0988,  0.1524, -0.1141,\n",
            "         0.0477, -0.0241, -0.1594, -0.1010, -0.0797,  0.0493,  0.0735, -0.0022,\n",
            "         0.1629,  0.1469, -0.1326, -0.0571, -0.0918,  0.1502, -0.1350,  0.1124,\n",
            "         0.0931,  0.0575,  0.0672,  0.0183, -0.1143, -0.1099, -0.1530, -0.1762,\n",
            "         0.0780,  0.1376, -0.0860, -0.1055,  0.1580,  0.0085, -0.0786, -0.0516,\n",
            "        -0.0395,  0.0193, -0.1183,  0.0661, -0.0159, -0.0805,  0.1173,  0.1082,\n",
            "         0.1655,  0.0983,  0.0553,  0.0523], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0473,  0.0004,  0.1082, -0.0401, -0.0157,  0.0150,  0.0555, -0.1827,\n",
            "         0.0623, -0.0706,  0.1373, -0.0463,  0.1220,  0.0134, -0.0733,  0.0717,\n",
            "         0.0252,  0.1785,  0.1303, -0.0631, -0.1241, -0.1176, -0.0266, -0.1354,\n",
            "        -0.1576, -0.1550,  0.2007,  0.0229,  0.1773,  0.0541, -0.1410,  0.0430,\n",
            "        -0.0179,  0.0018,  0.0989,  0.1437, -0.0055, -0.0357, -0.0967, -0.0789,\n",
            "         0.0733, -0.0531,  0.0196, -0.0402,  0.0315,  0.2547,  0.0537,  0.1554,\n",
            "         0.1049,  0.0856,  0.0335, -0.0709, -0.0276,  0.1026,  0.1558, -0.1163,\n",
            "         0.0486, -0.0254, -0.1594, -0.1026, -0.0769,  0.0494,  0.0748,  0.0012,\n",
            "         0.1638,  0.1504, -0.1335, -0.0568, -0.0901,  0.1493, -0.1348,  0.1155,\n",
            "         0.0950,  0.0565,  0.0662,  0.0163, -0.1161, -0.1089, -0.1560, -0.1782,\n",
            "         0.0798,  0.1353, -0.0868, -0.1084,  0.1583,  0.0075, -0.0774, -0.0515,\n",
            "        -0.0359,  0.0205, -0.1186,  0.0648, -0.0148, -0.0784,  0.1176,  0.1110,\n",
            "         0.1625,  0.0969,  0.0577,  0.0512], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0488,  0.0040,  0.1103, -0.0406, -0.0215,  0.0190,  0.0545, -0.1816,\n",
            "         0.0661, -0.0683,  0.1362, -0.0498,  0.1208,  0.0119, -0.0709,  0.0726,\n",
            "         0.0236,  0.1776,  0.1277, -0.0608, -0.1236, -0.1167, -0.0216, -0.1352,\n",
            "        -0.1558, -0.1553,  0.2016,  0.0276,  0.1779,  0.0554, -0.1411,  0.0463,\n",
            "        -0.0193,  0.0041,  0.1042,  0.1404, -0.0052, -0.0372, -0.0935, -0.0783,\n",
            "         0.0696, -0.0559,  0.0210, -0.0416,  0.0314,  0.2576,  0.0595,  0.1558,\n",
            "         0.1072,  0.0810,  0.0323, -0.0709, -0.0314,  0.1005,  0.1548, -0.1169,\n",
            "         0.0492, -0.0278, -0.1573, -0.1021, -0.0810,  0.0457,  0.0765, -0.0004,\n",
            "         0.1644,  0.1485, -0.1321, -0.0554, -0.0905,  0.1498, -0.1359,  0.1121,\n",
            "         0.0901,  0.0579,  0.0678,  0.0172, -0.1125, -0.1098, -0.1539, -0.1792,\n",
            "         0.0771,  0.1393, -0.0897, -0.1062,  0.1557,  0.0087, -0.0786, -0.0509,\n",
            "        -0.0379,  0.0222, -0.1170,  0.0640, -0.0150, -0.0835,  0.1199,  0.1086,\n",
            "         0.1636,  0.0987,  0.0565,  0.0504], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0459,  0.0037,  0.1095, -0.0375, -0.0194,  0.0165,  0.0574, -0.1824,\n",
            "         0.0657, -0.0695,  0.1348, -0.0464,  0.1213,  0.0124, -0.0753,  0.0710,\n",
            "         0.0284,  0.1790,  0.1311, -0.0614, -0.1241, -0.1156, -0.0229, -0.1361,\n",
            "        -0.1582, -0.1543,  0.2016,  0.0249,  0.1794,  0.0547, -0.1413,  0.0458,\n",
            "        -0.0180,  0.0012,  0.1011,  0.1413, -0.0056, -0.0382, -0.0961, -0.0786,\n",
            "         0.0701, -0.0539,  0.0191, -0.0419,  0.0323,  0.2560,  0.0557,  0.1544,\n",
            "         0.1050,  0.0820,  0.0345, -0.0697, -0.0283,  0.1008,  0.1552, -0.1168,\n",
            "         0.0486, -0.0274, -0.1576, -0.1031, -0.0775,  0.0465,  0.0751,  0.0015,\n",
            "         0.1671,  0.1512, -0.1333, -0.0563, -0.0907,  0.1495, -0.1334,  0.1141,\n",
            "         0.0940,  0.0566,  0.0662,  0.0174, -0.1136, -0.1104, -0.1534, -0.1790,\n",
            "         0.0778,  0.1371, -0.0891, -0.1087,  0.1584,  0.0065, -0.0773, -0.0535,\n",
            "        -0.0354,  0.0207, -0.1209,  0.0636, -0.0156, -0.0803,  0.1170,  0.1087,\n",
            "         0.1617,  0.0964,  0.0588,  0.0511], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0472,  0.0013,  0.1099, -0.0408, -0.0174,  0.0156,  0.0559, -0.1815,\n",
            "         0.0663, -0.0691,  0.1369, -0.0470,  0.1209,  0.0132, -0.0730,  0.0717,\n",
            "         0.0263,  0.1777,  0.1307, -0.0623, -0.1231, -0.1185, -0.0231, -0.1365,\n",
            "        -0.1574, -0.1556,  0.1985,  0.0256,  0.1773,  0.0537, -0.1403,  0.0456,\n",
            "        -0.0184,  0.0025,  0.1014,  0.1402, -0.0047, -0.0355, -0.0966, -0.0784,\n",
            "         0.0727, -0.0539,  0.0210, -0.0401,  0.0333,  0.2549,  0.0546,  0.1557,\n",
            "         0.1067,  0.0853,  0.0338, -0.0682, -0.0288,  0.1000,  0.1549, -0.1169,\n",
            "         0.0494, -0.0258, -0.1594, -0.1023, -0.0775,  0.0473,  0.0753,  0.0004,\n",
            "         0.1638,  0.1482, -0.1318, -0.0560, -0.0920,  0.1502, -0.1339,  0.1143,\n",
            "         0.0931,  0.0571,  0.0662,  0.0168, -0.1153, -0.1099, -0.1554, -0.1790,\n",
            "         0.0762,  0.1388, -0.0876, -0.1088,  0.1585,  0.0075, -0.0774, -0.0505,\n",
            "        -0.0363,  0.0218, -0.1180,  0.0643, -0.0151, -0.0799,  0.1191,  0.1093,\n",
            "         0.1615,  0.0957,  0.0578,  0.0519], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0287,  0.0053,  0.1033, -0.0367, -0.0200,  0.0306,  0.0561, -0.1760,\n",
            "         0.0642, -0.0640,  0.1593, -0.0256,  0.1336,  0.0326, -0.0589,  0.0674,\n",
            "         0.0157,  0.1670,  0.1155, -0.0733, -0.1265, -0.1238, -0.0306, -0.1439,\n",
            "        -0.1393, -0.1533,  0.1967,  0.0463,  0.1807,  0.0549, -0.1379,  0.0435,\n",
            "        -0.0341,  0.0224,  0.1024,  0.1542,  0.0052, -0.0209, -0.1074, -0.0829,\n",
            "         0.0618, -0.0564,  0.0143, -0.0603,  0.0234,  0.2490,  0.0457,  0.1438,\n",
            "         0.1095,  0.0930,  0.0289, -0.0582, -0.0249,  0.0920,  0.1501, -0.1100,\n",
            "         0.0694, -0.0091, -0.1426, -0.0911, -0.0975,  0.0509,  0.0791,  0.0014,\n",
            "         0.1644,  0.1534, -0.1227, -0.0578, -0.0810,  0.1515, -0.1315,  0.1034,\n",
            "         0.1043,  0.0517,  0.0696,  0.0226, -0.1239, -0.0798, -0.1430, -0.1851,\n",
            "         0.0828,  0.1566, -0.0540, -0.1247,  0.1441,  0.0311, -0.0864, -0.0389,\n",
            "        -0.0314,  0.0251, -0.1126,  0.0515, -0.0278, -0.0756,  0.1210,  0.1095,\n",
            "         0.1481,  0.0988,  0.0461,  0.0446], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0392, -0.0034,  0.1022, -0.0499, -0.0365,  0.0204,  0.0337, -0.1881,\n",
            "         0.0583, -0.0706,  0.1437, -0.0637,  0.1168,  0.0199, -0.0599,  0.0661,\n",
            "         0.0346,  0.1692,  0.1201, -0.0736, -0.1166, -0.1251, -0.0345, -0.1328,\n",
            "        -0.1438, -0.1522,  0.2030,  0.0262,  0.1729,  0.0682, -0.1442,  0.0434,\n",
            "        -0.0175,  0.0079,  0.1087,  0.1356,  0.0132, -0.0331, -0.0925, -0.0811,\n",
            "         0.0596, -0.0432,  0.0178, -0.0449,  0.0241,  0.2526,  0.0563,  0.1526,\n",
            "         0.1131,  0.0885,  0.0095, -0.0685, -0.0415,  0.0944,  0.1450, -0.1168,\n",
            "         0.0438, -0.0194, -0.1568, -0.0995, -0.0816,  0.0583,  0.0891, -0.0045,\n",
            "         0.1490,  0.1410, -0.1151, -0.0510, -0.0757,  0.1608, -0.1443,  0.1025,\n",
            "         0.0797,  0.0656,  0.0808,  0.0111, -0.1154, -0.1199, -0.1441, -0.1766,\n",
            "         0.0804,  0.1586, -0.0885, -0.1034,  0.1519,  0.0278, -0.0918, -0.0302,\n",
            "        -0.0457,  0.0181, -0.0935,  0.0629, -0.0101, -0.0796,  0.1456,  0.1174,\n",
            "         0.1702,  0.1034,  0.0542,  0.0629], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0471,  0.0023,  0.1082, -0.0391, -0.0190,  0.0177,  0.0564, -0.1808,\n",
            "         0.0662, -0.0691,  0.1363, -0.0475,  0.1217,  0.0134, -0.0734,  0.0724,\n",
            "         0.0270,  0.1786,  0.1302, -0.0612, -0.1238, -0.1165, -0.0241, -0.1351,\n",
            "        -0.1569, -0.1541,  0.2000,  0.0254,  0.1783,  0.0550, -0.1407,  0.0445,\n",
            "        -0.0187,  0.0019,  0.1013,  0.1415, -0.0057, -0.0362, -0.0962, -0.0779,\n",
            "         0.0717, -0.0548,  0.0205, -0.0413,  0.0327,  0.2559,  0.0570,  0.1546,\n",
            "         0.1059,  0.0849,  0.0333, -0.0697, -0.0286,  0.1009,  0.1544, -0.1156,\n",
            "         0.0498, -0.0253, -0.1575, -0.1015, -0.0786,  0.0474,  0.0740,  0.0011,\n",
            "         0.1649,  0.1495, -0.1340, -0.0559, -0.0918,  0.1488, -0.1347,  0.1131,\n",
            "         0.0929,  0.0571,  0.0655,  0.0179, -0.1145, -0.1102, -0.1529, -0.1778,\n",
            "         0.0777,  0.1368, -0.0888, -0.1085,  0.1579,  0.0073, -0.0778, -0.0525,\n",
            "        -0.0359,  0.0212, -0.1201,  0.0648, -0.0148, -0.0799,  0.1176,  0.1092,\n",
            "         0.1624,  0.0955,  0.0576,  0.0507], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-2.9907e-01,  3.7809e-01, -2.4084e-01, -3.8690e-02,  3.4848e-01,\n",
            "        -3.6409e-01,  2.9397e-02,  9.7300e-02, -4.2505e-01, -2.8074e-01,\n",
            "         4.4870e-01,  2.0856e-01,  5.8759e-01, -1.6263e-01,  2.0800e-01,\n",
            "         1.2068e-01,  4.0514e-01,  6.9146e-01, -9.2815e-02, -1.0697e-01,\n",
            "         1.3610e-01,  7.1361e-02, -4.9296e-01, -3.5853e-01, -5.2353e-01,\n",
            "        -6.8176e-04,  6.0456e-01,  1.0576e-01,  5.9809e-01, -2.3491e-03,\n",
            "         3.6622e-01,  6.7682e-02,  5.4569e-02, -4.2505e-01,  2.3224e-01,\n",
            "         2.8202e-01,  3.2375e-01,  4.9232e-01,  1.3086e-01, -3.9163e-01,\n",
            "        -9.4406e-02,  5.6676e-01, -2.8135e-01, -3.3656e-01,  1.3894e-01,\n",
            "        -4.6411e-01,  5.2619e-01,  6.8669e-02,  5.2442e-01,  4.9080e-01,\n",
            "         1.7822e-01, -5.6465e-01, -1.5021e-01,  8.7892e-02, -6.9031e-02,\n",
            "        -4.1371e-01, -2.7273e-01, -7.5066e-02,  3.0097e-01,  9.5308e-02,\n",
            "         7.0742e-02,  1.4214e-01,  2.4345e-01,  6.8185e-01, -3.1594e-01,\n",
            "         1.2593e-01, -5.9034e-01, -4.2609e-01,  6.3146e-03,  5.8838e-01,\n",
            "        -3.4433e-01, -2.2272e-01,  2.4246e-01, -3.0097e-01,  2.1618e-01,\n",
            "         2.1259e-01,  1.1023e-01, -2.8793e-01, -6.0046e-02, -4.0386e-01,\n",
            "         1.6205e-01, -3.3893e-01,  1.8816e-03, -3.8218e-01, -5.3480e-02,\n",
            "         2.3567e-01, -7.5494e-02, -6.3614e-01, -1.1136e-01, -4.0025e-01,\n",
            "        -4.6049e-02,  6.2989e-01, -1.0963e-01, -4.0088e-01,  1.6580e-01,\n",
            "         5.5207e-01,  4.8514e-02,  1.2243e-01, -9.3353e-02, -2.7124e-01],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0480, -0.0021,  0.1074, -0.0403, -0.0167,  0.0144,  0.0546, -0.1819,\n",
            "         0.0648, -0.0718,  0.1365, -0.0497,  0.1219,  0.0141, -0.0714,  0.0714,\n",
            "         0.0268,  0.1785,  0.1331, -0.0599, -0.1251, -0.1200, -0.0227, -0.1340,\n",
            "        -0.1560, -0.1562,  0.2011,  0.0276,  0.1778,  0.0561, -0.1431,  0.0452,\n",
            "        -0.0143,  0.0005,  0.1030,  0.1400, -0.0003, -0.0355, -0.0959, -0.0753,\n",
            "         0.0713, -0.0521,  0.0184, -0.0418,  0.0334,  0.2567,  0.0578,  0.1544,\n",
            "         0.1070,  0.0876,  0.0308, -0.0727, -0.0301,  0.0968,  0.1506, -0.1140,\n",
            "         0.0480, -0.0244, -0.1613, -0.1032, -0.0766,  0.0488,  0.0713, -0.0036,\n",
            "         0.1583,  0.1453, -0.1303, -0.0560, -0.0942,  0.1508, -0.1374,  0.1123,\n",
            "         0.0931,  0.0581,  0.0691,  0.0188, -0.1145, -0.1118, -0.1546, -0.1751,\n",
            "         0.0779,  0.1349, -0.0879, -0.1039,  0.1616,  0.0097, -0.0814, -0.0524,\n",
            "        -0.0382,  0.0176, -0.1189,  0.0654, -0.0139, -0.0815,  0.1161,  0.1101,\n",
            "         0.1643,  0.0953,  0.0569,  0.0518], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0474,  0.0032,  0.1082, -0.0393, -0.0194,  0.0177,  0.0569, -0.1819,\n",
            "         0.0655, -0.0693,  0.1369, -0.0467,  0.1216,  0.0131, -0.0730,  0.0722,\n",
            "         0.0263,  0.1783,  0.1295, -0.0617, -0.1228, -0.1176, -0.0234, -0.1354,\n",
            "        -0.1574, -0.1541,  0.1995,  0.0264,  0.1782,  0.0537, -0.1400,  0.0450,\n",
            "        -0.0194,  0.0027,  0.1018,  0.1417, -0.0061, -0.0368, -0.0954, -0.0781,\n",
            "         0.0720, -0.0555,  0.0209, -0.0412,  0.0321,  0.2559,  0.0564,  0.1555,\n",
            "         0.1059,  0.0847,  0.0334, -0.0690, -0.0280,  0.1001,  0.1543, -0.1162,\n",
            "         0.0500, -0.0249, -0.1582, -0.1012, -0.0787,  0.0471,  0.0749,  0.0007,\n",
            "         0.1657,  0.1494, -0.1333, -0.0564, -0.0921,  0.1486, -0.1351,  0.1130,\n",
            "         0.0928,  0.0571,  0.0660,  0.0176, -0.1144, -0.1091, -0.1533, -0.1787,\n",
            "         0.0775,  0.1387, -0.0883, -0.1092,  0.1566,  0.0079, -0.0776, -0.0520,\n",
            "        -0.0359,  0.0212, -0.1196,  0.0637, -0.0149, -0.0804,  0.1182,  0.1086,\n",
            "         0.1621,  0.0957,  0.0572,  0.0508], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0463,  0.0018,  0.1109, -0.0407, -0.0170,  0.0161,  0.0574, -0.1820,\n",
            "         0.0660, -0.0687,  0.1365, -0.0482,  0.1200,  0.0125, -0.0733,  0.0723,\n",
            "         0.0253,  0.1785,  0.1317, -0.0621, -0.1220, -0.1168, -0.0220, -0.1372,\n",
            "        -0.1567, -0.1555,  0.1995,  0.0263,  0.1758,  0.0546, -0.1396,  0.0450,\n",
            "        -0.0178,  0.0020,  0.1019,  0.1405, -0.0054, -0.0374, -0.0962, -0.0794,\n",
            "         0.0727, -0.0551,  0.0218, -0.0401,  0.0328,  0.2557,  0.0547,  0.1559,\n",
            "         0.1069,  0.0845,  0.0348, -0.0682, -0.0295,  0.0998,  0.1546, -0.1170,\n",
            "         0.0499, -0.0273, -0.1587, -0.1032, -0.0780,  0.0450,  0.0746, -0.0013,\n",
            "         0.1654,  0.1482, -0.1329, -0.0571, -0.0919,  0.1489, -0.1335,  0.1146,\n",
            "         0.0925,  0.0558,  0.0663,  0.0192, -0.1131, -0.1099, -0.1536, -0.1783,\n",
            "         0.0767,  0.1386, -0.0876, -0.1082,  0.1600,  0.0069, -0.0773, -0.0512,\n",
            "        -0.0364,  0.0208, -0.1186,  0.0626, -0.0153, -0.0806,  0.1200,  0.1082,\n",
            "         0.1619,  0.0950,  0.0580,  0.0511], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.1368,  0.0534,  0.1487, -0.2547, -0.0980,  0.0302,  0.0871, -0.1785,\n",
            "         0.0771,  0.0012,  0.2038, -0.1595,  0.0705, -0.0091, -0.2068,  0.1387,\n",
            "         0.0702,  0.1190,  0.1901, -0.1530, -0.1574, -0.1212, -0.0498, -0.1832,\n",
            "        -0.2125,  0.0772,  0.2019,  0.1755,  0.3502, -0.0681, -0.1667,  0.0607,\n",
            "        -0.1602,  0.0549,  0.2140,  0.0842, -0.1498,  0.0412, -0.0856, -0.0075,\n",
            "        -0.0441, -0.0661, -0.0049, -0.1918,  0.0783,  0.1767,  0.1004,  0.0919,\n",
            "         0.0955,  0.0159,  0.1055,  0.0266,  0.0086,  0.0998,  0.2825, -0.2535,\n",
            "        -0.0862, -0.0040, -0.2953, -0.0966, -0.0322, -0.0741,  0.0295,  0.0626,\n",
            "         0.1304, -0.0130, -0.1092,  0.0999, -0.0615,  0.0362, -0.0927, -0.0044,\n",
            "        -0.0408, -0.0098,  0.0012, -0.1113, -0.1280, -0.1980, -0.1648, -0.2072,\n",
            "         0.0521,  0.2418, -0.1682, -0.0790,  0.1933, -0.1520,  0.0533, -0.0284,\n",
            "        -0.0075, -0.1069, -0.0506,  0.1002, -0.1329, -0.2012,  0.4069,  0.1800,\n",
            "         0.0414,  0.0488,  0.0239,  0.0140], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0503, -0.0060,  0.1075, -0.0381, -0.0171,  0.0121,  0.0484, -0.1802,\n",
            "         0.0658, -0.0763,  0.1377, -0.0487,  0.1251,  0.0199, -0.0756,  0.0722,\n",
            "         0.0232,  0.1771,  0.1362, -0.0649, -0.1309, -0.1247, -0.0187, -0.1357,\n",
            "        -0.1583, -0.1595,  0.1963,  0.0265,  0.1792,  0.0528, -0.1475,  0.0464,\n",
            "        -0.0124, -0.0026,  0.1012,  0.1355,  0.0044, -0.0347, -0.0979, -0.0745,\n",
            "         0.0765, -0.0465,  0.0170, -0.0341,  0.0354,  0.2515,  0.0553,  0.1518,\n",
            "         0.1077,  0.0846,  0.0264, -0.0741, -0.0273,  0.1008,  0.1514, -0.1140,\n",
            "         0.0379, -0.0263, -0.1612, -0.1018, -0.0757,  0.0502,  0.0765,  0.0012,\n",
            "         0.1584,  0.1402, -0.1259, -0.0523, -0.0904,  0.1551, -0.1372,  0.1108,\n",
            "         0.0911,  0.0621,  0.0665,  0.0170, -0.1128, -0.1139, -0.1601, -0.1759,\n",
            "         0.0730,  0.1347, -0.0866, -0.1033,  0.1614,  0.0108, -0.0789, -0.0498,\n",
            "        -0.0410,  0.0162, -0.1141,  0.0673, -0.0165, -0.0798,  0.1184,  0.1112,\n",
            "         0.1674,  0.1028,  0.0578,  0.0601], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "torch.Size([32, 46, 100])\n",
            "torch.Size([32, 17])\n",
            "tensor([[ 0.6349, -4.5691, -0.9479, -4.2436,  1.2434, -1.7452,  1.7573,  2.3211,\n",
            "          0.5127,  0.7841,  0.9022,  1.0098,  0.9606,  0.9377,  0.9489,  0.9603,\n",
            "          0.9478],\n",
            "        [-2.5746, -2.7974,  2.2973, -0.8829, -2.3914, -2.2876, -3.9037, -3.1606,\n",
            "          1.7396,  1.4701,  0.8069,  0.9584,  0.9649,  1.0224,  0.9335,  0.9284,\n",
            "          0.9495],\n",
            "        [-0.5588, -1.0992, -1.6896,  1.1670,  1.1946,  1.1380,  1.2816,  1.1111,\n",
            "          0.9757,  0.9797,  1.0205,  0.9743,  0.9305,  0.9420,  0.9459,  0.9451,\n",
            "          0.9481],\n",
            "        [-2.4712, -3.6810, -1.7805, -0.6769, -0.2094, -1.6433,  1.3621,  1.6193,\n",
            "          1.5339,  0.5846,  1.0658,  0.8933,  1.0361,  0.9536,  0.9251,  0.9417,\n",
            "          0.9482],\n",
            "        [-0.5223, -2.4755, -2.9906, -1.5538,  0.3002, -3.2343,  1.4387, -0.6112,\n",
            "         -3.4558,  0.3409,  0.6706,  0.8277,  1.1281,  0.9806,  0.9516,  1.0497,\n",
            "          0.9316],\n",
            "        [-3.0226, -2.7283, -1.1892, -0.2063,  0.2190, -2.1244, -0.1537,  0.9457,\n",
            "          0.1306,  1.3769,  1.2694,  1.0565,  0.9712,  0.9213,  0.9547,  0.9513,\n",
            "          0.9461],\n",
            "        [-3.2722, -3.2357, -0.5642, -5.5837, -0.4600, -3.2274, -0.4137, -0.5762,\n",
            "         -1.8373, -3.2551,  2.0931,  1.4841,  0.5709,  0.5954,  0.9264,  1.0195,\n",
            "          0.9500],\n",
            "        [ 0.6068, -0.5703,  1.8108,  0.9689,  3.0630, -4.5907,  1.3428,  1.5364,\n",
            "          0.7575,  0.7827,  0.9754,  0.9307,  1.0032,  0.9277,  0.9279,  0.9519,\n",
            "          0.9501],\n",
            "        [ 3.9515, -1.1361, -2.5748, -2.1310, -2.9120, -1.7308, -0.2593, -2.6058,\n",
            "          4.4928, -0.4265,  0.6971,  0.5773,  0.8440,  0.9711,  1.0138,  1.0384,\n",
            "          0.9361],\n",
            "        [ 2.9639,  1.6104,  1.0876, -2.4137,  2.1823,  1.8836,  0.4313,  1.6870,\n",
            "          0.6445,  0.9040,  1.0544,  0.9766,  0.8923,  0.9317,  0.9558,  0.9468,\n",
            "          0.9504],\n",
            "        [ 1.9780, -0.5439, -6.5996, -0.6973,  0.8677, -1.7978, -3.7785, -0.9980,\n",
            "          1.1468, -0.3378,  1.5985,  1.0154,  1.1077,  1.0038,  0.9294,  0.9565,\n",
            "          0.9423],\n",
            "        [-3.1875, -0.1768, -1.1137, -2.3724, -0.1674, -1.0390,  0.6803,  4.1603,\n",
            "          0.4681,  0.5165,  0.6905,  1.4110,  1.0845,  0.8821,  0.9580,  0.9559,\n",
            "          0.9443],\n",
            "        [ 0.1289, -0.3271, -2.0353, -0.4717, -4.4155, -4.1798, -2.2012,  0.6480,\n",
            "         -5.5318, -1.9444, -2.8600,  1.8560,  0.5162,  0.1431,  0.7625,  1.1080,\n",
            "          0.9910],\n",
            "        [-1.3997, -1.5771, -4.2415, -1.7367,  1.5150,  0.3232, -5.0854,  1.1334,\n",
            "          3.0711,  1.6483,  0.6339,  1.0774,  1.0273,  0.8959,  0.9667,  0.9658,\n",
            "          0.9400],\n",
            "        [ 1.5489,  3.1279, -5.4649, -0.7068, -2.5200, -0.7477, -0.9687, -0.5902,\n",
            "          1.0291,  1.0964,  0.9941,  1.0551,  1.0345,  0.9091,  0.9368,  0.9501,\n",
            "          0.9501],\n",
            "        [ 0.0583, -2.1769,  3.1352, -1.7806, -0.2827, -1.7563,  2.6708, -0.0374,\n",
            "          2.3595,  3.2779,  6.4925,  0.0473,  2.0065,  1.4734,  1.0120,  1.0335,\n",
            "          0.9666],\n",
            "        [-0.3742, -1.5155, -4.1874,  2.8611,  0.3641, -1.6439,  0.5955,  0.5445,\n",
            "          1.0617,  0.9870,  0.9666,  0.9720,  0.9541,  0.9478,  0.9502,  0.9438,\n",
            "          0.9519],\n",
            "        [-0.5672, -1.0171,  0.4260, -1.4790, -1.1047,  1.3692,  0.4654, -4.2009,\n",
            "          1.4386,  0.1302, -4.1048,  0.1579, -2.0514, -1.1221, -0.6018,  0.9450,\n",
            "          1.3792],\n",
            "        [-2.1404,  1.0098, -4.0162, -2.4252, -2.8555, -2.5312,  0.9241, -0.0393,\n",
            "          0.0892,  0.4035,  0.3121,  1.3710,  0.9184,  1.0519,  0.9928,  0.9397,\n",
            "          0.9426],\n",
            "        [-2.6573,  0.1162, -1.0012, -2.7637, -3.6240, -2.2336,  2.0610,  3.3996,\n",
            "         -0.1971,  0.3939,  1.2133,  1.2325,  1.1132,  0.8011,  0.9001,  0.9826,\n",
            "          0.9330],\n",
            "        [ 0.8268,  0.6876, -1.5898, -0.9255, -2.9676, -5.4325,  3.5252, -1.3982,\n",
            "          1.7347, -0.2539,  0.6579,  1.2077,  0.9966,  0.9969,  0.9946,  0.9131,\n",
            "          0.9380],\n",
            "        [-1.2002, -1.7038, -3.9768,  3.2542, -1.3015,  5.9428,  1.2496, -2.0242,\n",
            "          0.9769,  1.0102,  1.1443,  0.8532,  0.8952,  0.9490,  0.9116,  0.9486,\n",
            "          0.9382],\n",
            "        [-1.9709, -0.0425, -5.4147, -6.0588, -3.0267, -0.7638,  1.8892, -0.6378,\n",
            "          2.1157,  0.7036,  0.9566,  1.1701,  0.9380,  0.9382,  0.9376,  0.9517,\n",
            "          0.9424],\n",
            "        [-1.8959, -2.3552,  0.6506,  0.7942,  1.9941,  2.1295, -1.0125, -0.6939,\n",
            "          4.5651, -1.4450,  4.2592,  2.8265,  0.4704,  0.1319,  0.9996,  1.1405,\n",
            "          1.1396],\n",
            "        [ 0.0768, -1.7064,  0.0546, -0.6124,  3.1996,  0.7173,  1.4330,  0.1728,\n",
            "          6.8179, -1.1464, -1.0883, -0.6956,  0.6243,  1.1366,  1.3438,  0.6902,\n",
            "          0.9931],\n",
            "        [-1.2083,  3.7521,  5.6638, -1.2558, -0.2225, -1.6651,  0.0126, -0.1402,\n",
            "          1.1644,  1.0576,  1.0912,  1.0068,  1.0361,  0.9547,  0.9228,  0.9487,\n",
            "          0.9446],\n",
            "        [-0.8574, -1.7606,  1.4216, -2.6684, -3.2867, -1.0663, -1.1970,  0.5346,\n",
            "          3.0641, -4.4602, -0.6648, -5.5241, -0.1975, -2.5070,  3.6189,  6.0709,\n",
            "          1.7270],\n",
            "        [-0.1053, -1.5930, -0.0400,  0.3741, -1.3980, -5.3369, -1.0494,  3.9049,\n",
            "          0.3060,  0.1593,  1.3127,  1.3293,  0.9818,  0.9461,  0.9745,  0.9387,\n",
            "          0.9233],\n",
            "        [-0.3073, -0.9733,  0.2609,  2.5143, -4.0306,  0.8321,  0.7224,  0.7981,\n",
            "          1.1552,  1.0147,  1.0075,  1.0680,  0.9346,  0.9087,  0.9496,  0.9458,\n",
            "          0.9425],\n",
            "        [-2.8882,  0.3970, -0.2353,  0.0112, -2.3923,  1.8743,  1.1741,  0.6479,\n",
            "          1.0464,  0.7396,  0.5288,  1.1284,  1.0425,  0.9726,  0.9325,  0.9324,\n",
            "          0.9406],\n",
            "        [-0.8063,  1.0575, -1.4498,  3.3070, -3.3408, -0.2231, -1.6027, -4.7850,\n",
            "          0.7733, -0.7337, -3.1019, -6.4718, -0.8677, -5.1481, -0.8981, -0.0782,\n",
            "         -0.5272],\n",
            "        [ 0.3207, -0.8498,  0.2218, -3.6064, -2.9609,  2.0384, -1.6454, -0.1536,\n",
            "         -0.1538,  1.9437,  0.0321,  0.6716,  1.2255,  1.0003,  1.0757,  1.0303,\n",
            "          0.9194]], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([[ 8],\n",
            "        [10],\n",
            "        [ 0],\n",
            "        [ 4],\n",
            "        [ 4],\n",
            "        [ 8],\n",
            "        [ 6],\n",
            "        [ 1],\n",
            "        [ 6],\n",
            "        [ 6],\n",
            "        [ 9],\n",
            "        [ 4],\n",
            "        [ 0],\n",
            "        [ 5],\n",
            "        [ 7],\n",
            "        [ 7],\n",
            "        [ 4],\n",
            "        [ 9],\n",
            "        [ 7],\n",
            "        [ 1],\n",
            "        [ 9],\n",
            "        [11],\n",
            "        [ 1],\n",
            "        [13],\n",
            "        [ 2],\n",
            "        [ 6],\n",
            "        [12],\n",
            "        [ 2],\n",
            "        [ 2],\n",
            "        [ 3],\n",
            "        [15],\n",
            "        [10]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-3a81d030fa8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         }\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtraining_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# REMEMBER: delete the breaks after debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ee0915d1a497>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, batch_size, iterator_train, iterator_validation, optimizer, loss, num_train_samples, device)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ee0915d1a497>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, iterator_train, device, optimizer, loss, epoch, epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#     raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# claim, evid as inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# REMEMBER: round to nearest integer for dense and softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-886ad6ad9c05>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, claim, evid)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_claim_idxs_first_zero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'last_state'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# problemi con le label e target, sempre piu allineati"
      ],
      "metadata": {
        "id": "lYDUmjg749st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# saving model\n",
        "torch.save(model, 'model/model.pkl')"
      ],
      "metadata": {
        "id": "iB9jcI2YXUbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model_params = {\n",
        "            'embedding_dim': EMBEDDING_DIMENSION,\n",
        "            'output_dim': NUM_CLASSES,\n",
        "            'pre_trained_emb': torch.tensor(test.emb_matrix).to(device),\n",
        "            'merging_type': 'last_state',\n",
        "            'sentence_type': 'concatenation'\n",
        "        }\n",
        "\n",
        "\n",
        "model = Model(**model_params)\n",
        "model = torch.load('model/model.pkl')\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "-8CzEllyXf8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model:nn.Module,\n",
        "            claims,\n",
        "            evids):\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        claims = claims.to(torch.float32)\n",
        "        evids = evids.to(torch.float32)\n",
        "\n",
        "        prediction = torch.round(model(claims, evids)).cpu().tolist()\n",
        "        print(f\"PREDICTED LABEL: {'supports' if prediction == [.0, 1.0] else 'refutes'}\")\n",
        "\n",
        "for i, batch in enumerate(dataloader_test):\n",
        "    for j in range(BATCH_SIZE):\n",
        "\n",
        "        labels = batch[1].cpu().tolist()\n",
        "        claims = batch[0][0]\n",
        "        evids = batch[0][1]\n",
        "\n",
        "        # print(' '.join([test.tokenizer.key_to_value[el.item()] for el in claims[j]]))\n",
        "        # print(' '.join([test.tokenizer.key_to_value[el.item()] for el in evids[j]]))\n",
        "        print(f\"TRUE LABEL: {'supports' if labels[j] == [0, 1] else 'refutes'}\")\n",
        "\n",
        "        predict(model, torch.unsqueeze(claims[j], 0), torch.unsqueeze(evids[j], 0))\n",
        "\n",
        "    if i == 20:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOnU0LVpUAJL",
        "outputId": "28c303f1-476d-4c6b-913f-bed66bced7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: supports\n",
            "PREDICTED LABEL: refutes\n",
            "TRUE LABEL: refutes\n",
            "PREDICTED LABEL: refutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSWBjOyWys0g"
      },
      "source": [
        "#1 SISTEMARE LOSS\n",
        "#2 NON STAMPARE SEMPRE LO STESSO\n",
        "#3 "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}