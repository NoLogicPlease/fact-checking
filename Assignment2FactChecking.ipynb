{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2FactChecking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGoJUhHIxywn"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6CoTN8vxzKx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import requests\n",
        "import zipfile\n",
        "import pickle\n",
        "import gensim\n",
        "import random\n",
        "import gensim.downloader as gloader\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fhHGHAk10EH"
      },
      "source": [
        "EMBEDDING_DIMENSION = 50\n",
        "BATCH_SIZE = 64\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS = 3\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYwOT8L87ORu",
        "outputId": "9bbe9cd0-a59e-4517-de44-081bbcded487"
      },
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use. \n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True        \n",
        "    \n",
        "fix_random(42)\n",
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec  4 18:25:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    28W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk5ohYbXyO69"
      },
      "source": [
        "# PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC-BPPk4yOuY",
        "outputId": "ae41090f-3fd1-4590-d971-5974dc6a3147"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                           params={'id': toy_data_url_id},\n",
        "                                           stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "\n",
        "def pre_process(dataset, filename):  # clean the dataset\n",
        "    dataset.drop(dataset.columns[0], axis=1, inplace=True)  # remove first column of dataframe containing numbers\n",
        "    dataset.drop(['ID'], axis=1, inplace=True)\n",
        "    # remove numbers before each evidence\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'^\\d+\\t', '', x))\n",
        "    # remove everything after the period\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r' \\..*', ' .', x))\n",
        "    # remove round brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LRB-.*-RRB-', '', x))\n",
        "    # remove square brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LSB-.*-RSB-', '', x))\n",
        "\n",
        "    n_before = dataset.shape[0]\n",
        "    # removes instances longer than a threshold on evidence\n",
        "    dataset = dataset[dataset['Evidence'].str.split().str.len() <= 100]\n",
        "    # remove all rows where there are single brackets in the evidence\n",
        "    dataset = dataset[~dataset['Evidence'].str.contains('|'.join(['-LRB-', '-LSB-', '-RRB-', '-RSB-']))]\n",
        "    n_after = dataset.shape[0]\n",
        "\n",
        "    # removes punctuation and excessive spaces\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r' +', ' ', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'^ +', '', x))\n",
        "    dataset = dataset.applymap(lambda x: x.lower())\n",
        "\n",
        "    labels = {'supports': 1, 'refutes': 0}\n",
        "    dataset = dataset.replace({'Label': labels})\n",
        "    # removes rows with empty elements\n",
        "    dataset = dataset[dataset['Evidence'] != '']\n",
        "    dataset = dataset[dataset['Claim'] != '']\n",
        "    dataset = dataset[dataset['Label'] != '']\n",
        "\n",
        "\n",
        "\n",
        "    rem_elements = n_before - n_after\n",
        "    print(f\"Removed {rem_elements}\\t ({100 * rem_elements / n_before:.2F}%)\"\n",
        "          f\" elements because of inconsistency on {filename}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB=True\n",
        "except:\n",
        "    IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"We're running Colab\")\n",
        "    # Mount the Google Drive at mount\n",
        "    mount='/content/gdrive'\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Switch to the directory on the Google Drive that you want to use\n",
        "    drive_root = mount + \"/My Drive/NLP/Assignment2\"\n",
        "    \n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "        print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "        os.makedirs(drive_root, exist_ok=True)\n",
        "    \n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "    print(\"Checking working directory:\")\n",
        "    %pwd\n",
        "\n",
        "# download_data('dataset')\n",
        "\n",
        "if not len(os.listdir(\"dataset_cleaned\")):\n",
        "    for file in os.listdir(\"dataset\"):\n",
        "        dataset_cleaned = pre_process(pd.read_csv(\"dataset/\" + file, sep=','), file)\n",
        "        dataset_cleaned.to_csv(os.path.join(\"dataset_cleaned\", file))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/NLP/Assignment2  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/NLP/Assignment2\n",
            "/content/gdrive/My Drive/NLP/Assignment2\n",
            "Checking working directory:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGgYfXJqyD4n"
      },
      "source": [
        "# CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkUKdAkSyHvT"
      },
      "source": [
        "## FactDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPOc3HdsvyDI"
      },
      "source": [
        "class FactDataset(Dataset):\n",
        "    def __init__(self, emb_dim, glove_dict, glove_matrix, split='train', tokenizer=None):\n",
        "        pairs_claim_evid = pd.read_csv(f\"dataset_cleaned/{split}_pairs.csv\")\n",
        "        # tokenization & embeddings\n",
        "        text = pairs_claim_evid['Claim'] + ' ' + pairs_claim_evid['Evidence']\n",
        "        text = text.astype(str).to_list()\n",
        "\n",
        "        # IMPORTANT: if a tokenizer is already computed for example on the train set, for val set we have to extend\n",
        "        if tokenizer:\n",
        "            self.tokenizer = tokenizer\n",
        "            self.tokenizer.dataset_sentences = text\n",
        "        else:\n",
        "            self.tokenizer = Tokenizer(text, emb_dim, glove_dict, glove_matrix)\n",
        "\n",
        "        self.tokenizer.tokenize()\n",
        "        self.val_to_key = self.tokenizer.get_val_to_key()\n",
        "        self.emb_matrix = self.tokenizer.build_embedding_matrix()\n",
        "        print(len(self.emb_matrix))\n",
        "\n",
        "        lengths = pairs_claim_evid['Evidence'].str.split().str.len().to_numpy()\n",
        "\n",
        "        # print(len([i for i, el in enumerate(lengths) if el > 100]))\n",
        "\n",
        "        self.max_claim_len = int(pairs_claim_evid['Claim'].str.split().str.len().max())\n",
        "        self.max_evid_len = int(pairs_claim_evid['Evidence'].str.split().str.len().max())\n",
        "        self.max_seq_len = int(max(self.max_evid_len, self.max_claim_len))\n",
        "\n",
        "        # tester function\n",
        "        if not self._check_tokenizer():\n",
        "            raise ValueError\n",
        "\n",
        "        # create x as list of sentences\n",
        "        # sentences are splitted in claim evidence\n",
        "        self.x = []\n",
        "        print(f\"Creating FactDataset: \\n\")\n",
        "        for i, (claim_sen, evid_sen) in tqdm(enumerate(zip(pairs_claim_evid['Claim'], pairs_claim_evid['Evidence']))):\n",
        "            sentence = [[], []]\n",
        "            for word in claim_sen.split():\n",
        "                sentence[0].append(self.val_to_key[word])\n",
        "            for word in evid_sen.split():\n",
        "                sentence[1].append(self.val_to_key[word])\n",
        "\n",
        "            sentence[0] = sentence[0] + [0] * (self.max_seq_len - len(claim_sen.split()))\n",
        "            sentence[1] = sentence[1] + [0] * (self.max_seq_len - len(evid_sen.split()))\n",
        "            self.x.append(sentence)\n",
        "\n",
        "        self.x = torch.tensor(self.x)\n",
        "        self.y = torch.tensor(pairs_claim_evid['Label'])\n",
        "        self.n_samples = pairs_claim_evid.shape[0]\n",
        "        print(f\"Max sequence length: {self.max_seq_len}\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # xi = [[claim][evid]], yi = [label]\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def _check_tokenizer(self):\n",
        "        word = 'the'\n",
        "        for i, el in enumerate(self.tokenizer.glove_matrix[self.tokenizer.glove_dict[word]]):\n",
        "            if el != self.emb_matrix[self.val_to_key[word]][i]:\n",
        "                print(\"Check Tokenizer, possible bugs\")\n",
        "                return False\n",
        "        return True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJHDwAfYyK_X"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mHt8SIP1WY_"
      },
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, dataset_sentences, embedding_dim, glove_dict, glove_matrix):\n",
        "        self.embedding_matrix = None\n",
        "        self.value_to_key = {}\n",
        "        self.value_to_key_new = {}\n",
        "        self.key_to_value = {}\n",
        "        self.num_unique_words = 0\n",
        "        self.dataset_sentences = dataset_sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.glove_dict = glove_dict\n",
        "        self.glove_matrix = glove_matrix\n",
        "        self.unique_words = set()\n",
        "\n",
        "    def get_val_to_key(self):\n",
        "        return copy.deepcopy(self.value_to_key)\n",
        "\n",
        "    def tokenize(self):\n",
        "        self.value_to_key_new = {}\n",
        "        unique_words = set()\n",
        "        for sen in self.dataset_sentences:\n",
        "            for w in sen.split():\n",
        "                unique_words.add(w)  # get set of unique words\n",
        "        new_unique = unique_words - self.unique_words\n",
        "        for i, word in enumerate(new_unique):\n",
        "            if self.embedding_matrix is not None:\n",
        "                self.key_to_value[i + len(self.embedding_matrix)] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i + len(self.embedding_matrix)\n",
        "            else:\n",
        "                self.key_to_value[i] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i\n",
        "            self.value_to_key_new[word] = i\n",
        "\n",
        "        self.num_unique_words = len(new_unique)\n",
        "        self.unique_words = self.unique_words | new_unique  # union of unique words and new unique words\n",
        "\n",
        "\n",
        "    def __build_embedding_matrix_glove(self):\n",
        "        oov_words = []\n",
        "        tmp_embedding_matrix = np.zeros((self.num_unique_words, self.embedding_dim), dtype=np.float32)\n",
        "        len_old_emb_matrix = len(self.embedding_matrix) if self.embedding_matrix is not None else 0\n",
        "        print(f\"Finding OOVs: \")\n",
        "        for word, idx in tqdm(self.value_to_key_new.items()):\n",
        "            try:\n",
        "                embedding_vector = self.glove_matrix[self.glove_dict[word]]\n",
        "                tmp_embedding_matrix[idx] = embedding_vector\n",
        "            except (KeyError, TypeError):\n",
        "                oov_words.append((word, idx + len_old_emb_matrix))\n",
        "        if self.embedding_matrix is not None:\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, tmp_embedding_matrix))\n",
        "        else:\n",
        "            self.embedding_matrix = tmp_embedding_matrix\n",
        "        return oov_words\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        oov_words = self.__build_embedding_matrix_glove()\n",
        "        print(f\"Solving OOVs: \")\n",
        "        for word, idx in tqdm(oov_words):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=self.embedding_dim)\n",
        "            self.embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "        # PADDING (feat. David Guetta)\n",
        "        first = self.embedding_matrix[0]\n",
        "        if np.count_nonzero(first) == 0:\n",
        "            first = self.embedding_matrix[0]\n",
        "            self.embedding_matrix[0] = np.zeros(self.embedding_dim)\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, first))\n",
        "\n",
        "            word_to_change = min(self.value_to_key.items(), key=lambda x: x[1])[0]\n",
        "            self.value_to_key[word_to_change] = len(self.embedding_matrix) - 1\n",
        "            self.key_to_value[len(self.embedding_matrix) - 1] = word_to_change\n",
        "\n",
        "        return copy.deepcopy(self.embedding_matrix)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERVX9LTl1Y5O"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuZv5Omm1gA_"
      },
      "source": [
        "##### General functions for Models #####\n",
        "\n",
        "def progress_bar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
        "    \"\"\"\n",
        "    credits to: https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total: \n",
        "        print()\n",
        "\n",
        "def train_epoch(model:nn.Module,\n",
        "              iterator_train: torch.utils.data.DataLoader,\n",
        "              device: torch.device,\n",
        "              optimizer: torch.optim,\n",
        "              loss,\n",
        "              epoch,\n",
        "              epochs):\n",
        "\n",
        "    accuracy_epoch = 0\n",
        "    loss_epoch = 0\n",
        "    num_batches = len(iterator_train)\n",
        "\n",
        "    progress_bar(0, num_batches, prefix='', suffix='Initializing', length=20, printEnd='')\n",
        "\n",
        "    \n",
        "    model.train()\n",
        "    for i, (inputs, labels) in enumerate(iterator_train):\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.float32)\n",
        "            \n",
        "            predictions = model(inputs).squeeze(1)\n",
        "            \n",
        "            # REMEMBER: round to nearest integer for dense and argmax for softmax\n",
        "            accuracy_step = binary_accuracy(torch.round(predictions), labels)\n",
        "            accuracy_epoch += accuracy_step.item()\n",
        "\n",
        "            loss_step = loss(predictions, labels)\n",
        "            loss_epoch += loss_step.item()\n",
        "\n",
        "            loss_step.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i + 1) % (num_batches // 20) == 0:\n",
        "\n",
        "                train_info_str = f\"Epoch: {epoch}/{epochs} \"\\\n",
        "                                 f\"-- Step: {i + 1}/{num_batches} \"\\\n",
        "                                 f\"-- Acc: {accuracy_epoch/(i+1):.2F} \"\\\n",
        "                                 f\"-- Loss: {loss_epoch/(i+1):.2F}\"\n",
        "\n",
        "                progress_bar(i + 1, num_batches, prefix='', suffix=train_info_str, length=20, printEnd='')\n",
        "\n",
        "            \n",
        "    loss_epoch /= num_batches\n",
        "    accuracy_epoch /= num_batches\n",
        "    return loss_epoch, accuracy_epoch\n",
        "\n",
        "def validate_epoch(model:nn.Module,\n",
        "                iterator_val: torch.utils.data.DataLoader,\n",
        "                device: torch.device,\n",
        "                optimizer: torch.optim,\n",
        "                loss,\n",
        "                epoch):\n",
        "\n",
        "    accuracy_epoch = 0\n",
        "    loss_epoch = 0\n",
        "    num_batches = len(iterator_val)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(iterator_val):\n",
        "                inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.float32)  \n",
        "                predictions = model(inputs).squeeze(1)\n",
        "                \n",
        "                # REMEMBER: round to nearest integer for dense and argmax for softmax\n",
        "                accuracy_step = binary_accuracy(torch.round(predictions), labels)\n",
        "                accuracy_epoch += accuracy_step.item()\n",
        "\n",
        "                loss_step = loss(predictions, labels)\n",
        "                loss_epoch += loss_step.item()\n",
        "\n",
        "    loss_epoch /= num_batches\n",
        "    accuracy_epoch /= num_batches\n",
        "\n",
        "    return loss_epoch, accuracy_epoch\n",
        "    \n",
        "\n",
        "def train_model(model, epochs, batch_size, iterator_train, iterator_validation, optimizer, loss, num_train_samples, device):\n",
        "    model.to(device)\n",
        "    num_iters = num_train_samples // batch_size\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train_epoch(model, iterator_train, device, optimizer, loss, epoch, epochs)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc = validate_epoch(model, iterator_validation, device, optimizer, loss, epoch)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        epoch_info_str = f\"Epoch: {epoch}/{epochs} \"\\\n",
        "                         f\"-- Step: {num_iters}/{num_iters} \"\\\n",
        "                         f\"-- Acc: {train_acc:.2F} \"\\\n",
        "                         f\"-- Loss: {train_loss:.2F} \"\\\n",
        "                         f\"-- Acc_Val: {val_acc:.2F} \"\\\n",
        "                         f\"-- Loss_Val: {val_loss:.2F}\"\n",
        "        progress_bar(num_iters, num_iters, prefix='', suffix=epoch_info_str, length=20, printEnd='')\n",
        "        \n",
        "    return train_losses, train_accs, val_losses, val_accs\n",
        "\n",
        "\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    correct = (preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "\n",
        "#########################################\n",
        "class Model_RNN1(nn.Module):\n",
        "    def __init__(self, sentence_len, embedding_dim, output_dim, pre_trained_emb):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings=pre_trained_emb, freeze=True, padding_idx=0)\n",
        "        self.rnn_claim = nn.RNN(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
        "        self.rnn_evid = nn.RNN(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(embedding_dim * 2, 1)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # claim = [max_tok, emb_dim]\n",
        "        # evid = [max_tok, emb_dim]\n",
        "        # print(\"input shape:\", x.shape)\n",
        "\n",
        "        # print(x[:, 0].long())\n",
        "        emb_claim = self.embedding(x[:, 0].long())\n",
        "        emb_evid = self.embedding(x[:, 1].long())\n",
        "        emb_evid = self.embedding(x[:, 1].long())\n",
        "        # print(\"emb_merda:\", emb_evid.shape)  # [32, 90, 50]iterator_val\n",
        "\n",
        "        output_claim, hidden_claim = self.rnn_claim(emb_claim)\n",
        "        output_evid, hidden_evid = self.rnn_evid(emb_evid)\n",
        "        # print(\"hidden merda:\", hidden_claim.shape)  # [1, 1, 50]\n",
        "\n",
        "        concat = torch.cat((hidden_claim[0], hidden_evid[0]), 1)\n",
        "        # print(\"concat merda\", concat.shape)  # [1, 1, 100]\n",
        "        # print(concat)\n",
        "        dense = self.fc(concat)\n",
        "        return dense"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWCd1T3Y2MVE"
      },
      "source": [
        "# MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQMjEAKg1tEj",
        "outputId": "9cb424a1-f194-4972-ab65-e3c4ba1a7319"
      },
      "source": [
        "def load_dataset():\n",
        "    # LOAD GLOVE\n",
        "    try:\n",
        "        with open(f\"glove-{EMBEDDING_DIMENSION}.pkl\", 'rb') as f:\n",
        "            emb_model = pickle.load(f)\n",
        "    except Exception:\n",
        "        emb_model = gloader.load(f\"glove-wiki-gigaword-{EMBEDDING_DIMENSION}\")\n",
        "        with open(f\"glove-{EMBEDDING_DIMENSION}.pkl\", 'wb') as f:\n",
        "            pickle.dump(emb_model, f)\n",
        "\n",
        "    glove_dict = emb_model.key_to_index\n",
        "    glove_matrix = emb_model.vectors\n",
        "\n",
        "    # LOAD CLEANED DATA IN TORCH DATASET OBJECTS\n",
        "    splits = {}\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        try:\n",
        "            with open(f\"{os.path.join('dataset_torched', split)}.pkl\", 'rb') as f:\n",
        "                splits[split] = pickle.load(f)\n",
        "        except Exception:\n",
        "            if split == 'train':\n",
        "                tokenizer = None\n",
        "            elif split == 'val':\n",
        "                tokenizer = splits['train'].tokenizer\n",
        "            else:\n",
        "                tokenizer = splits['val'].tokenizer \n",
        "\n",
        "            splits[split] = FactDataset(EMBEDDING_DIMENSION, glove_dict, glove_matrix, split, tokenizer=tokenizer)\n",
        "            with open(f\"{os.path.join('dataset_torched', split)}.pkl\", 'wb') as f:\n",
        "                pickle.dump(splits[split], f)\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "splits = load_dataset()\n",
        "train, val, test = splits['train'], splits['val'], splits['test']\n",
        "dataloader_train = DataLoader(dataset=train, batch_size=BATCH_SIZE, num_workers=2,\n",
        "                              pin_memory=True, shuffle=True)\n",
        "dataloader_val = DataLoader(dataset=val, batch_size=BATCH_SIZE, num_workers=2,\n",
        "                            pin_memory=True, shuffle=True)\n",
        "dataloader_test = DataLoader(dataset=test, batch_size=BATCH_SIZE, num_workers=2, \n",
        "                             pin_memory=True, shuffle=True)\n",
        "\n",
        "'''\n",
        "# TO REMOVE\n",
        "dataiter_train = iter(dataloader_train)\n",
        "dataiter_val = iter(dataloader_val)\n",
        "dataiter_test = iter(dataloader_test)\n",
        "'''\n",
        "\n",
        "\n",
        "# print(torch.tensor(val.emb_matrix, device=device))\n",
        "print(max(train.val_to_key.values()))\n",
        "print(val.emb_matrix.shape)\n",
        "\n",
        "model_params = {\n",
        "    'sentence_len': train.max_seq_len,\n",
        "    'embedding_dim': EMBEDDING_DIMENSION,\n",
        "    'output_dim': NUM_CLASSES,\n",
        "    'pre_trained_emb': torch.tensor(val.emb_matrix).to(device)\n",
        "}\n",
        "\n",
        "model = Model_RNN1(**model_params)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "loss = loss.to(device)\n",
        "\n",
        "# summary(model, (2, 90), batch_size=32)\n",
        "# quit()\n",
        "\n",
        "training_info = {\n",
        "    'model': model,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'iterator_train': dataloader_train,\n",
        "    'iterator_validation': dataloader_val,\n",
        "    'optimizer': optimizer,\n",
        "    'loss': loss,\n",
        "    'num_train_samples': train.n_samples,\n",
        "    'device': device\n",
        "}\n",
        "\n",
        "train_losses, train_accs, val_losses, val_accs = train_model(**training_info)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31987\n",
            "(33548, 50)\n",
            " |████████████████████| 100.0% Epoch: 1/3 -- Step: 1892/1892 -- Acc: 0.73 -- Loss: 0.58 -- Acc_Val: 0.50 -- Loss_Val: 0.81\n",
            " |████████████████████| 100.0% Epoch: 2/3 -- Step: 1892/1892 -- Acc: 0.73 -- Loss: 0.58 -- Acc_Val: 0.50 -- Loss_Val: 0.84\n",
            " |████████████████████| 100.0% Epoch: 3/3 -- Step: 1892/1892 -- Acc: 0.73 -- Loss: 0.58 -- Acc_Val: 0.51 -- Loss_Val: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JleCKqxecVF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}