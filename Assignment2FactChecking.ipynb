{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2FactChecking.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGoJUhHIxywn"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6CoTN8vxzKx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import requests\n",
        "import zipfile\n",
        "import pickle\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fhHGHAk10EH"
      },
      "source": [
        "EMBEDDING_DIMENSION = 50\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk5ohYbXyO69"
      },
      "source": [
        "# PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC-BPPk4yOuY",
        "outputId": "f9054f08-a657-4f96-884b-4677aa54e3dd"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                           params={'id': toy_data_url_id},\n",
        "                                           stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "\n",
        "def pre_process(dataset, filename):  # clean the dataset\n",
        "    dataset.drop(dataset.columns[0], axis=1, inplace=True)  # remove first column of dataframe containing numbers\n",
        "    dataset.drop(['ID'], axis=1, inplace=True)\n",
        "    # remove numbers before each evidence\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'^\\d+\\t', '', x))\n",
        "    # remove everything after the period\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r' \\..*', ' .', x))\n",
        "    # remove round brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LRB-.*-RRB-', '', x))\n",
        "    # remove square brackets and what they contain\n",
        "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LSB-.*-RSB-', '', x))\n",
        "\n",
        "    n_before = dataset.shape[0]\n",
        "    # removes instances longer than a threshold on evidence\n",
        "    dataset = dataset[dataset['Evidence'].str.split().str.len() <= 100]\n",
        "    # remove all rows where there are single brackets in the evidence\n",
        "    dataset = dataset[~dataset['Evidence'].str.contains('|'.join(['-LRB-', '-LSB-', '-RRB-', '-RSB-']))]\n",
        "    n_after = dataset.shape[0]\n",
        "\n",
        "    # removes punctuation and excessive spaces\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r' +', ' ', x))\n",
        "    dataset = dataset.applymap(lambda x: re.sub(r'^ +', '', x))\n",
        "    dataset = dataset.applymap(lambda x: x.lower())\n",
        "\n",
        "    labels = {'supports': 1, 'refutes': 0}\n",
        "    dataset = dataset.replace({'Label': labels})\n",
        "    # removes rows with empty elements\n",
        "    dataset = dataset[dataset['Evidence'] != '']\n",
        "    dataset = dataset[dataset['Claim'] != '']\n",
        "    dataset = dataset[dataset['Label'] != '']\n",
        "\n",
        "\n",
        "\n",
        "    rem_elements = n_before - n_after\n",
        "    print(f\"Removed {rem_elements}\\t ({100 * rem_elements / n_before:.2F}%)\"\n",
        "          f\" elements because of inconsistency on {filename}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#########################################\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB=True\n",
        "except:\n",
        "    IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"We're running Colab\")\n",
        "    # Mount the Google Drive at mount\n",
        "    mount='/content/gdrive'\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Switch to the directory on the Google Drive that you want to use\n",
        "    drive_root = mount + \"/My Drive/NLP/Assignment2\"\n",
        "    \n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "        print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "        os.makedirs(drive_root, exist_ok=True)\n",
        "    \n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "    print(\"Checking working directory:\")\n",
        "    %pwd\n",
        "\n",
        "# download_data('dataset')\n",
        "\n",
        "for file in os.listdir(\"dataset\"):\n",
        "    dataset_cleaned = pre_process(pd.read_csv(\"dataset/\" + file, sep=','), file)\n",
        "    dataset_cleaned.to_csv(os.path.join(\"dataset_cleaned\", file))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/NLP/Assignment2  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/NLP/Assignment2\n",
            "/content/gdrive/My Drive/NLP/Assignment2\n",
            "Checking working directory:\n",
            "Removed 492\t (0.40%) elements because of inconsistency on train_pairs.csv\n",
            "Removed 41\t (0.57%) elements because of inconsistency on val_pairs.csv\n",
            "Removed 4\t (0.06%) elements because of inconsistency on test_pairs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGgYfXJqyD4n"
      },
      "source": [
        "# CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkUKdAkSyHvT"
      },
      "source": [
        "## FactDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPOc3HdsvyDI"
      },
      "source": [
        "class FactDataset(Dataset):\n",
        "    def __init__(self, emb_dim, glove_dict, glove_matrix, split='train'):\n",
        "        pairs_claim_evid = pd.read_csv(f\"dataset_cleaned/{split}_pairs.csv\")\n",
        "        # tokenization & embeddings\n",
        "        text = pairs_claim_evid['Claim'] + ' ' + pairs_claim_evid['Evidence']\n",
        "        text = text.astype(str).to_list()\n",
        "\n",
        "        self.tokenizer = Tokenizer(text, emb_dim, glove_dict, glove_matrix)\n",
        "        self.tokenizer.tokenize()\n",
        "        self.val_to_key = self.tokenizer.get_val_to_key()\n",
        "        self.emb_matrix = self.tokenizer.build_embedding_matrix()\n",
        "        lengths = pairs_claim_evid['Evidence'].str.split().str.len().to_numpy()\n",
        "\n",
        "        # print(len([i for i, el in enumerate(lengths) if el > 100]))\n",
        "\n",
        "        self.max_claim_len = int(pairs_claim_evid['Claim'].str.split().str.len().max())\n",
        "        self.max_evid_len = int(pairs_claim_evid['Evidence'].str.split().str.len().max())\n",
        "        self.max_seq_len = int(max(self.max_evid_len, self.max_claim_len))\n",
        "\n",
        "        if not self._check_tokenizer():\n",
        "            raise ValueError\n",
        "\n",
        "        # create x as list of sentences\n",
        "        # sentences are splitted in claim evidence\n",
        "        self.x = []\n",
        "        print(f\"Creating FactDataset: \\n\")\n",
        "        for i, (claim_sen, evid_sen) in tqdm(enumerate(zip(pairs_claim_evid['Claim'], pairs_claim_evid['Evidence']))):\n",
        "            sentence = [[], []]\n",
        "            for word in claim_sen.split():\n",
        "                sentence[0].append(self.val_to_key[word])\n",
        "            for word in evid_sen.split():\n",
        "                sentence[1].append(self.val_to_key[word])\n",
        "\n",
        "            sentence[0] = sentence[0] + [0] * (self.max_seq_len - len(claim_sen.split()))\n",
        "            sentence[1] = sentence[1] + [0] * (self.max_seq_len - len(evid_sen.split()))\n",
        "            self.x.append(sentence)\n",
        "\n",
        "        self.x = torch.tensor(self.x)\n",
        "        self.y = torch.tensor(pairs_claim_evid['Label'])\n",
        "        self.n_samples = pairs_claim_evid.shape[0]\n",
        "        print(f\"Max sequence length: {self.max_seq_len}\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # xi = [[claim][evid]], yi = [label]\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def _check_tokenizer(self):\n",
        "        word = 'the'\n",
        "        for i, el in enumerate(self.tokenizer.glove_matrix[self.tokenizer.glove_dict[word]]):\n",
        "            if el != self.emb_matrix[self.val_to_key[word]][i]:\n",
        "                print(\"Check Tokenizer, possible bugs\")\n",
        "                return False\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJHDwAfYyK_X"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mHt8SIP1WY_"
      },
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, dataset_sentences, embedding_dim, glove_dict, glove_matrix):\n",
        "        self.embedding_matrix = None\n",
        "        self.value_to_key = {}\n",
        "        self.value_to_key_new = {}\n",
        "        self.key_to_value = {}\n",
        "        self.num_unique_words = 0\n",
        "        self.dataset_sentences = dataset_sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.glove_dict = glove_dict\n",
        "        self.glove_matrix = glove_matrix\n",
        "        self.unique_words = set()\n",
        "\n",
        "    def get_val_to_key(self):\n",
        "        return copy.deepcopy(self.value_to_key)\n",
        "\n",
        "    def tokenize(self):\n",
        "        self.value_to_key_new = {}\n",
        "        unique_words = set()\n",
        "        for sen in self.dataset_sentences:\n",
        "            for w in sen.split():\n",
        "                unique_words.add(w)  # get se of unique words\n",
        "        new_unique = unique_words - self.unique_words\n",
        "        for i, word in enumerate(new_unique):\n",
        "            if self.embedding_matrix is not None:\n",
        "                self.key_to_value[i + len(self.embedding_matrix)] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i + len(self.embedding_matrix)\n",
        "            else:\n",
        "                self.key_to_value[i] = word  # build two dictionaries for key value correspondence\n",
        "                self.value_to_key[word] = i\n",
        "            self.value_to_key_new[word] = i\n",
        "\n",
        "        self.num_unique_words = len(new_unique)\n",
        "        self.unique_words = self.unique_words | new_unique  # union of unique words and new unique words\n",
        "\n",
        "\n",
        "    def __build_embedding_matrix_glove(self):\n",
        "        oov_words = []\n",
        "        tmp_embedding_matrix = np.zeros((self.num_unique_words, self.embedding_dim), dtype=np.float32)\n",
        "        len_old_emb_matrix = len(self.embedding_matrix) if self.embedding_matrix is not None else 0\n",
        "        print(f\"Finding OOVs: \")\n",
        "        for word, idx in tqdm(self.value_to_key_new.items()):\n",
        "            try:\n",
        "                embedding_vector = self.glove_matrix[self.glove_dict[word]]\n",
        "                tmp_embedding_matrix[idx] = embedding_vector\n",
        "            except (KeyError, TypeError):\n",
        "                oov_words.append((word, idx + len_old_emb_matrix))\n",
        "        if self.embedding_matrix is not None:\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, tmp_embedding_matrix))\n",
        "        else:\n",
        "            self.embedding_matrix = tmp_embedding_matrix\n",
        "        return oov_words\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        oov_words = self.__build_embedding_matrix_glove()\n",
        "        print(f\"Solving OOVs: \")\n",
        "        for word, idx in tqdm(oov_words):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=self.embedding_dim)\n",
        "            self.embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "        # PADDING (feat. David Guetta)\n",
        "        first = self.embedding_matrix[0]\n",
        "        if np.count_nonzero(first) == 0:\n",
        "            first = self.embedding_matrix[0]\n",
        "            self.embedding_matrix[0] = np.zeros(self.embedding_dim)\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, first))\n",
        "\n",
        "            word_to_change = min(self.value_to_key.items(), key=lambda x: x[1])[0]\n",
        "            self.value_to_key[word_to_change] = len(self.embedding_matrix) - 1\n",
        "            self.key_to_value[len(self.embedding_matrix) - 1] = word_to_change\n",
        "\n",
        "        return copy.deepcopy(self.embedding_matrix)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERVX9LTl1Y5O"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuZv5Omm1gA_"
      },
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "##### General functions for Models #####\n",
        "def train_model(model, epochs, batch_size, iterator_train, iterator_validation, optimizer, loss, num_train_samples):\n",
        "    model.to(device)\n",
        "    num_iters = num_train_samples // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        epoch_loss_val = 0\n",
        "        epoch_acc_val = 0\n",
        "        for i, (inputs, labels) in enumerate(iterator_train):\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.float32)\n",
        "            predictions = model(inputs).squeeze(1)\n",
        "            loss_iter = loss(predictions, labels)\n",
        "            acc_iter = binary_accuracy(predictions, labels)\n",
        "            loss_iter.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss_iter.item()\n",
        "            epoch_acc += acc_iter.item()\n",
        "            if (i + 1) % (num_iters // 5) == 0:\n",
        "                print(f\"Epoch: {epoch + 1}/{epochs} -- Step: {i + 1}/{num_iters} \"\n",
        "                      f\"-- Acc: {acc_iter.item():.2F} -- Loss: {loss_iter.item():.2F}\")\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(iterator_validation):\n",
        "            inputs, labels = inputs.to(device).to(torch.float32), labels.to(device).to(torch.float32)\n",
        "            with torch.no_grad():\n",
        "                predictions = model(inputs).squeeze(1)\n",
        "                loss_iter = loss(predictions, labels)\n",
        "                acc_iter = binary_accuracy(predictions, labels)\n",
        "                epoch_loss_val += loss_iter.item()\n",
        "                epoch_acc_val += acc_iter.item()\n",
        "\n",
        "        print(f\"Epoch: {epoch + 1}/{epochs} -- Step: {num_iters}/{num_iters} \"\n",
        "              f\"-- Acc: {epoch_acc / len(iterator_train):.2F} \"\n",
        "              f\"-- Loss: {epoch_loss / len(iterator_train):.2F} \"\n",
        "              f\"-- Acc_Val: {epoch_acc_val / len(iterator_validation):.2F} \"\n",
        "              f\"-- Loss_Val: {epoch_loss_val / len(iterator_validation):.2F}\")\n",
        "\n",
        "    return epoch_acc / len(iterator_train), epoch_loss / len(iterator_train)\n",
        "\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "\n",
        "#########################################\n",
        "class Model_RNN1(nn.Module):\n",
        "    def __init__(self, sentence_len, embedding_dim, output_dim, pre_trained_emb):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings=pre_trained_emb, freeze=True, padding_idx=0)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=embedding_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(100, 1)\n",
        "        # self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # claim = [max_tok, emb_dim]\n",
        "        # evid = [max_tok, emb_dim]\n",
        "        # print(\"input shape:\", x.shape)\n",
        "\n",
        "        # print(x[:, 0].long())\n",
        "        emb_claim = self.embedding(x[:, 0].long())\n",
        "        emb_evid = self.embedding(x[:, 1].long())\n",
        "        # print(\"emb_merda:\", emb_evid.shape)  # [32, 90, 50]\n",
        "\n",
        "        output_claim, hidden_claim = self.rnn(emb_claim)\n",
        "        output_evid, hidden_evid = self.rnn(emb_evid)\n",
        "        # print(\"hidden merda:\", hidden_claim.shape)  # [1, 1, 50]\n",
        "\n",
        "        concat = torch.cat((hidden_claim[0], hidden_evid[0]), 1)\n",
        "        # print(\"concat merda\", concat.shape)  # [1, 1, 100]\n",
        "        # print(concat)\n",
        "        dense = self.fc(concat)\n",
        "        return dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWCd1T3Y2MVE"
      },
      "source": [
        "# MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQMjEAKg1tEj",
        "outputId": "bb57daf2-d808-49f6-934a-c253f600f468"
      },
      "source": [
        "\n",
        "def load_dataset():\n",
        "    # LOAD GLOVE\n",
        "    try:\n",
        "        with open(f\"glove-{EMBEDDING_DIMENSION}.pkl\", 'rb') as f:\n",
        "            emb_model = pickle.load(f)\n",
        "    except Exception:\n",
        "        emb_model = gloader.load(f\"glove-wiki-gigaword-{EMBEDDING_DIMENSION}\")\n",
        "        with open(f\"glove-{EMBEDDING_DIMENSION}.pkl\", 'wb') as f:\n",
        "            pickle.dump(emb_model, f)\n",
        "\n",
        "    glove_dict = emb_model.key_to_index\n",
        "    glove_matrix = emb_model.vectors\n",
        "\n",
        "    # LOAD CLEANED DATA IN TORCH DATASET OBJECTS\n",
        "    splits = {}\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        try:\n",
        "            with open(f\"{os.path.join('dataset_torched', split)}.pkl\", 'rb') as f:\n",
        "                splits[split] = pickle.load(f)\n",
        "        except Exception:\n",
        "            splits[split] = FactDataset(EMBEDDING_DIMENSION, glove_dict, glove_matrix, split)\n",
        "            with open(f\"{os.path.join('dataset_torched', split)}.pkl\", 'wb') as f:\n",
        "                pickle.dump(splits[split], f)\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "splits = load_dataset()\n",
        "train, val, test = splits['train'], splits['val'], splits['test']\n",
        "dataloader_train = DataLoader(dataset=train, batch_size=BATCH_SIZE, num_workers=2, shuffle=True)\n",
        "dataloader_val = DataLoader(dataset=val, batch_size=BATCH_SIZE, num_workers=2, shuffle=True)\n",
        "dataloader_test = DataLoader(dataset=test, batch_size=BATCH_SIZE, num_workers=2, shuffle=True)\n",
        "\n",
        "dataiter_train = iter(dataloader_train)\n",
        "dataiter_val = iter(dataloader_val)\n",
        "dataiter_test = iter(dataloader_test)\n",
        "\n",
        "# print(torch.tensor(train.emb_matrix, device=device))\n",
        "# print(max(train.val_to_key.values()))\n",
        "# print(train.emb_matrix.shape)\n",
        "\n",
        "model_params = {\n",
        "    'sentence_len': train.max_seq_len,\n",
        "    'embedding_dim': EMBEDDING_DIMENSION,\n",
        "    'output_dim': NUM_CLASSES,\n",
        "    'pre_trained_emb': torch.tensor(train.emb_matrix).to(device)\n",
        "}\n",
        "model = Model_RNN1(**model_params)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "loss = loss.to(device)\n",
        "\n",
        "# summary(model, (2, 90), batch_size=32)\n",
        "# quit()\n",
        "\n",
        "training_info = {\n",
        "    'model': model,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'iterator_train': dataiter_train,\n",
        "    'iterator_validation': dataiter_val,\n",
        "    'optimizer': optimizer,\n",
        "    'loss': loss,\n",
        "    'num_train_samples': train.n_samples\n",
        "}\n",
        "\n",
        "train_model(**training_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/2 -- Step: 378/3784 -- Acc: 0.75 -- Loss: 0.57\n",
            "Epoch: 1/2 -- Step: 756/3784 -- Acc: 0.78 -- Loss: 0.53\n",
            "Epoch: 1/2 -- Step: 1134/3784 -- Acc: 0.81 -- Loss: 0.50\n",
            "Epoch: 1/2 -- Step: 1512/3784 -- Acc: 0.69 -- Loss: 0.63\n",
            "Epoch: 1/2 -- Step: 1890/3784 -- Acc: 0.62 -- Loss: 0.69\n",
            "Epoch: 1/2 -- Step: 2268/3784 -- Acc: 0.66 -- Loss: 0.66\n",
            "Epoch: 1/2 -- Step: 2646/3784 -- Acc: 0.72 -- Loss: 0.59\n",
            "Epoch: 1/2 -- Step: 3024/3784 -- Acc: 0.81 -- Loss: 0.50\n",
            "Epoch: 1/2 -- Step: 3402/3784 -- Acc: 0.69 -- Loss: 0.63\n",
            "Epoch: 1/2 -- Step: 3780/3784 -- Acc: 0.81 -- Loss: 0.50\n",
            "Epoch: 1/2 -- Step: 3784/3784 -- Acc: 0.73 -- Loss: 0.58 -- Acc_Val: 0.50 -- Loss_Val: 0.82\n",
            "Epoch: 2/2 -- Step: 3784/3784 -- Acc: 0.00 -- Loss: 0.00 -- Acc_Val: 0.00 -- Loss_Val: 0.00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxn-G4Zn5rUW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}