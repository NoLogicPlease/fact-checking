{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb78692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 3\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True        \n",
    "    \n",
    "fix_random(42)\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4acf8c4",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cdb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "def download_data(data_path):\n",
    "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
    "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
    "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(toy_data_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:\n",
    "            response = current_session.get(toy_url,\n",
    "                                           params={'id': toy_data_url_id},\n",
    "              value                             stream=True)\n",
    "        save_response_content(response, toy_data_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
    "            loaded_zip.extractall(data_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "\n",
    "def pre_process(dataset, filename):  # clean the dataset\n",
    "    dataset.drop(dataset.columns[0], axis=1, inplace=True)  # remove first column of dataframe containing numbers\n",
    "    dataset.drop(['ID'], axis=1, inplace=True)\n",
    "    # remove numbers before each evidence\n",
    "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'^\\d+\\t', '', x))\n",
    "    # remove everything after the period\n",
    "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r' \\..*', ' .', x))\n",
    "    # remove round brackets and what they contain\n",
    "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LRB-.*-RRB-', '', x))\n",
    "    # remove square brackets and what they contain\n",
    "    dataset['Evidence'] = dataset['Evidence'].apply(lambda x: re.sub(r'-LSB-.*-RSB-', '', x))\n",
    "\n",
    "    n_before = dataset.shape[0]\n",
    "    # removes instances longer than a threshold on evidence\n",
    "    # TODO: only on train\n",
    "    dataset = dataset[dataset['Evidence'].str.split().str.len() <= 100]\n",
    "    # remove all rows where there are single brackets in the evidence\n",
    "    dataset = dataset[~dataset['Evidence'].str.contains('|'.join(['-LRB-', '-LSB-', '-RRB-', '-RSB-']))]\n",
    "    n_after = dataset.shape[0]\n",
    "\n",
    "    # removes punctuation and excessive spaces\n",
    "    dataset = dataset.applymap(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    dataset = dataset.applymap(lambda x: re.sub(r' +', ' ', x))\n",
    "    dataset = dataset.applymap(lambda x: re.sub(r'^ +', '', x))\n",
    "    dataset = dataset.applymap(lambda x: x.lower())\n",
    "\n",
    "    labels = {'supports': 1, 'refutes': 0}\n",
    "    dataset = dataset.replace({'Label': labels})\n",
    "    # removes rows with empty elements\n",
    "    dataset = dataset[dataset['Evidence'] != '']\n",
    "    dataset = dataset[dataset['Claim'] != '']\n",
    "    dataset = dataset[dataset['Label'] != '']\n",
    "\n",
    "\n",
    "\n",
    "    rem_elements = n_before - n_after\n",
    "    print(f\"Removed {rem_elements}\\t ({100 * rem_elements / n_before:.2F}%)\"\n",
    "          f\" elements because of inconsistency on {filename}\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#########################################\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB=True\n",
    "except:\n",
    "    IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"We're running Colab\")\n",
    "    # Mount the Google Drive at mount\n",
    "    mount='/content/gdrive'\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Switch to the directory on the Google Drive that you want to use\n",
    "    drive_root = mount + \"/My Drive/NLP/Assignment2\"\n",
    "    \n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "        print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "        os.makedirs(drive_root, exist_ok=True)\n",
    "    \n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "    print(\"Checking working directory:\")\n",
    "    %pwd\n",
    "\n",
    "# download_data('dataset')\n",
    "\n",
    "if not len(os.listdir(\"dataset_cleaned\")):\n",
    "    for file in os.listdir(\"dataset\"):\n",
    "        dataset_cleaned = pre_process(pd.read_csv(\"dataset/\" + file, sep=','), file)\n",
    "        dataset_cleaned.to_csv(os.path.join(\"dataset_cleaned\", file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11402a",
   "metadata": {},
   "source": [
    "# GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a42578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(coco_data, anns, most_common, path, batch_size, list_imgs):\n",
    "    list_imgs = list_imgs\n",
    "    dataset_size = len(list_imgs)\n",
    "\n",
    "    i = 0\n",
    "    X = np.zeros((batch_size, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    y = np.zeros((batch_size, IMG_HEIGHT, IMG_WIDTH, CATEGORIES))\n",
    "\n",
    "    while True:\n",
    "        for j in range(i, i + batch_size):\n",
    "            img_id = int(list_imgs[j].lstrip(\"0\").rstrip(\".jpg\"))  # getting image id\n",
    "            img = cv2.imread(os.path.join(path, list_imgs[j]), cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            y[j - i] = (generate_mask(img_id, img.shape, coco_data, anns, most_common))\n",
    "            X[j - i] = (cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT)))\n",
    "\n",
    "        i += batch_size\n",
    "        if i + batch_size >= dataset_size:\n",
    "            i = 0\n",
    "            random.shuffle(list_imgs)\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57177769",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002bbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, dataset_sentences, embedding_dim, glove_dict, glove_matrix):\n",
    "        self.embedding_matrix = None\n",
    "        self.value_to_key = {}\n",
    "        self.value_to_key_new = {}\n",
    "        self.key_to_value = {}\n",
    "        self.num_unique_words = 0\n",
    "        self.dataset_sentences = dataset_sentences\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.glove_dict = glove_dict\n",
    "        self.glove_matrix = glove_matrix\n",
    "        self.unique_words = set()\n",
    "\n",
    "    def get_val_to_key(self):\n",
    "        return copy.deepcopy(self.value_to_key)\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.value_to_key_new = {}\n",
    "        unique_words = set()\n",
    "        for sen in self.dataset_sentences:\n",
    "            for w in sen.split():\n",
    "                unique_words.add(w)  # get set of unique words\n",
    "        new_unique = unique_words - self.unique_words\n",
    "        for i, word in enumerate(new_unique):\n",
    "            if self.embedding_matrix is not None:\n",
    "                self.key_to_value[i + len(self.embedding_matrix)] = word  # build two dictionaries for key value correspondence\n",
    "                self.value_to_key[word] = i + len(self.embedding_matrix)\n",
    "            else:\n",
    "                self.key_to_value[i] = word  # build two dictionaries for key value correspondence\n",
    "                self.value_to_key[word] = i\n",
    "            self.value_to_key_new[word] = i\n",
    "\n",
    "        self.num_unique_words = len(new_unique)\n",
    "        self.unique_words = self.unique_words | new_unique  # union of unique words and new unique words\n",
    "\n",
    "\n",
    "    def __build_embedding_matrix_glove(self):\n",
    "        oov_words = []\n",
    "        tmp_embedding_matrix = np.zeros((self.num_unique_words, self.embedding_dim), dtype=np.float32)\n",
    "        len_old_emb_matrix = len(self.embedding_matrix) if self.embedding_matrix is not None else 0\n",
    "        print(f\"Finding OOVs: \")\n",
    "        for word, idx in tqdm(self.value_to_key_new.items()):\n",
    "            try:\n",
    "                embedding_vector = self.glove_matrix[self.glove_dict[word]]\n",
    "                tmp_embedding_matrix[idx] = embedding_vector\n",
    "            except (KeyError, TypeError):\n",
    "                oov_words.append((word, idx + len_old_emb_matrix))\n",
    "        if self.embedding_matrix is not None:\n",
    "            self.embedding_matrix = np.vstack((self.embedding_matrix, tmp_embedding_matrix))\n",
    "        else:\n",
    "            self.embedding_matrix = tmp_embedding_matrix\n",
    "        return oov_words\n",
    "\n",
    "    def build_embedding_matrix(self):\n",
    "        oov_words = self.__build_embedding_matrix_glove()\n",
    "        print(f\"Solving OOVs: \")\n",
    "        for word, idx in tqdm(oov_words):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=self.embedding_dim)\n",
    "            self.embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "        # PADDING (feat. David Guetta)\n",
    "        first = self.embedding_matrix[0]\n",
    "        if np.count_nonzero(first) != 0:\n",
    "            first = self.embedding_matrix[0]\n",
    "            self.embedding_matrix[0] = np.zeros(self.embedding_dim)\n",
    "            self.embedding_matrix = np.vstack((self.embedding_matrix, first))\n",
    "\n",
    "            word_to_change = min(self.value_to_key.items(), key=lambda x: x[1])[0]\n",
    "            self.key_to_value[0] = '<PAD>'\n",
    "            self.value_to_key['<PAD>'] = 0\n",
    "            self.value_to_key[word_to_change] = len(self.embedding_matrix) - 1\n",
    "            self.key_to_value[len(self.embedding_matrix) - 1] = word_to_change\n",
    "\n",
    "        return copy.deepcopy(self.embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c24ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
